{"access":{"can_view":true,"can_edit":false,"can_remove":false,"can_add":false,"can_publish":false,"can_get_doi":false,"can_share":true,"can_move":true,"can_move_outside":true,"can_transfer":true,"can_download":true,"limited_run":false,"limited_private_links":false,"limited_blind_links":false,"is_locked":false},"authors":[{"name":"Brughanya Subramanian","affiliation":"Shri Bhagwan Mahavir Vitreoretinal Services, SankaraNethralaya, Chennai, Tamil Nadu, India","affiliation_url":null,"username":"n4ule122t115ple1","link":null,"image":{"source":"/img/avatars/012.png","placeholder":"/img/avatars/012.png","webp_source":""},"note":"","is_verified_user":false},{"name":"A. Q. M Sala Uddin Pathan","affiliation":"School of Optometry and Vision Science, University of New South Wales, Syndey, NSW, 2052, Australia; Department of Computer Science and Telecommunication Engineering, Noakhali Science and Technology University","affiliation_url":null,"username":"n4ule122t105ule1","link":null,"image":{"source":"/img/avatars/017.png","placeholder":"/img/avatars/017.png","webp_source":""},"note":"","is_verified_user":true},{"name":"Maitreyee Roy","affiliation":"School of Optometry and Vision Science, University of New South Wales, Syndey, NSW, 2052, Australia","affiliation_url":null,"username":"n4ule122t105vle1","link":null,"image":{"source":"/img/avatars/014.png","placeholder":"/img/avatars/014.png","webp_source":""},"note":"","is_verified_user":false},{"name":"Dhanashree Ratra","affiliation":"Shri Bhagwan Mahavir Vitreoretinal Services, SankaraNethralaya, Chennai, Tamil Nadu, India","affiliation_url":null,"username":"n4ule122t105wle1","link":null,"image":{"source":"/img/avatars/006.png","placeholder":"/img/avatars/006.png","webp_source":""},"note":"","is_verified_user":false},{"name":"Salil S. Kanhere","affiliation":"School of Computer Science and Engineering, The University of New South Wales, Sydney, Australia","affiliation_url":null,"username":"n4ule122t105xle1","link":null,"image":{"source":"/img/avatars/012.png","placeholder":"/img/avatars/012.png","webp_source":""},"note":"","is_verified_user":false},{"name":"Matthew P Simunovic","affiliation":"The University of Sydney Save Sight Institute, Sydney, New South Wales, Australia; Sydney Hospital and Sydney Eye Hospital, Sydney, New South Wales, Australia","affiliation_url":null,"username":"n4ule122t105yle1","link":null,"image":{"source":"/img/avatars/003.png","placeholder":"/img/avatars/003.png","webp_source":""},"note":"","is_verified_user":false},{"name":"Rajiv Raman","affiliation":"Shri Bhagwan Mahavir Vitreoretinal Services, SankaraNethralaya, Chennai, Tamil Nadu, India","affiliation_url":null,"username":"rajiv-raman","link":"","image":{"source":"/img/avatars/005.png","placeholder":"/img/avatars/005.png","webp_source":""},"note":"","is_verified_user":true}],"before_start":"","book_chapter":null,"can_accept_authorship":false,"can_be_copied":true,"can_claim_authorship":false,"can_manage_keywords":true,"can_remove_fork":false,"can_sign":false,"child_steps":{},"cited_protocols":[],"collection_items":[],"created_on":1694234271,"creator":{"name":"Rajiv Raman","affiliation":"","affiliation_url":null,"username":"rajiv-raman","link":"","image":{"source":"/img/avatars/005.png","placeholder":"/img/avatars/005.png","webp_source":""},"badges":[{"id":2,"name":"Author","image":{"source":"/img/badges/bronze.svg","placeholder":"/img/badges/bronze.svg"}}],"affiliations":[]},"description":"{\"blocks\":[{\"key\":\"aa1i\",\"text\":\"This study is a continuation of our previous work published with PLOS One [1] and this\\nprotocol provides a walk-through for designing a deep learning based AI model, that aids in classifying the vitreomacular adhesion noticed in patients with diabetic macular oedema using SD-OCT images.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"sup\",\"offset\":74,\"length\":3}],\"entityRanges\":[],\"data\":{}},{\"key\":\"5gfcr\",\"text\":\"\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{}}","disclaimer":"","document":"","documents":[],"doi":"dx.doi.org/10.17504/protocols.io.4r3l2295pl1y/v1","doi_status":1,"ethics_statement":"{\"blocks\":[{\"key\":\"eliif\",\"text\":\"The institutional review board approved the study. All the research adhered to the tenets of the Helsinki Declaration. Written\\ninformed consent was obtained from the enrolled subjects, and all authors had access to subject information for data collection.Â  \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"46prc\",\"text\":\"\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{}}","fork_id":null,"fork_info":null,"forks":[],"funders":[],"groups":[{"id":105320,"uri":"sankara-nethralaya","title":"Sankara nethralaya","is_public":false,"image":{"source":"https://content.protocols.io/files/23952A144E3211EEAD330A58A9FEAC02-placeholder.png","placeholder":"https://content.protocols.io/files/23952A144E3211EEAD330A58A9FEAC02-placeholder.png"},"tech_support":{"email":null,"phone":null,"use_email":false,"hide_contact":false,"url":null}}],"guid":"95E2E2F98CE44E05B238695C1829C9F9","guidelines":"","has_references":true,"has_step_reagents":false,"has_versions":false,"id":87591,"image":{"source":"https://www.protocols.io/img/default_protocol.png","webp_source":null,"placeholder":"https://www.protocols.io/img/default_protocol.png","webp_placeholder":null},"image_attribution":"","in_trash":false,"is_bookmarked":false,"is_contact_suspended":false,"is_content_confidential":false,"is_content_warning":false,"is_doi_reserved":false,"is_in_pending_publishing":false,"is_in_transfer":false,"is_owner":true,"is_research":true,"is_retracted":false,"is_shared_directly":false,"is_subprotocol":null,"is_unlisted":false,"item_id":1159964,"journal":null,"journals":[],"keywords":"vitreomacular adhesion, machine learning,,diabetic macular edema,diabetic retinopathy","last_modified":1694520524,"link":"","location":null,"manuscript_citation":"","materials":[],"materials_text":"","ownership_history":null,"parent_collections":[],"parent_protocols":[],"peer_reviewed":false,"protocol_references":"{\"blocks\":[{\"key\":\"6l5rm\",\"text\":\"1) Subramanian B, Devishamani C, Raman R, Ratra D. Association of OCT biomarkers and visual\\nimpairment in patients with diabetic macular oedema with vitreomacular\\nadhesion. Plos one. 2023 Jul 18;18(7):e0288879.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"118ge\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"cb507\",\"text\":\"2) Duker JS, Kaiser PK, Binder S, de Smet MD, Gaudric A, Reichel E et\\nal. The International Vitreomacular Traction Study Group classification of\\nvitreomacular adhesion, traction, and macular hole. Ophthalmology 2013; 120:2611-9.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":196,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"l5rq\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8uth0\",\"text\":\"3) Jackson TL, Nicod E, Simpson A, Angelis A, Grimaccia F, Kanavos P.\\nSymptomatic vitreomacular adhesion. Retina 2013; 33:1503-11.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"21rc9\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4a4b0\",\"text\":\"4) Duker JS, Kaiser PK, Binder S, de Smet MD, Gaudric A, Reichel E et\\nal. The International Vitreomacular Traction Study Group classification of\\nvitreomacular adhesion, traction, and macular hole. Ophthalmology 2013; 120:2611-9.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":196,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"fbf7t\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"2oek8\",\"text\":\"5) Ting DS, Cheung CY, Lim G, Tan GS, Quang ND, Gan A, Hamzah H, Garcia-Franco R, San Yeo\\nIY, Lee SY, Wong EY. Development and validation of a deep learning system for\\ndiabetic retinopathy and related eye diseases using retinal images from\\nmultiethnic populations with diabetes. Jama. 2017 Dec 12;318(22):2211-23.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"5813o\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"aim1d\",\"text\":\"6) Lains I,Wang JC, Cui Y, Katz R, Vingopoulos F, Staurenghi G, Vavvas DG, Miller JW,\\nMiller JB. Retinal applications of swept source optical coherence tomography\\n(OCT) and optical coherence tomography angiography (OCTA). Progress in retinal\\nand eye research. 2021 Sep 1;84:100951.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"be8ko\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3795p\",\"text\":\"7) Schneider EW, Fowler SC. Optical coherence tomography angiography in the management of\\nage-related macular degeneration. Current opinion in ophthalmology. 2018 May\\n1;29(3):217-25.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3akak\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"72uqr\",\"text\":\"8) Corvi F, Cozzi M, Invernizzi A, Pace L, Sadda SR, Staurenghi G. Optical coherence\\ntomography angiography for detection of macular neovascularization associated\\nwith atrophy in age-related macular degeneration. Graefe's Archive for Clinical\\nand Experimental Ophthalmology. 2021 Feb;259:291-9.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"bl64t\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"bs1qi\",\"text\":\"9) LindtjÃ¸rn B, Krohn J, Forsaa VA. Optical coherence tomography features and risk of\\nmacular hole formation in the fellow eye. BMC ophthalmology. 2021\\nDec;21(1):1-7.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"b0fl9\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"aq5gp\",\"text\":\"10) Jiang F, Jiang Y, Zhi H, Dong Y, Li H, Ma S, Wang Y, Dong Q, Shen H, Wang Y. Artificial\\nintelligence in healthcare: past, present and future. Stroke and vascular\\nneurology. 2017 Dec 1;2(4).\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"cg3lr\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"46o2g\",\"text\":\"11) Gulshan V, Peng L, Coram M, Stumpe MC, Wu D, Narayanaswamy A, Venugopalan S, Widner K,\\nMadams T, Cuadros J, Kim R. Development and validation of a deep learning\\nalgorithm for detection of diabetic retinopathy in retinal fundus photographs.\\njama. 2016 Dec 13;316(22):2402-10.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"6apfr\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"d4vsa\",\"text\":\"12) Li Z, HeY, Keel S, Meng W, Chang RT, He M. Efficacy of a deep learning system for\\ndetecting glaucomatous optic neuropathy based on color fundus photographs.\\nOphthalmology. 2018 Aug 1;125(8):1199-206.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8dm93\",\"text\":\"\\n13) Burlina PM, Joshi N, Pekala M, Pacheco KD, Freund DE, Bressler NM. Automated grading of\\nage-related macular degeneration from color fundus images using deep\\nconvolutional neural networks. JAMA ophthalmology. 2017 Nov 1;135(11):1170-6.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"d365c\",\"text\":\"\\n14) Lo YC, Lin KH, Bair H, Sheu WH, Chang CS, Shen YC, Hung CL. Epiretinal membrane\\ndetection at the ophthalmologist level using deep learning of optical coherence\\ntomography. Scientific reports. 2020 May 21;10(1):8424.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"6759g\",\"text\":\"\\n15) Crincoli E, Savastano MC, Savastano A, Caporossi T, Bacherini D, Miere A, Gambini G, De\\nVico U, Baldascino A, Minnella AM, Scupola A. New artificial intelligence\\nanalysis for prediction of long-term visual improvement after epiretinal\\nmembrane surgery. Retina. 2022 Aug 17:10-97.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"c58ge\",\"text\":\"\\n\\n\\n\\n\\n\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"b4b7i\",\"text\":\"\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8o1n2\",\"text\":\"\\n\\n\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"2u3r5\",\"text\":\"\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4jq24\",\"text\":\"\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"cj8k9\",\"text\":\"\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8qnm1\",\"text\":\"\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3sbsc\",\"text\":\"\\n\\n\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"42r2a\",\"text\":\"\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{}}","public":true,"public_fork_note":"","published_on":1694520524,"references":[],"related_equipments":[],"related_materials":[],"reserved_doi":"","retraction_reason":null,"samples":{},"shared_access_id":100,"show_comparison":false,"sign_info":null,"state_version_id":1041,"stats":{"is_voted":false,"number_of_views":15,"number_of_steps":7,"number_of_bookmarks":0,"number_of_comments":0,"number_of_bookmarked_comments":0,"number_of_steps_comments":0,"number_of_protocol_comments":0,"number_of_exports":0,"number_of_runs":0,"number_of_votes":0,"number_of_reagents":0,"number_of_equipments":0,"number_of_collections":0,"number_of_forks":{"private":0,"public":0},"number_of_accessible_forks":0},"status":{"id":2,"info":"We are still developing and optimizing this protocol"},"steps":[{"id":1786749,"guid":"005489C7D8CC4B26BF4F3D9A070A8F07","previous_id":0,"previous_guid":null,"section":"\u003cp\u003eIntroduction\u003c/p\u003e","section_color":"#A492FF","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"6bqhb\",\"text\":\"VMA describes residual adhesion between the vitreous and macula, occurring within the context of an incomplete posterior vitreous detachment. It might not lead to any retinal abnormality but may exert traction on the underlying macula, distorting the retinal architecture and leading to\\nVMT. [2] VMA can be broadly classified as either focal (\\u003c1500 mm) or broad (â¥1500 mm) based on the size of the adhesion. [3] \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"sup\",\"offset\":292,\"length\":3},{\"style\":\"sup\",\"offset\":408,\"length\":4}],\"entityRanges\":[],\"data\":{}},{\"key\":\"9916v\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4b3i6\",\"text\":\"The researchers so far have concentrated mainly on the outer retina in the OCT scans to look for biomarkers to predict the visual outcomes in patients with DME. The vitreomacular interface has largely been unexplored in connection with its potential to act as a biomarker. Recently, it was shown to be of some importance in DME, where it was seen that in the presence of VMA, the OCT biomarkers such as high reflective dots, bridging processes and inner nuclear layer cysts were more likely to be associated with visual impairment than when not associated with VMA. [1] It is worthwhile to explore this further and define the role of the vitreomacular interface as a biomarker in DME.Â  \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"sup\",\"offset\":565,\"length\":4}],\"entityRanges\":[],\"data\":{}},{\"key\":\"uefg\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3d7ta\",\"text\":\"The purpose of this protocol is to design an automated deep learning model that can effectively identify the presence of VMA and categorize OCT images into broad VMA, focal VMA, and control groups, which in turn can serve as a reliable and efficient diagnostic tool. This accurate classification has the potential to significantly benefit ophthalmologists in making well-informed decisions, thereby enhancing patient care. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"73d7\",\"text\":\"\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{}}","data":null,"protocol_id":87591,"case_id":0,"critical_ids":"","duration":0,"original_id":0,"number":"1","cases":[],"critical":null},{"id":1786751,"guid":"34095764D23248158E6BE09C2A7EBF7E","previous_id":1786749,"previous_guid":"005489C7D8CC4B26BF4F3D9A070A8F07","section":"\u003cp\u003eMaterials and methods\u003c/p\u003e","section_color":"#94EBFF","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"avki2\",\"text\":\"\\nStudy design: Prospective observational study\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":1,\"length\":14}],\"entityRanges\":[],\"data\":{}},{\"key\":\"3eb6e\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"fo59e\",\"text\":\"Sample: We will use the same sample from our previous study to design\\nthe AI model. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":8}],\"entityRanges\":[],\"data\":{}},{\"key\":\"arefa\",\"text\":\"\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{}}","data":null,"protocol_id":87591,"case_id":0,"critical_ids":"","duration":0,"original_id":0,"number":"2","cases":[],"critical":null},{"id":1786758,"guid":"312C8EF0A27241C98B0AE86FF06959FB","previous_id":1786751,"previous_guid":"34095764D23248158E6BE09C2A7EBF7E","section":"\u003cp\u003eMaterials and methods\u003c/p\u003e","section_color":"#94EBFF","section_duration":0,"is_substep":true,"step":"{\"blocks\":[{\"key\":\"a9229\",\"text\":\"Collection method followed\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":26}],\"entityRanges\":[],\"data\":{}},{\"key\":\"7kp4i\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3mvnl\",\"text\":\"In our previous study we categorized participants into two groups; cases in the presence of VMA (+) and controls in the absence of VMA (-). The presence of VMA in our study was identified according to The International Vitreomacular Traction Study Group classification [4].Where the authors defined VMA according to following criteria 1) Evidence of perifoveal vitreous cortex detachment from retinal surface 2) Macular attachment of the vitreous cortex within a 3mm radius of the fovea 3) No detectable change in foveal contour or underlying retinal tissues. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"sup\",\"offset\":269,\"length\":4}],\"entityRanges\":[],\"data\":{}},{\"key\":\"24csm\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"9ok0v\",\"text\":\"We followed the same International Vitreomacular Traction Study Group classification to further subdivide VMA (+) group in our study into two groups, either focal (\\u003c1500 mm) or broad (â¥1500 mm), based on the size of the adhesion. [4] \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"sup\",\"offset\":230,\"length\":3}],\"entityRanges\":[],\"data\":{}},{\"key\":\"f886u\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"2h58j\",\"text\":\"The medical records of all individuals were reviewed for baseline demographics, including age, gender, duration of DM, DR severity, cardiovascular co morbidities, dyslipidaemia, slit-lamp bio microscopy examination, intraocular pressure, the number and type of anti-VEGF injections\\ngiven and dilated fundus evaluation. BCVA was measured with Snellen charts and\\nconverted to the Logarithm of the Minimum Angle of Resolution (Log MAR). \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"e20ov\",\"text\":\"\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"5bgdg\",\"text\":\"\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{}}","data":null,"protocol_id":87591,"case_id":0,"critical_ids":"","duration":0,"original_id":0,"number":"2.1","cases":[],"critical":null},{"id":1786759,"guid":"6E4594942AF44C3E888D08B005A5C9FE","previous_id":1786758,"previous_guid":"312C8EF0A27241C98B0AE86FF06959FB","section":"\u003cp\u003eMaterials and methods\u003c/p\u003e","section_color":"#94EBFF","section_duration":0,"is_substep":true,"step":"{\"blocks\":[{\"key\":\"embja\",\"text\":\"Inclusion and Exclusion criteria\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":32}],\"entityRanges\":[],\"data\":{}},{\"key\":\"acshl\",\"text\":\"\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"a7mpr\",\"text\":\"Inclusion and exclusion criteria will remain the same as our prior work. \",\"type\":\"align-justify\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8d9n3\",\"text\":\"\\nInclusion:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":1,\"length\":10}],\"entityRanges\":[],\"data\":{}},{\"key\":\"c7aqt\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"fk741\",\"text\":\"Subjects were included in the study if they met the following criteria: (1) Individuals (18 years or older) with type 2 diabetes mellitus and DME, (2) availability of SD-OCT scans of sufficient quality for\\ngrading, and (3) no other confounding ocular condition that could decrease visual acuity other than DME. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"f15du\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"73bml\",\"text\":\"Exclusion:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":10}],\"entityRanges\":[],\"data\":{}},{\"key\":\"9oc5\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7rh23\",\"text\":\"Subjects were excluded if they exhibit the following: (1) vitreomacular interface abnormalities besides VMA such as ERM, VMT, proliferative membranes, tractional retinal detachment, hazy media,\\nvitreous haemorrhage and lamellar or full thickness macular hole, (2) pre-existing retinal or macular disease other than DR or DME and (3) SD-OCT images with poor quality that was insufficient for assessment. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3ktog\",\"text\":\"\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{}}","data":null,"protocol_id":87591,"case_id":0,"critical_ids":"","duration":0,"original_id":0,"number":"2.2","cases":[],"critical":null},{"id":1786762,"guid":"7892ABFF9BD544FDA55E2D84167E909C","previous_id":1786759,"previous_guid":"6E4594942AF44C3E888D08B005A5C9FE","section":"\u003cp\u003eMaterials and methods\u003c/p\u003e","section_color":"#94EBFF","section_duration":0,"is_substep":true,"step":"{\"blocks\":[{\"key\":\"7jcte\",\"text\":\"Methods\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":7}],\"entityRanges\":[],\"data\":{}},{\"key\":\"9p8oi\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"50efb\",\"text\":\"1) Data processing\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"duicq\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"a4iiq\",\"text\":\"a. The dataset comprises of high quality SD-OCT tiff images from VMA Broad, VMA focal, and Control group cases, ensuring a representative sample of each class. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"es2d3\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"9u54l\",\"text\":\"b. Pre-processing steps involve noise reduction techniques, intensity normalization to address variations in brightness, and cropping to remove irrelevant regions.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"ej4mm\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"2j81f\",\"text\":\"c. Anonymization and patient privacy measures are implemented to comply with ethical guidelines and ensure confidentiality.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7if9g\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"5lsv\",\"text\":\"2) Data Annotation and Labelling\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"anj3o\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"c831e\",\"text\":\"a. In our previous work, two graders (optometrist) reviewed each OCT image and assigned appropriate labels by annotating the data and classified the data as, Broad VMA, focal VMA, and\\nControl group, based on specific diagnostic criteria. In\\ncase of discrepancies the annotations were examined by an expert (retina\\nspecialist).\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3hum4\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7k1b1\",\"text\":\"b. In this study we will be using the pre-annotated data for training the AI model. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"c4uc8\",\"text\":\"\\n3) Data Split and Validation\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"68j11\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"5t883\",\"text\":\"a. Divide the dataset into training and testing subsets to facilitate model development and evaluation.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"82ev6\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4tf9g\",\"text\":\"b. The split ratio is determined, considering factors such as dataset size, class distribution, and the need for robust performance evaluation.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8ssur\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"aa9k9\",\"text\":\"c. Employ imbalanced learning to ensure the dataset contains a proportional representation of VMA Broad, VMA focal, and Control group images, maintaining the original class distribution.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"alavu\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"30tem\",\"text\":\"4) Deep Learning Model Architecture\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"5fd7v\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"18j\",\"text\":\"a. Select different suitable deep learning architectures for the classification task, considering previous research and the complexity of the problem.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"a36ru\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"di8sn\",\"text\":\"b. Describe the chosen architectures, in detail, including the layers, activation functions, and any specific modifications made for the VMA disease classification task.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"2e3cm\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"6rmda\",\"text\":\"c. Capture relevant features from OCT images and facilitate accurate classification using the selected architectures.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"277ju\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"f3if7\",\"text\":\"5) Model Training\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"75qms\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"9gktb\",\"text\":\"a. Train the deep learning model using the labelled training subset of the dataset.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4ndn\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"ap671\",\"text\":\"b. Specify the optimization algorithm and its hyper parameters.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"42fed\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"csbr5\",\"text\":\"c. Determine the batch size, number of epochs, and early stopping criteria to prevent over fitting and achieve optimal training performance.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"rs6t\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8pjne\",\"text\":\"d. Employ data augmentation techniques to increase the dataset's diversity and enhance model generalization.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"1jvls\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"e4qrf\",\"text\":\"6) Model Evaluation and Validation\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"9lfss\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"ahk2\",\"text\":\"a. Define evaluation metrics, including accuracy, precision, recall, F1-score, and AUC to assess the model's performance.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"b37ht\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4qk5j\",\"text\":\"b. Evaluate the trained model using the testing subset to measure its classification accuracy and generalization ability.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"2hmng\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"j3os\",\"text\":\"c. Calculate performance metrics for each class separately to analyze the model's effectiveness in identifying VMA Broad, VMA focal, and Control group OCT images.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4lr3c\",\"text\":\"\\nd. Discuss potential challenges or limitations encountered during the evaluation process and highlight areas for improvement.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"1s99n\",\"text\":\"\\n7) Model Optimization and Fine-Tuning\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"6idqu\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"9lj9i\",\"text\":\"a. Outline a model optimization procedure based on the initial evaluation results.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"453jg\",\"text\":\"\\nb. Employ hyper parameter tuning, architecture modifications, or ensemble methods to improve the model's performance.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"c6c95\",\"text\":\"\\nc. Validate the optimized model using a separate testing subset to ensure unbiased assessment and verify enhanced classification accuracy.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8n2jk\",\"text\":\"\\n8) Result Analysis and Interpretation\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"f034m\",\"text\":\"\\na. Present the classification results obtained by the trained deep learning model for VMA Broad, VMA focal, and Control group OCT images.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8hmo6\",\"text\":\"\\nb. Provide in-depth analysis and interpretation of the results, identifying patterns, correlations, and potential insights into the classification performance.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4oqm4\",\"text\":\"\\nc. Compare the performance of the developed models with existing approaches or expert annotations, highlighting the advantages and limitations.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"1h22l\",\"text\":\"\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{}}","data":null,"protocol_id":87591,"case_id":0,"critical_ids":"","duration":0,"original_id":0,"number":"2.3","cases":[],"critical":null},{"id":1786766,"guid":"A13A6D3B81A74638B04ED528B101CB8D","previous_id":1786762,"previous_guid":"7892ABFF9BD544FDA55E2D84167E909C","section":"\u003cp\u003eTimeline of the study\u003c/p\u003e","section_color":"#84CE84","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"5u12j\",\"text\":\"The study is planned for a duration of 6 months. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"fi45c\",\"text\":\"\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{}}","data":null,"protocol_id":87591,"case_id":0,"critical_ids":"","duration":0,"original_id":0,"number":"3","cases":[],"critical":null},{"id":1786769,"guid":"E330D1598FDA426D806185042B97B667","previous_id":1786766,"previous_guid":"A13A6D3B81A74638B04ED528B101CB8D","section":"\u003cp\u003eDiscussion\u003c/p\u003e","section_color":"#FFED92","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"8q5os\",\"text\":\"\\nOCT has become indispensable in diagnosing various retinal conditions like DR, AMD, ERM and MH with other techniques like fundus photography and fluorescein angiography. [7-9] Deep\\nlearning (DL) a subset of machine learning (ML) is revolutionizing how we approach the diagnosis and management of medical conditions. [10]Advanced DL methods can effectively identify pathological features, and in recent years, several ML methods have emerged for recognizing OCT images in patients with significant eye disorders, including DR, AMD, ERM, glaucoma and CSCR. [11-15]\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"sup\",\"offset\":171,\"length\":6},{\"style\":\"sup\",\"offset\":317,\"length\":4},{\"style\":\"sup\",\"offset\":556,\"length\":7}],\"entityRanges\":[],\"data\":{}},{\"key\":\"3u2qt\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"9gump\",\"text\":\"The automatic detection of abnormal signs in retinal OCT images serves as a crucial component in diagnosing retinal pathologies. This capability provides ophthalmologists with valuable insights, aiding them in decision making process. In conclusion, it lays foundation for a more comprehensive and accessible approach to diagnosing and managing retinal diseases. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"dtbaj\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"48nmp\",\"text\":\"Study limitations:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"9m86u\",\"text\":\"\\n1) High-performance pre-processing is required to enhance image quality.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"esv61\",\"text\":\"2) Deep Learning (DL) models are considered \\\"black boxes\\\" where it is tough to understand how they make the predictions. This poor understanding of the models creates a trust issue among healthcare professionals and patients. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"5gcei\",\"text\":\"3) The DL model will be trained with smaller samples. Limited data availability. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4ji0i\",\"text\":\"\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"1lpu4\",\"text\":\"\\n\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{}}","data":null,"protocol_id":87591,"case_id":0,"critical_ids":"","duration":0,"original_id":0,"number":"4","cases":[],"critical":null}],"template_id":1,"title":"Protocol for Deep Learning-Based Classification of Vitreomacular Adhesion in Diabetic Macular Edema using SD-OCT.","title_html":"Protocol for Deep Learning-Based Classification of Vitreomacular Adhesion in Diabetic Macular Edema using SD-OCT.","type_id":1,"units":[{"id":1,"type_id":3,"name":"ÂµL","can_manage":0,"read_only":0},{"id":2,"type_id":3,"name":"mL","can_manage":0,"read_only":0},{"id":3,"type_id":3,"name":"L","can_manage":0,"read_only":0},{"id":4,"type_id":3,"name":"Âµg","can_manage":0,"read_only":0},{"id":5,"type_id":3,"name":"mg","can_manage":0,"read_only":0},{"id":6,"type_id":3,"name":"g","can_manage":0,"read_only":0},{"id":7,"type_id":3,"name":"kg","can_manage":0,"read_only":0},{"id":8,"type_id":3,"name":"ng","can_manage":0,"read_only":0},{"id":9,"type_id":3,"name":"Hz","can_manage":0,"read_only":0},{"id":10,"type_id":24,"name":"Â°C","can_manage":0,"read_only":0},{"id":11,"type_id":24,"name":"Â°Ð","can_manage":0,"read_only":0},{"id":12,"type_id":24,"name":"Â°F","can_manage":0,"read_only":0},{"id":13,"type_id":25,"name":"Mass Percent","can_manage":0,"read_only":0},{"id":14,"type_id":25,"name":"% volume","can_manage":0,"read_only":0},{"id":15,"type_id":25,"name":"Mass / % volume","can_manage":0,"read_only":0},{"id":16,"type_id":25,"name":"Parts per Million (PPM)","can_manage":0,"read_only":0},{"id":17,"type_id":25,"name":"Parts per Billion (PPB)","can_manage":0,"read_only":0},{"id":18,"type_id":25,"name":"Parts per Trillion (PPT)","can_manage":0,"read_only":0},{"id":19,"type_id":25,"name":"Mole Fraction","can_manage":0,"read_only":0},{"id":20,"type_id":25,"name":"Mole Percent","can_manage":0,"read_only":0},{"id":21,"type_id":25,"name":"Molarity (M)","can_manage":0,"read_only":1},{"id":22,"type_id":25,"name":"Molarity (m)","can_manage":0,"read_only":0},{"id":23,"type_id":25,"name":"Genome copies per ml","can_manage":0,"read_only":0},{"id":24,"type_id":3,"name":"Î¼V","can_manage":0,"read_only":0},{"id":25,"type_id":3,"name":"ms","can_manage":0,"read_only":0},{"id":26,"type_id":3,"name":"pg","can_manage":0,"read_only":0},{"id":27,"type_id":25,"name":"Molarity dilutions","can_manage":0,"read_only":0},{"id":28,"type_id":25,"name":"millimolar (mM)","can_manage":0,"read_only":0},{"id":29,"type_id":25,"name":"micromolar (ÂµM)","can_manage":0,"read_only":0},{"id":30,"type_id":25,"name":"nanomolar (nM)","can_manage":0,"read_only":0},{"id":31,"type_id":25,"name":"picomolar (pM)","can_manage":0,"read_only":0},{"id":32,"type_id":24,"name":"Room temperature","can_manage":0,"read_only":0},{"id":33,"type_id":30,"name":"rpm","can_manage":0,"read_only":0},{"id":34,"type_id":30,"name":"x g","can_manage":0,"read_only":0},{"id":165,"type_id":24,"name":"On ice","can_manage":0,"read_only":0},{"id":200,"type_id":32,"name":"cm","can_manage":0,"read_only":0},{"id":201,"type_id":32,"name":"mm","can_manage":0,"read_only":0},{"id":202,"type_id":32,"name":"Âµm","can_manage":0,"read_only":0},{"id":203,"type_id":32,"name":"nm","can_manage":0,"read_only":0},{"id":204,"type_id":25,"name":"mg/mL","can_manage":0,"read_only":0},{"id":205,"type_id":25,"name":"Âµg/ÂµL","can_manage":0,"read_only":0},{"id":206,"type_id":25,"name":"% (v/v)","can_manage":0,"read_only":0},{"id":1324,"type_id":30,"name":"rcf","can_manage":0,"read_only":0},{"id":1359,"type_id":35,"name":"Bar","can_manage":0,"read_only":0},{"id":1360,"type_id":35,"name":"Pa","can_manage":0,"read_only":0}],"uri":"protocol-for-deep-learning-based-classification-of-czsfx6bn","url":"https://www.protocols.io/view/protocol-for-deep-learning-based-classification-of-czsfx6bn","version_class":87591,"version_data":{"id":0,"code":"czsfx6bn","version_class":87591,"parent_id":null,"parent_uri":null,"is_same_owner":false,"is_parent_public":false,"has_pending_merge_request":false,"has_approved_merge_request":false,"merge_request":null},"version_id":0,"version_uri":"protocol-for-deep-learning-based-classification-of-4r3l2295pl1y/v1","versions":[{"id":87591,"title":"Protocol for Deep Learning-Based Classification of Vitreomacular Adhesion in Diabetic Macular Edema using SD-OCT.","title_html":"Protocol for Deep Learning-Based Classification of Vitreomacular Adhesion in Diabetic Macular Edema using SD-OCT.","image":{"source":"https://www.protocols.io/img/default_protocol.png","webp_source":null,"placeholder":"https://www.protocols.io/img/default_protocol.png","webp_placeholder":null},"doi":"dx.doi.org/10.17504/protocols.io.4r3l2295pl1y/v1","uri":"protocol-for-deep-learning-based-classification-of-czsfx6bn","published_on":1694520524,"modified_on":1694520524,"version_class":87591,"version_id":0,"version_code":"czsfx6bn","version_uri":"protocol-for-deep-learning-based-classification-of-4r3l2295pl1y/v1","created_on":1694234271,"categories":null,"type_id":1,"creator":{"name":"Rajiv Raman","affiliation":null,"affiliation_url":null,"username":"rajiv-raman","link":"","image":{"source":"/img/avatars/005.png","placeholder":"/img/avatars/005.png","webp_source":""}},"stats":{"number_of_comments":0,"last_comment_time":0}}],"warning":""}