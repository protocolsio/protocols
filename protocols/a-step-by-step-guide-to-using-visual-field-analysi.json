{"id":39029,"title":"A step by step guide  to using Visual Field Analysis","title_html":"<p>A step by step guide  to using Visual Field Analysis<\/p>","image":{"source":"https:\/\/s3.amazonaws.com\/protocols-files\/public\/2f2d94c63d089f7d1ba78298481ad985c67c82c3c5c32cc2e405ee031563bbac\/cei49ys6.png","placeholder":"https:\/\/s3.amazonaws.com\/protocols-files\/public\/2f2d94c63d089f7d1ba78298481ad985c67c82c3c5c32cc2e405ee031563bbac\/cei49ys6.png"},"doi":"dx.doi.org\/10.17504\/protocols.io.bicvkaw6","doi_status":2,"uri":"a-step-by-step-guide-to-using-visual-field-analysi-bicvkaw6","type_id":4,"template_id":3,"published_on":1596124506,"parent_protocols":[],"parent_collections":[],"cited_protocols":[],"version_id":0,"created_on":1594221483,"categories":null,"creator":{"name":"Bastien S. Lemaire","affiliation":"University of Trento, Center for Mind\/Brain Sciences","affiliations":[{"affiliation":"University of Trento, Center for Mind\/Brain Sciences","url":"https:\/\/www.cimec.unitn.it\/en","is_default":1}],"username":"bastien-lemaire","link":null,"image":{"source":"https:\/\/s3.amazonaws.com\/protocols-files\/public\/2f2d94c63d089f7d1ba78298481ad985c67c82c3c5c32cc2e405ee031563bbac\/cd6k9ys6.jpg","placeholder":"https:\/\/s3.amazonaws.com\/protocols-files\/public\/2f2d94c63d089f7d1ba78298481ad985c67c82c3c5c32cc2e405ee031563bbac\/cd6k9ys6.jpg"},"badges":[{"id":2,"image":{"source":"\/img\/badges\/bronze.svg","placeholder":"\/img\/badges\/bronze.svg"},"name":"Author"}],"verified":0,"is_verified_user":false,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},"journal":null,"journal_name":null,"journal_link":null,"article_citation":null,"public":1,"has_versions":0,"link":null,"total_collections":0,"number_of_steps":0,"authors":[{"name":"Mathilde Josserand","affiliation":"Ecole Normale Sup\u00e9rieure Lyon","affiliations":[],"username":null,"link":null,"image":{"source":null,"placeholder":null},"badges":[],"verified":0,"is_verified_user":false,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},{"name":"Bastien Lemaire","affiliation":"University of Trento, Center for Mind\/Brain Sciences","affiliations":[],"username":"bastien-lemaire","link":null,"image":{"source":"https:\/\/s3.amazonaws.com\/protocols-files\/public\/2f2d94c63d089f7d1ba78298481ad985c67c82c3c5c32cc2e405ee031563bbac\/cd6k9ys6.jpg","placeholder":"https:\/\/s3.amazonaws.com\/protocols-files\/public\/2f2d94c63d089f7d1ba78298481ad985c67c82c3c5c32cc2e405ee031563bbac\/cd6k9ys6.jpg"},"badges":[],"verified":1,"is_verified_user":true,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false}],"versions":[],"groups":[],"is_owner":1,"has_subprotocols":0,"is_subprotocol":0,"is_bookmarked":0,"can_claim_authorship":0,"can_accept_authorship":0,"can_be_copied":1,"can_remove_fork":1,"fork_id":null,"url":"https:\/\/www.protocols.io\/view\/a-step-by-step-guide-to-using-visual-field-analysi-bicvkaw6","forks_count":{"private":0,"public":0},"access":{"can_view":1,"can_remove":0,"can_add":0,"can_edit":0,"can_publish":0,"can_get_doi":0,"can_share":1,"can_move":1,"can_move_outside":1,"can_transfer":1,"can_download":1,"is_locked":0},"guid":"249878D0C12E11EA8BFE5B3598B1F4B8","state_version_id":1142,"steps":[],"document":"<div class = \"text-blocks\"><div class = \"text-block\"><h1><\/h1><\/div><div class = \"text-block\"><h1><span style = \"text-align:center;\">A step by step guide<\/span><\/h1><\/div><div class = \"text-block\"><h1><span style = \"text-align:center;\"> <\/span><\/h1><\/div><div class = \"text-block\"><h1><span style = \"text-align:center;\">to using Visual Field Analysis<\/span><\/h1><\/div><div class = \"text-block\"><div class = \"justify\" style = \"text-align:justify\"><\/div><style>\n\t\t\t\t\t\t\t  .justify:after {\n\t\t\t\t\t\t\t    content: \"\";\n\t\t\t\t\t\t\t    display:inline-block;\n\t\t\t\t\t\t\t    width: 100%;\n\t\t\t\t\t\t\t  }\n\t\t\t\t\t\t\t<\/style><\/div><div class = \"text-block\"><div class = \"justify\" style = \"text-align:center\">In this protocol, we provide a step by step guide to using Visual Field Analysis (VFA) successfully. VFA is a python program based on DeepLabCut toolbox (Nath et al., 2019). Using VFA, it is possible to score reliably the eye use, activity, and time spent in different zones of different animal species and experimental paradigms for more reproducible research.<\/div><\/div><div class = \"text-block\"><div class = \"justify\" style = \"text-align:left\"><\/div><\/div><div class = \"text-block\"><div class = \"justify\" style = \"text-align:left\"><span>                                      <\/span><span style = \":justify;font-weight:bold;\">1. Requirements<\/span><\/div><\/div><div class = \"text-block\"><span style = \"font-weight:bold;:justify;\">Video<\/span><\/div><div class = \"text-block\"><span style = \":justify;\">Our method is entirely based on video recordings. Therefore, a good quality recording is required. However, the highest resolution and frame rate do not necessarily provide the most accurate results. The best settings are specific to the experimental condition and DeepLabCut process (Nath et al. 2019). The camera choice and its settings can be manipulated depending on experimental conditions and animal models. If working with very fast-moving animals, we recommend using a higher frame rate. Even though the camera choice and recording settings can be modified at will, there are specific parameters that are essentials and must be strictly followed.<\/span><\/div><div class = \"text-block\"><span style = \":justify;\">List of requirements for the videos recordings: <\/span><\/div><div class = \"text-block\"><span style = \":justify;\">- the video must be encapsulated as .avi or .mp4;<\/span><\/div><div class = \"text-block\"><span style = \":justify;\">- the camera should not move during the acquisition;<\/span><\/div><div class = \"text-block\"><span style = \":justify;\">- the video recording should be taken from above with no distortions (avoid fisheye lenses), to get accurate measurements of the eye-use;<\/span><\/div><div class = \"text-block\"><span style = \":justify;\">- the apparatus and the stimuli must be already visible at the beginning of each analyzed video recording, to accurately set the arena borders and stimuli location (2. 'data processing', step 1);<\/span><\/div><div class = \"text-block\"><span style = \":justify;\">- the stimuli observed by the subjects should always be visible on the video recording, to allow measurements of the eye-use.<\/span><\/div><div class = \"text-block\"><span style = \":justify;\">- the framerate used to record videos must be an integer: if you need to have a number of framerate with decimals (7.5 f\/s), then you could not select the option \"gather by seconds\" later.<\/span><\/div><div class = \"text-block\"><span style = \"font-weight:bold;:justify;\">DeepLabCut<\/span><\/div><div class = \"text-block\"><span style = \":justify;\">Video recordings must be tracked using DeepLabCut toolbox. During stage II of the DeepLabCut process (Nath et al. 2019) the head areas (called body parts in DeepLabCut) must be named as follows: \u2018leftHead\u2019, \u2018topHead\u2019, and \u2018rightHead\u2019 (Figure 1A). If the stimuli observed by the animals are moving objects, they should be tracked using DeepLabCut too. If only one stimulus is present, it should be named \u2018stimulus\u2019 (Figure 1B). In this case, the stimuli should be named \u2018stimulusLeft\u2019, \u2018stimulusRight\u2019 in a left\/right arena configuration, or \u2018stimulusTop\u2019, \u2018stimulusBottom\u2019 in a top\/bottom arena configuration (Figure 1C, information about the arena configuration is provided in 2. 'complete launcher information', step 14). <\/span><span><\/span><\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/protocols-files\/public\/2f2d94c63d089f7d1ba78298481ad985c67c82c3c5c32cc2e405ee031563bbac\/ceh69ys6.png\" \/><\/div><div class = \"text-block\"><span style = \":justify;\">During stage IV (\u2018labelling of the frames\u2019) of the DeepLabCut process, a certain amount of frames must be labelled by the experimenter indicating the position of the three points previously defined during stage II (200 frames of different situations labelled manually usually provide accurate results, Nath et al. 2019). The accuracy of this manual labelling step is crucial to obtain accurate tracking. If in these frames, the eyes are not visible, we suggest placing the labels as close as the real eye positions, to obtain the most accurate tracking (Figure 2).<\/span><span><\/span><\/div><div class = \"text-block\"><span style = \":;:justify;\"> <\/span><a href=\"#\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;:justify;\"> <\/span><\/a><span style = \":justify;\">The \u2018leftHead\u2019 label must be placed on the left eye or as close as possible to it, while the \u2018rightHead\u2019 label should be located on the right eye or as close as possible to it. The \u2018topHead\u2019 label must be placed between these two points, but in a slightly more rostral position (as in Figure 3). The labels must represent the head orientation of the animal correctly.<\/span><span><\/span><\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/protocols-files\/public\/2f2d94c63d089f7d1ba78298481ad985c67c82c3c5c32cc2e405ee031563bbac\/ceia9ys6.png\" \/><\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/protocols-files\/public\/2f2d94c63d089f7d1ba78298481ad985c67c82c3c5c32cc2e405ee031563bbac\/ceic9ys6.png\" \/><\/div><div class = \"text-block\"><span style = \":justify;\">During stage IX of the DeepLabCut process, the command \u2018save_as_csv=True\u2019 must be added in the \u2018deeplabcut.analyse_videos\u2019 function, to obtain files with the right extension (.csv, Figure 4).<\/span><span><\/span><\/div><div class = \"text-block\"><a href=\"#\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;:justify;\"> <\/span><\/a><span style = \":;:justify;font-weight:bold;\"> <\/span><span style = \":justify;font-weight:bold;\">Excel<\/span><\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/protocols-files\/public\/2f2d94c63d089f7d1ba78298481ad985c67c82c3c5c32cc2e405ee031563bbac\/ceie9ys6.png\" \/><\/div><div class = \"text-block\"><span style = \":justify;\">Every sheet should contain at least the animal identification number (all animals should have a different number) and the starting\/ending time (filed in the following format: hh:mm:ss) of the portion of the video to be analyzed. Visual Field Analysis will analyze only the frames contained between the starting and the ending time located in the excel sheet. New columns can also be added to incorporate additional variables that the experimenter wishes to record about each subject or trial (Figure 5). This information will then be automatically copied within the output files produced by Visual Field Analysis.<\/span><\/div><div class = \"text-block\"><span style = \":justify;\">If the same animal is tested several times and the different trials are recorded on separate videos, we suggest using for each video a decimal animal identification number corresponding to the id of the animal and the trial number (such as 1.1, 1.2, etc.). If the trials are recorded as one continuous video, we suggest indicating the starting time of the first trial and the ending time of the last trial, so that Visual Field Analysis will analyze all the trials at once. The output produced by our application can then be easily divided manually or computationally into different sessions.<\/span><\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/protocols-files\/public\/2f2d94c63d089f7d1ba78298481ad985c67c82c3c5c32cc2e405ee031563bbac\/ceig9ys6.png\" \/><\/div><div class = \"text-block\"><span style = \":justify;\">                    <\/span><span style = \":justify;font-weight:bold;\">2. Visual Fields AnalysisInstallation'<\/span><\/div><div class = \"text-block\"><div class = \"justify\" style = \"text-align:left\"><span>To run our application, the \u2018Visual Field Analysis\u2019 must be downloaded on our <\/span><a href=\"https:\/\/github.com\/mathjoss\/VisualFieldsAnalysis\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">GitHub<\/span><\/a><span> (<\/span><a href=\"https:\/\/github.com\/mathjoss\/VisualFieldsAnalysis\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">mathjoss\/VisualFieldsAnalysis<\/span><\/a><span>). Visual Field Analysis is based on Python 3. We suggest installing Anaconda (version 1.9.2 or latest) with the latest Python version (3.7 at the moment) and Spyder (version 3.3.6). Furthermore, the following libraries must be installed: pandas, matplotlib, cv2, NumPy and xlrd.<\/span><\/div><\/div><div class = \"text-block\"><div class = \"justify\" style = \"text-align:left\">We used a lot of different versions which all worked. OpenCV works with version 3.4.2, and so does matplotlib with version 2.0.2. <\/div><\/div><div class = \"text-block\"><div class = \"justify\" style = \"text-align:left\"><span>To run our example, the \u2018Example\u2019 folder within our <\/span><a href=\"https:\/\/github.com\/mathjoss\/VisualFieldsAnalysis\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">GitHub<\/span><\/a><span> can be downloaded, and the next steps followed.<\/span><\/div><\/div><div class = \"text-block\"><span style = \"font-weight:bold;:justify;\">Files and folders naming<\/span><\/div><div class = \"text-block\"><div class = \"justify\" style = \"text-align:justify\"><span>For each experiment, <\/span><a href=\"\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">we advise creating different directories (see \u2018Example\u2019 architecture in our <\/span><\/a><a href=\"https:\/\/github.com\/mathjoss\/VisualFieldsAnalysis\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">GitHub<\/span><\/a><span>) where to locate the input files (input 1, 2, labelled and 3 should be located in different directories). <\/span><\/div><style>\n\t\t\t\t\t\t\t  .justify:after {\n\t\t\t\t\t\t\t    content: \"\";\n\t\t\t\t\t\t\t    display:inline-block;\n\t\t\t\t\t\t\t    width: 100%;\n\t\t\t\t\t\t\t  }\n\t\t\t\t\t\t\t<\/style><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Videos<\/span><span style = \":justify;\">. We recommend gathering all videos in a folder. The videos can be named with any name as soon as this name is coherent within videos. Only a number corresponding to the animal identification number can vary between the different videos. For example, videos can be named \u201cmychick1.mp4\u201d, \u201cmychick4.mp4\u201d, \u201cmychick1.3.mp4\u201d, but should not be named \u201cfirstchick4.mp4\u201d, \u201csecondchick1.3.mp4\u201d.<\/span><span><\/span><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">DeepLabCut files<\/span><span style = \"font-style:italic;:justify;font-weight:bold;\">.<\/span><span style = \":justify;\"> Each video is linked to its DeepLabCut files. DeepLabCut files must be gathered in a folder. However, the files\u2019 names must be created according to the following conditions: first, the animal name, second, the animal identification number, and third \u201c_dlc.csv\u201d. In our example, our files name are \u201cchick1_dlc.csv\u201d. Only the identification number must vary between the videos. Files must always follow this format: \u201canimalNUMBER_dlc.csv\u201d.<\/span><span><\/span><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Excel file.<\/span><span style = \":justify;\"> The name of the excel file does not follow any specific constraints. However, the name of the sheets inside the excel file must correspond to the animal name and its identification number. In our example, our sheets must be called \u201cchick1\u201d and \u201cchick2\u201d. It is impossible to have alternative names for the animal (\u201cfirstchick1\u201d, \u201cfirstchich2\u201d will not work).<\/span><span><\/span><\/div><div class = \"text-block\"><span style = \"font-weight:bold;:justify;\">Complete launcher information<\/span><\/div><div class = \"text-block\"><div class = \"justify\" style = \"text-align:left\"><span>To start the application, open \u2018main_coordinator.py\u2019 using Spyder (located within the <\/span><a href=\"https:\/\/github.com\/mathjoss\/VisualFieldsAnalysis\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">GitHub<\/span><\/a><span> directory downloaded previously). The program will open an interface where information for the experiment can be selected (Figure 6). In the next paragraph, we will illustrate every step of this procedure, referring again to the experiment we performed as an example.<\/span><\/div><\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/protocols-files\/public\/2f2d94c63d089f7d1ba78298481ad985c67c82c3c5c32cc2e405ee031563bbac\/ceii9ys6.png\" \/><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Step 1:<\/span><span style = \":justify;\"> Write the animal type. This name should be coherent with the name of your animal written in your excel sheets and DeepLabCut files. In our case, the input names all started with \u2018chick\u2019, so this is what we entered into the program.<\/span><span><\/span><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Step 2:<\/span><span style = \":justify;\"> Browse the folder where you stored the DeepLabCut output files.<\/span><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Step 3:<\/span><span style = \":justify;\"> Browse the folder where you stored the videos.<\/span><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Step 4 and 5:<\/span><span style = \":justify;\"> If the program has worked, but you could not visualize frames, reduce the size of the videos by two and store the new videos in a folder. Then, answer \u201cyes\u201d to this question and browse the folder with the reduced videos. Please note that in step 3, you should still enter the folder with the non-reduced videos.<\/span><span><\/span><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Step 6:<\/span><span style = \":justify;\"> Enter the format of the video name and replace the number by %s. For example, if your videos are named \u201cmyfish1.1.mp4\u201d and \u201cmyfish1.2.mp4\u201d, you should write \u201cmyfish%s.mp4\u201d.<\/span><span><\/span><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Step 7:<\/span><span style = \":justify;\"> Browse the location of the excel file. Please note that the file itself must be selected, and not the folder like in the previous steps.<\/span><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Step 8:<\/span><span style = \":justify;\"> Select if the apparatus has a top\/bottom or left\/right orientation. In our chick 1 example, the stimulus is located on the top of the video recording, so we chose the top\/bottom configuration. For our chick 2 example, we selected left\/right configuration.<\/span><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Step 9 and 10:<\/span><span style = \":justify;\"> Indicate the number of stimuli and if they are moving or static. Visual Field Analysis allows to track simultaneously two stimuli only if they are located on the two opposite sides of the arena (top and bottom or left and right). It is impossible to simultaneously track more than one moving stimulus if they are located on the same side of the arena. <\/span><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Step 11:<\/span><span style = \":justify;\"> Choose whether to group the results by second or to perform a frame by frame analysis. <\/span><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Step 12: <\/span><span style = \":justify;\">Specify the animals\u2019 identification number to analyze. In our example, we only tracked one animal, but the program can analyze multiple animals at a time. <\/span><span><\/span><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Step 13:<\/span><span style = \":justify;\"> Define an error \u2018threshold\u2019. This threshold specifies the acceptable level of between-frames variability in the distances between the three points tracked on the head of the animal. In some cases, DeepLabCut may track some frames inaccurately. Consequently, we created an error threshold to help to exclude frames in which tracking accuracy was low. Later in the program, the distance between each tracked body part (\u2018leftHead\u2019 to \u2018rightHead\u2019, \u2018leftHead\u2019 to \u2018topHead\u2019, and \u2018rightHead\u2019 to \u2018topHead\u2019) will be computed. This allows us to check for potential errors in the DeepLabCut file. In our example, the average distance between the \u2018leftHead\u2019 and \u2018rightHead\u2019 points was of 58 pixels. If in other frames the distance between these same two points is 20 or 80 pixels, it is highly probable that the labels have been wrongly located. These frames should be considered as outliers and excluded from the results. Through the threshold choice, the program will identify potential outliers. A higher threshold will allow a greater variability and will include a higher number of frames into the analysis. This means that we will keep in the analysis frames for which the inter-points distance shows a substantial variation from the average value obtained for that video. In contrast, a lower threshold will be more restrictive and lead to a high number of frames excluded as outliers. The value for this threshold can be as low as 0, which will exclude any variability in the tracking. To choose the best threshold, outlier frames can be visualized later in the program. This allows verifying if the excluded frames did represent cases of inaccurate tracking (Figure 10). It is thus advisable to initially set a lower threshold and proceed to increase it, until only inaccurately tracked frames are excluded. After visualization of the excluded frames, the threshold can be changed. A threshold of 3 has been found to be effective in most of the tests conducted in our laboratory.<\/span><span><\/span><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Step 14:<\/span><span style = \":justify;\"> Select the number of areas in which the arena should be divided and their lengths (the arena cannot be divided into more than five areas). This information will be later used to determine the time spent in different areas. The different areas must form virtual rectangles of identical width, juxtaposed side by side along with the arena (Figure 7). The overall length of all areas equals the arena length (Figure 7). For example, to divide the arena into three areas (left\/top, center, right\/bottom), the length of area1 and area5 should be given a value of 0, while areas2, 3, and 4 must be given a different length. We advise setting the length of each area so that it corresponds to its actual size in cm. In our example with chick 1, we were not interested in computing the time spent by the chick in different areas, since the animal was immobile. Consequently, we attributed a value of 20 (corresponding to the length in cm of the arena drawn later in the program, see stage 3, step 1) to the length of the center zone (area3), while the lengths of all the other areas were attributed a value of 0.<\/span><\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/protocols-files\/public\/2f2d94c63d089f7d1ba78298481ad985c67c82c3c5c32cc2e405ee031563bbac\/ceik9ys6.png\" \/><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Step 15: <\/span><span style = \":justify;\">Define the size of each portion of the visual field. Visual Field Analysis will automatically score which hemifield is predominantly used to look at a stimulus. This will be done using projection lines from the head\u2019s \u2018midline\u2019. The midline is perpendicular to the imaginary line connecting the label points corresponding to the left eye (leftHead) and the right eye (rightHead, Figure 8). At this stage, two visual fields within each hemifield should be defined: the frontal and lateral visual fields. The angle from the midline must be defined for the left visual fields (the sum of the frontal and lateral visual hemifields must be 180\u00b0 maximum). The program will automatically apply the same values to the right hemifield. The value entered for each visual field corresponds to the angle starting from the midline, which is considered as 0\u00b0.In our example, we defined the left frontal visual field as 15\u00b0 wide from the midline (30\u00b0 in total for the sum of the left and right frontal visual fields), while the lateral visual field was defined as ending at 150\u00b0 wide from the midline (135\u00b0 for each lateral visual field, excluding the width of the frontal visual field, Figure 8). Values from 150\u00b0 to 180\u00b0 in each hemifield will automatically be defined as the blind spot of the animal. <\/span><span><\/span><\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/protocols-files\/public\/2f2d94c63d089f7d1ba78298481ad985c67c82c3c5c32cc2e405ee031563bbac\/cein9ys6.png\" \/><\/div><div class = \"text-block\"><span style = \"font-weight:bold;:justify;\">Data processing<\/span><\/div><div class = \"text-block\"><span style = \":justify;\">The program performs the computations that are specific to every video, to measure the behaviors accurately. <\/span><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Step 1:<\/span><span style = \":justify;\"> Specify the borders of the arena and the position of the stimuli. These data must be defined separately for every video. Indeed, the camera may accidentally move from one video to another, slightly altering the position of the arena and stimuli, from one video to another. <\/span><\/div><div class = \"text-block\"><span style = \":justify;\">At this step, the program opens the first frame of each video recording, for each animal analyzed. To specify the arena borders, the user needs to mark the four corners of the arena. Similarly, for tests with static stimuli, also the stimuli location needs to be defined at this stage. This can be done by marking two points on its borders. (The position of moving stimuli will be tracked by DeepLabCut, see above). The instructions should be followed and the borders placed using the left click of the mouse. Important: the border of the arena should include all possible positions where the animal can be tracked on the video, which can depend on the camera angle (Figure 9). The total length of the arena (Figure 7) corresponds to the distance top\/bottom or left\/right, depending on the arena orientation.<\/span><\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/protocols-files\/public\/2f2d94c63d089f7d1ba78298481ad985c67c82c3c5c32cc2e405ee031563bbac\/ceiq9ys6.png\" \/><\/div><div class = \"text-block\"><span style = \":justify;\">The precise location of the borders is particularly important to compute how much time the animal spends in different areas fo the arena (Figure 7) but does not matter if you are only interested in the visual fields used by the animal to look toward a stimulus.<\/span><\/div><div class = \"text-block\"><span style = \":justify;\">This step creates the first output file, which will automatically be generated within a new directory called \u2018files\u2019, located inside the folder where DeepLabCut files are stored. This output contains the information provided in input 3 and the exact positions (in pixels) of the borders defined at this step. The location of the borders will be used in step 3 to assess the time spent in different areas. The location of the stimuli will then be used in step 4 to assess eye-use. <\/span><span><\/span><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Step 2:<\/span><span style = \":justify;\"> Check for outliers. At this step, the program checks for errors and computes the distance between pairs of labels. Doing so, it selects outlier frames according to the threshold previously indicated. In our example, we chose a threshold of 3. With this threshold, only 1.33% of the frames were counted as outliers and excluded from the analysis. These frames can be visualized to address the accuracy of this process. In Figure 10, we report two frames that were removed in this step. In both cases, we can see that the labels were not correctly placed on the head of the animal. If the selected threshold is not satisfying, it should be changed running the program again. With this procedure, it is possible to find the most appropriate threshold to each experimental condition. <\/span><\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/protocols-files\/public\/2f2d94c63d089f7d1ba78298481ad985c67c82c3c5c32cc2e405ee031563bbac\/ceis9ys6.png\" \/><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Step 3: <\/span><span style = \":justify;\">The program also automatically excludes the frames from the analysis where the likelihood ratio reported in the DeepLabCut tracking file (Figure 4) is lower than 0.9. If the option selected is \u201cmoving stimulus tracked by DeepLabCut\u201d, all frames where the stimulus is absent are included inside this percentage. At the end of the analysis, the percentage of frames excluded due to this criterion will be reported in the analysis output, with and without the stimuli outliers. If too many frames are excluded, it is probably better to improve the DeepLabCut tracking by refining the labels (see DeepLabCut process, Nath et al. 2019) and\/or optimizing the video recordings (modifying brightness or contrast for example).<\/span><span><\/span><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Step 4: <\/span><span style = \":justify;\">At this stage, the program offers you the possibility to visualize the projection lines of the visual fields on random frames, to control that the program works appropriately and accurately (Figure 11). Using the visual fields defined previously (Figure 7) and the location of stimuli, the program will assess in which hemifield the stimuli fall in each frame. Besides, the software will also assess whether the stimuli fall in the frontal or lateral portion of each hemifield. If the stimulus(i) is located within a visual field, a value of 1 will be attributed to it (see the light-green dash line on Figure 11.A). If the stimulus is straddling on two visual fields, the proportion of the object located within each visual field is attributed to each one of them (see the light-green dash line on Figure 11.B). The output for eye-use data varies depending on the location and number of stimuli. If there is one stimulus on each side of the arena, Visual Field Analysis computes eye-use for both stimuli. However, if there is only one stimulus on one side of the arena, Visual Field Analysis computes eye-use for this stimulus, but also fo the corresponding empty spot on the opposite side of the arena (see the dark-green dash lines in Figure 11). This data can be used as a control representing eye-use to monitor a neutral region of space or simply disregarded. <\/span><\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/protocols-files\/public\/2f2d94c63d089f7d1ba78298481ad985c67c82c3c5c32cc2e405ee031563bbac\/ceiu9ys6.png\" \/><\/div><div class = \"text-block\"><span style = \"font-style:italic;:justify;\">Step 5:<\/span><span style = \":justify;\"> For each video, the program produces a second output, located within a new directory called \u2018results\u2019 which is inside the folder where DeepLabCut output files are stored. This output includes all the information specified within the input 3 and the behavioral measurements obtained by Visual Field Analysis (see Figure 12).<\/span><span><\/span><\/div><div class = \"text-block\"><ul style = \"list-style-type:disc;\"><li style = \"counter-reset:ol0;list-style-type:disc;\"><span style = \":justify;\">The first columns of this output correspond to the columns contained in input 3.<\/span><\/li><li style = \"counter-reset:ol0;list-style-type:disc;\"><\/li><li style = \"counter-reset:ol0;list-style-type:disc;\"><span style = \":justify;\">The following column, named \u2018distanceMoved\u2019 provides information concerning <\/span><span style = \":justify;font-weight:bold;\">the activity level of the animal,<\/span><span style = \":justify;\"> operationalized as the total distance covered of the \u2018topHead\u2019 label. In our example, this measurement gives direct information about head movements done by the chick, since it was standing in a fixed position and only its head was moving. Instead, if the animal observed is moving freely within an arena, this value corresponds to the distance moved by the animal in the environment, plus its head movements.<\/span><span><\/span><\/li><li style = \"counter-reset:ol0;list-style-type:disc;\"><\/li><li style = \"counter-reset:ol0;list-style-type:disc;\"><span style = \":justify;\">The next five columns provide <\/span><span style = \":justify;font-weight:bold;\">the time spent<\/span><span style = \":justify;\"> in each area frame by frame (Figure 12A) or by seconds (Figure 12B) depending on your previous choice. In our main example, this information is not meaningful since the animal could not move across the arena, which was thus not subdivided into different areas. In Figure 12.B, we provide an additional example of this kind of data. To do so we report the output produced by Visual Field Analysis for one chick tested for its preference for two moving stimuli placed at the opposite ends of an arena, subdivided into five zones defined as in Figure 7.<\/span><\/li><li style = \"counter-reset:ol0;list-style-type:disc;\"><\/li><li style = \"counter-reset:ol0;list-style-type:disc;\"><span style = \":justify;\">The following columns provide <\/span><span style = \":justify;font-weight:bold;\">eye-use <\/span><span style = \":justify;\">measurements<\/span><span style = \":justify;font-weight:bold;\">. <\/span><span style = \":justify;\">Depending on the orientation of the apparatus (left\/right or top\/bottom) the column names will be different. The last word of the column name always indicates the position of the stimulus. In our main example, we had only one stimulus located on the top so we should focus only on the columns having the extension \u2018top\u2019 written at the end of the column name (columns with the extension \u2018bottom\u2019 should be ignored in this case, since they refer to the \u2018ghost stimulus\u2019 located at the bottom of the apparatus and are not meaningful in this context). In Figure 12, values above zero in \u2018frontaltop\u2019 indicate that the stimulus (or part of it) was located in the frontal field of the animal. Values above zero in \u2018blindtop\u2019 indicate that the stimulus was not seen by the animal. Values above zero in \u2018lateralLefttop\u2019 and \u2018lateralRighttop\u2019 indicate that the stimulus was located within the left or right lateral visual field of the animal, respectively. Values in \u2018leftALLtop\u2019, \u2018rightALLtop\u2019 indicate which hemifield was used to look at the stimulus (frontal and lateral visual fields pulled together, see Figure 12.A).<\/span><span><\/span><\/li><\/ul><\/div><div class = \"text-block\"><ul style = \"list-style-type:disc;\"><li style = \"counter-reset:ol0;list-style-type:disc;\"><span style = \":justify;\">The following column, named \u2018distanceMoved\u2019 provides information concerning <\/span><span style = \":justify;font-weight:bold;\">the activity level of the animal,<\/span><span style = \":justify;\"> operationalized as the total distance covered of the \u2018topHead\u2019 label. In our example, this measurement gives direct information about head movements done by the chick, since it was standing in a fixed position and only its head was moving. Instead, if the animal observed is moving freely within an arena, this value corresponds to the distance moved by the animal in the environment, plus its head movements.<\/span><span><\/span><\/li><li style = \"counter-reset:ol0;list-style-type:disc;\"><\/li><li style = \"counter-reset:ol0;list-style-type:disc;\"><span style = \":justify;\">The next five columns provide <\/span><span style = \":justify;font-weight:bold;\">the time spent<\/span><span style = \":justify;\"> in each area frame by frame (Figure 12A) or by seconds (Figure 12B) depending on your previous choice. In our main example, this information is not meaningful since the animal could not move across the arena, which was thus not subdivided into different areas. In Figure 12.B, we provide an additional example of this kind of data. To do so we report the output produced by Visual Field Analysis for one chick tested for its preference for two moving stimuli placed at the opposite ends of an arena, subdivided into five zones defined as in Figure 7.<\/span><\/li><li style = \"counter-reset:ol0;list-style-type:disc;\"><\/li><li style = \"counter-reset:ol0;list-style-type:disc;\"><span style = \":justify;\">The following columns provide <\/span><span style = \":justify;font-weight:bold;\">eye-use <\/span><span style = \":justify;\">measurements<\/span><span style = \":justify;font-weight:bold;\">. <\/span><span style = \":justify;\">Depending on the orientation of the apparatus (left\/right or top\/bottom) the column names will be different. The last word of the column name always indicates the position of the stimulus. In our main example, we had only one stimulus located on the top so we should focus only on the columns having the extension \u2018top\u2019 written at the end of the column name (columns with the extension \u2018bottom\u2019 should be ignored in this case, since they refer to the \u2018ghost stimulus\u2019 located at the bottom of the apparatus and are not meaningful in this context). In Figure 12, values above zero in \u2018frontaltop\u2019 indicate that the stimulus (or part of it) was located in the frontal field of the animal. Values above zero in \u2018blindtop\u2019 indicate that the stimulus was not seen by the animal. Values above zero in \u2018lateralLefttop\u2019 and \u2018lateralRighttop\u2019 indicate that the stimulus was located within the left or right lateral visual field of the animal, respectively. Values in \u2018leftALLtop\u2019, \u2018rightALLtop\u2019 indicate which hemifield was used to look at the stimulus (frontal and lateral visual fields pulled together, see Figure 12.A).<\/span><span><\/span><\/li><\/ul><\/div><div class = \"text-block\"><ul style = \"list-style-type:disc;\"><li style = \"counter-reset:ol0;list-style-type:disc;\"><span style = \":justify;\">The next five columns provide <\/span><span style = \":justify;font-weight:bold;\">the time spent<\/span><span style = \":justify;\"> in each area frame by frame (Figure 12A) or by seconds (Figure 12B) depending on your previous choice. In our main example, this information is not meaningful since the animal could not move across the arena, which was thus not subdivided into different areas. In Figure 12.B, we provide an additional example of this kind of data. To do so we report the output produced by Visual Field Analysis for one chick tested for its preference for two moving stimuli placed at the opposite ends of an arena, subdivided into five zones defined as in Figure 7.<\/span><\/li><li style = \"counter-reset:ol0;list-style-type:disc;\"><\/li><li style = \"counter-reset:ol0;list-style-type:disc;\"><span style = \":justify;\">The following columns provide <\/span><span style = \":justify;font-weight:bold;\">eye-use <\/span><span style = \":justify;\">measurements<\/span><span style = \":justify;font-weight:bold;\">. <\/span><span style = \":justify;\">Depending on the orientation of the apparatus (left\/right or top\/bottom) the column names will be different. The last word of the column name always indicates the position of the stimulus. In our main example, we had only one stimulus located on the top so we should focus only on the columns having the extension \u2018top\u2019 written at the end of the column name (columns with the extension \u2018bottom\u2019 should be ignored in this case, since they refer to the \u2018ghost stimulus\u2019 located at the bottom of the apparatus and are not meaningful in this context). In Figure 12, values above zero in \u2018frontaltop\u2019 indicate that the stimulus (or part of it) was located in the frontal field of the animal. Values above zero in \u2018blindtop\u2019 indicate that the stimulus was not seen by the animal. Values above zero in \u2018lateralLefttop\u2019 and \u2018lateralRighttop\u2019 indicate that the stimulus was located within the left or right lateral visual field of the animal, respectively. Values in \u2018leftALLtop\u2019, \u2018rightALLtop\u2019 indicate which hemifield was used to look at the stimulus (frontal and lateral visual fields pulled together, see Figure 12.A).<\/span><span><\/span><\/li><\/ul><\/div><div class = \"text-block\"><ul style = \"list-style-type:disc;\"><\/ul><\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/protocols-files\/public\/2f2d94c63d089f7d1ba78298481ad985c67c82c3c5c32cc2e405ee031563bbac\/ceiw9ys6.png\" \/><\/div><div class = \"text-block\"><div class = \"justify\" style = \"text-align:left\"><span>Our application is entirely open-source and freely available on <\/span><a href=\"https:\/\/github.com\/mathjoss\/VisualFieldsAnalysis\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">GitHub<\/span><\/a><span style = \":;\">.<\/span><\/div><\/div><div class = \"text-block\"><div class = \"justify\" style = \"text-align:left\"><span>Visual Field Analysis may evolve and improve with time. All further versions will be made available to the scientific community on our <\/span><a href=\"https:\/\/github.com\/mathjoss\/VisualFieldsAnalysis\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">GitHub<\/span><\/a><span style = \":;\">.<\/span><\/div><\/div><div class = \"text-block\"><span style = \"font-weight:bold;:justify;\">Reference<\/span><\/div><div class = \"text-block\"><div class = \"justify\" style = \"text-align:left\"><span>Nath, T., Mathis, A., Chen, A.C.<\/span><span style = \"font-style:italic;\">et al.<\/span><span>Using DeepLabCut for 3D markerless pose estimation across species and behaviors.<\/span><span style = \"font-style:italic;\">Nat Protoc<\/span><span style = \"font-weight:bold;\">14,<\/span><span>2152\u20132176 (2019). https:\/\/doi.org\/10.1038\/s41596-019-0176-0<\/span><\/div><\/div><\/div>","materials":[],"description":"<div class = \"text-blocks\"><div class = \"text-block\">In this protocol, we provide a step by step guide to using Visual Field Analysis (VFA) successfully. VFA is a python program based on DeepLabCut toolbox (Nath et al., 2019). Using our program, it is possible to score reliably the eye use, activity, and time spent in different zones of different animal species and experimental paradigms for more reproducible research.<\/div><\/div>","changed_on":1596124506}