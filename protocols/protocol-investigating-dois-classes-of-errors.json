{"id":49582,"title":"Protocol: Investigating DOIs classes of errors ","title_html":"<p>Protocol: Investigating DOIs classes of errors <\/p>","image":{"source":"https:\/\/www.protocols.io\/img\/default_protocol.png","placeholder":"https:\/\/www.protocols.io\/img\/default_protocol.png"},"doi":"dx.doi.org\/10.17504\/protocols.io.bunnnvde","doi_status":2,"uri":"protocol-investigating-dois-classes-of-errors-bunnnvde","type_id":1,"template_id":5,"published_on":1620053235,"parent_protocols":[],"parent_collections":[],"cited_protocols":[],"version_id":3,"version_data":{"id":"3","code":"bunnnvde","parent_id":49366,"parent_uri":"protocol-34-investigating-dois-classes-of-errors-3-bufwntpe","is_same_owner":true,"has_pending_merge_request":false,"has_approved_merge_request":true},"created_on":1619784757,"modified_on":null,"categories":null,"public":1,"is_unlisted":0,"creator":{"name":"Arcangelo Massari","affiliation":null,"affiliations":[],"username":"m4vle152w1s4ple1","note":null,"link":null,"image":{"source":"\/img\/avatars\/010.png","placeholder":"\/img\/avatars\/010.png"},"badges":[{"id":2,"image":{"source":"\/img\/badges\/bronze.svg","placeholder":"\/img\/badges\/bronze.svg"},"name":"Author"}],"verified":0,"is_verified_user":false,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},"journal":null,"journal_name":null,"journal_link":null,"article_citation":null,"has_versions":0,"link":"","total_collections":0,"number_of_steps":12,"authors":[{"name":"Cristian Santini","affiliation":"University of Bologna","affiliations":[],"username":"m4vle152x1u4yle1","note":"","link":null,"image":{"source":"\/img\/avatars\/001.png","placeholder":"\/img\/avatars\/001.png"},"badges":[],"verified":1,"is_verified_user":true,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},{"name":"Arcangelo Massari","affiliation":"University of Bologna","affiliations":[],"username":"m4vle152w1s4ple1","note":"","link":null,"image":{"source":"\/img\/avatars\/010.png","placeholder":"\/img\/avatars\/010.png"},"badges":[],"verified":1,"is_verified_user":true,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},{"name":"Ricarda Boente","affiliation":"University of Bologna","affiliations":[],"username":"m4vle152w1s4vle1","note":"","link":null,"image":{"source":"\/img\/avatars\/005.png","placeholder":"\/img\/avatars\/005.png"},"badges":[],"verified":1,"is_verified_user":true,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},{"name":"Deniz Tural","affiliation":"University of Bologna","affiliations":[],"username":"m4vle152x1v4qle1","note":"","link":null,"image":{"source":"\/img\/avatars\/008.png","placeholder":"\/img\/avatars\/008.png"},"badges":[],"verified":1,"is_verified_user":true,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false}],"versions":[],"groups":[{"id":22817,"uri":"open-science-20202021","title":"Open Science 2020\/2021","image":{"source":"https:\/\/www.protocols.io\/img\/group_placeholder.png","placeholder":"https:\/\/www.protocols.io\/img\/group_placeholder.png"},"tech_support":{"email":null,"phone":null,"hide_contact":0,"use_email":0,"url":null},"is_member":1,"request":{"id":22817,"uri":"open-science-20202021","title":"Open Science 2020\/2021","image":{"source":"https:\/\/www.protocols.io\/img\/group_placeholder.png","placeholder":"https:\/\/www.protocols.io\/img\/group_placeholder.png"},"tech_support":{"email":null,"phone":null,"hide_contact":0,"use_email":0,"url":null},"is_member":1,"description":null,"research_interests":null,"website":null,"location":null,"affiliation":null,"status":{"is_visible":true,"access_level":0},"stats":{"files":[],"total_members":0,"total_followers":0,"total_child_groups":0,"total_parent_groups":0,"has_collaborations":0},"user_status":{"is_member":true,"is_confirmed":true,"is_invited":false,"is_owner":false,"is_admin":false,"is_following":false},"join_link":null,"token":null,"owner":{"name":" ","affiliation":null,"affiliations":[],"username":null,"note":null,"link":null,"image":{"source":null,"placeholder":null},"badges":[],"verified":0,"is_verified_user":false,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},"is_protocol_requested":0,"is_group_requested":0,"is_my":false,"is_request":false,"is_confirmed":1,"is_declined":0,"requester":{"name":" ","affiliation":null,"affiliation_url":null,"username":null,"link":null},"protocol":{"id":0,"title":"Protocol: Investigating DOIs classes of errors ","title_html":"Protocol: Investigating DOIs classes of errors ","image":{"source":null,"placeholder":null},"doi":null,"doi_status":0,"uri":"protocol-investigating-dois-classes-of-errors-bunnnvde","type_id":1,"template_id":0,"published_on":null,"stats":{"number_of_views":0,"number_of_steps":0,"number_of_bookmarks":0,"number_of_comments":0,"number_of_exports":0,"number_of_runs":0,"number_of_votes":0,"is_voted":0},"parent_protocols":[],"parent_collections":[],"cited_protocols":[]},"created_on":1618332389,"resolve_on":0,"resolved_user":{"name":" ","affiliation":null,"affiliations":[],"username":null,"note":null,"link":null,"image":{"source":null,"placeholder":null},"badges":[],"verified":0,"is_verified_user":false,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},"shared":false}}],"is_owner":1,"has_subprotocols":0,"is_subprotocol":0,"is_bookmarked":1,"can_claim_authorship":0,"can_accept_authorship":0,"can_be_copied":1,"can_remove_fork":1,"fork_id":null,"url":"https:\/\/www.protocols.io\/view\/protocol-investigating-dois-classes-of-errors-bunnnvde","forks_count":{"private":0,"public":0},"access":{"can_view":1,"can_remove":0,"can_add":0,"can_edit":0,"can_publish":0,"can_get_doi":0,"can_share":1,"can_move":1,"can_move_outside":1,"can_transfer":1,"can_download":1,"is_locked":0},"guid":"72B7DB83BA4D4766B6636BEED0BED8B1","state_version_id":1028,"steps":[{"id":1170487,"guid":"E9635AD6F8564DC889662A07CA128C54","previous_id":null,"previous_guid":null,"modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"8150C037B0974FDB8DA2F924792D4433","order_id":1,"type_id":6,"title":"Section","source":{"title":"DATA IMPORT"}},{"id":1054724,"guid":"952F0DFCEE8441B8AE5179F1D455612F","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>We import a CSV containing invalid DOI-to-DOI citations. The CSV is provided on public license by Peroni, Silvio. (2021). Citations to invalid DOI-identified entities obtained from processing DOI-to-DOI citations to add in COCI (Version 1.0) [Data set]. <\/span><span style = \"font-style:italic;\">Zenodo<\/span><span>. <\/span><\/div><div class = \"text-block\"><a href=\"http:\/\/doi.org\/10.5281\/zenodo.4625300\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">http:\/\/doi.org\/10.5281\/zenodo.4625300<\/span><\/a><\/div><div class = \"text-block\">. <\/div><div class = \"text-block\">The CSV is organized as follows: the first column contains valid citing DOIs and the second contains invalid cited DOIs.<\/div><\/div>"}},{"id":1054725,"guid":"AD3AB2009AE711EBB6547342859BDF69","order_id":2,"type_id":9,"title":"dataset","source":{"name":"Citations to invalid DOIs obtained from Crossref","link":"https:\/\/zenodo.org\/record\/4625300\/files\/invalid_dois.csv"}},{"id":1054726,"guid":"F79361A8FFF740C29EA14BA46752E038","order_id":3,"type_id":26,"title":"notes","source":{"id":0,"parent_id":0,"uri":"","title":"","body":"<div class = \"text-blocks\"><table border><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.14778\/1920841.1920954<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.5555\/646836.708343\t<\/td><\/tr><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.5406\/ethnomusicology.59.2.0202\t<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.2307\/20184517\t<\/td><\/tr><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1161\/01.cir.63.6.1391\t<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1161\/circ.37.4.509\t<\/td><\/tr><\/table><\/div>","created_on":0,"changed_on":0,"creator":{"name":" ","affiliation":null,"affiliations":[],"username":"","note":null,"link":null,"image":{"source":"","placeholder":""},"badges":[],"verified":0,"is_verified_user":false,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},"comments":[]}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#A492FF","section_duration":36000,"critical":null,"critical_id":null,"duration":0},{"id":1170488,"guid":"FF066BF95CD745BCA8AD2B978CD2B83B","previous_id":1170501,"previous_guid":"751714F0A9AF11EBA9B15D1C25CF0A4D","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"D8535D59CCEC43CEA2DD08EA6B61A63A","order_id":1,"type_id":6,"title":"Section","source":{"title":"ERROR ANALYSIS"}},{"id":1054724,"guid":"CC37863AB72F4202A70BC832CBFED471","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>Taking as a reference the DOI error's taxonomy proposed in <\/span><span style = \"font-style:italic;\">Errors in DOI indexing by bibliometric databases<\/span><span> (<\/span><\/div><div class = \"text-block\"><a href=\"https:\/\/doi.org\/10.1007\/s11192-014-1503-4\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">Franceschini et al. 2015<\/span><\/a><\/div><div class = \"text-block\"><span>), there are two main classes of errors: author errors, made by authors when creating the list of cited articles for their publication, and database mapping errors, related to a data-entry error. This protocol deals only with the second kind of error, which can be further divided between prefix errors, suffix errors, and other-type errors, as proposed in <\/span><span style = \"font-style:italic;\">DOI errors and possible solutions for Web of Science<\/span><span> (<\/span><\/div><div class = \"text-block\"><a href=\"https:\/\/doi.org\/10.1007\/s11192-019-03162-4\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">Xu et al., 2019<\/span><\/a><\/div><div class = \"text-block\">). In order to solve our problem, we manually isolated from a subset of 100 DOIs recurrent strings at the beginning and at the end with corrupted DOI prefix and suffix respectively. In addition, we found other types of errors, like double underscores, double periods, XML tags, spaces, and forward slashes that could be removed at the end of the data cleaning process. <\/div><\/div>"}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#94EBFF","section_duration":0,"critical":null,"critical_id":null,"duration":0},{"id":1170489,"guid":"B6FC0E81AC184506B25054F6E243F9E6","previous_id":1170488,"previous_guid":"FF066BF95CD745BCA8AD2B978CD2B83B","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"1087F2CAF48E4C5B901E8B6C8DBEA5D4","order_id":1,"type_id":6,"title":"Section","source":{"title":"ERROR ANALYSIS"}},{"id":1054724,"guid":"775E1509436F4B13982B864F64D9EDE9","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>For cleaning <\/span><span style = \"font-weight:bold;\">prefix-type errors<\/span><span>, we took a modified version of the regular expression from (<\/span><\/div><div class = \"text-block\"><a href=\"https:\/\/doi.org\/10.1007\/s11192-019-03162-4\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">Xu et al., 2019<\/span><\/a><\/div><div class = \"text-block\">) in order to remove unwanted characters at the beginning of a DOI. The regular expression for matching prefix-type errors is the following:<\/div><div class = \"text-block\"><span>It is worth noting that in this as in the following regular expressions the character \"O\" and the number \"0\" both count as a match, since, as demonstrated in <\/span><span style = \"font-style:italic;\">DOI errors and possible solutions for Web of Science<\/span><span> (<\/span><\/div><div class = \"text-block\"><a href=\"https:\/\/doi.org\/10.1007\/s11192-018-2980-7\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">Zhu et al., 2018<\/span><\/a><\/div><div class = \"text-block\">), the two characters are often confused.<\/div><\/div>"}},{"id":1054725,"guid":"9355A623D7264A9095C5FC2A4DBB8053","order_id":2,"type_id":26,"title":"notes","source":{"id":0,"parent_id":0,"uri":"","title":"","body":"<div class = \"text-blocks\"><div class = \"text-block\">\"(.*?)(?:\\.)?(?:HTTP:\\\/\\\/DX\\.D[0|O]I\\.[0|O]RG\\\/|HTTPS:\\\/\\\/D[0|O]I\\.[0|O]RG\\\/)(.*)\"<\/div><\/div>","created_on":0,"changed_on":0,"creator":{"name":" ","affiliation":null,"affiliations":[],"username":"","note":null,"link":null,"image":{"source":"","placeholder":""},"badges":[],"verified":0,"is_verified_user":false,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},"comments":[]}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#94EBFF","section_duration":0,"critical":null,"critical_id":null,"duration":0},{"id":1170490,"guid":"15C33EF0D18548C88A08BDB10D2933A3","previous_id":1170489,"previous_guid":"B6FC0E81AC184506B25054F6E243F9E6","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"79A6FD071634427AB8AA32BCE53EEE0A","order_id":1,"type_id":6,"title":"Section","source":{"title":"ERROR ANALYSIS"}},{"id":1054724,"guid":"B54347A4ADC644A6B7915FE7D1F1CF07","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>For capturing <\/span><span style = \"font-weight:bold;\">suffix-type errors<\/span><span>, we enriched the approach provided by (<\/span><\/div><div class = \"text-block\"><a href=\"https:\/\/doi.org\/10.1007\/s11192-019-03162-4\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">Xu et al., 2019<\/span><\/a><\/div><div class = \"text-block\">) and we devised a set of 17 alternative suffix regular expressions. The regular expression are the following:<\/div><div class = \"text-block\">All the suffix regular expressions can be combined in a single regular expression, by using alternatives within a non-capturing group. This is how the regular expression for suffix-type errors looks like:<\/div><\/div>"}},{"id":1054725,"guid":"A608F4991B984D7899013DFD9DDAF4B2","order_id":2,"type_id":26,"title":"notes","source":{"id":0,"parent_id":0,"uri":"","title":"","body":"<div class = \"text-blocks\"><div class = \"text-block\">1) \"(.*?)(?:\\\/-\\\/DCSUPPLEMENTAL)$\"<\/div><div class = \"text-block\">2) \"(.*?)(?:SUPPINF[0|O](\\.)?)$\"<\/div><div class = \"text-block\">3) \"(.*?)(?:[\\.|\\(|,|;]?PMID:\\d+.*?)$\"<\/div><div class = \"text-block\">4) \"(.*?)(?:[\\.|\\(|,|;]?PMCID:PMC\\d+.*?)$\"<\/div><div class = \"text-block\">5) \"(.*?)(?:[\\(|\\[]EPUBAHEADOFPRINT[\\)\\]])$\"<\/div><div class = \"text-block\">6) \"(.*?)(?:[\\.|\\(|,|;]?ARTICLEPUBLISHEDONLINE.*?\\d{4})$\"<\/div><div class = \"text-block\">7) \"(.*?)(?:[\\.|\\(|,|;]*HTTP:\\\/\\\/.*?)$\"<\/div><div class = \"text-block\">8) \"(.*?)(?:\\\/(META|ABSTRACT|FULL|EPDF|PDF|SUMMARY)([>|\\)](LAST)?ACCESSED\\d+)?)$\"<\/div><div class = \"text-block\">9) \"(.*?)(?:[>|)](LAST)?ACCESSED\\d+)$\"<\/div><div class = \"text-block\">10) \"(.*?)(?:[\\.|(|,|;]?[A-Z]*\\.?SAGEPUB.*)$?\"<\/div><div class = \"text-block\">11) \"(.*?)(?:\\.{5}.*?)$\"<\/div><div class = \"text-block\">12) \"(.*?)(?:[\\.|,|<|&|(|;])$\"<\/div><div class = \"text-block\">13) \"(.*?)(?:[\\.|(|,|;]10.\\d{4}\\\/.*?)$\"<\/div><div class = \"text-block\">14) \"(.*?)(?:\\[DOI\\].*?)$\"<\/div><div class = \"text-block\">15) \"(.*?)(?:\\(\\d{4}\\)?)$\"<\/div><div class = \"text-block\">16) \"(.*?)(?:\\?.*?=.*?)$\"<\/div><div class = \"text-block\">17) \"(.*?)(?:#.*?)$\"<\/div><\/div>","created_on":0,"changed_on":0,"creator":{"name":" ","affiliation":null,"affiliations":[],"username":"","note":null,"link":null,"image":{"source":"","placeholder":""},"badges":[],"verified":0,"is_verified_user":false,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},"comments":[]}},{"id":1054726,"guid":"F22687AFD603416D9C260A20130CB463","order_id":3,"type_id":26,"title":"notes","source":{"id":0,"parent_id":0,"uri":"","title":"","body":"<div class = \"text-blocks\"><div class = \"text-block\">\"(.*?)(?:\\\/-\\\/DCSUPPLEMENTAL|SUPPINF[0|O](\\.)?|[\\.|\\(|,|;]?PMID:\\d+.*?|[\\.|\\(|,|;]?PMCID:PMC\\d+.*?|[\\(|\\[]EPUBAHEADOFPRINT[\\)\\]]|[\\.|\\(|,|;]?ARTICLEPUBLISHEDONLINE.*?\\d{4}|[\\.|\\(|,|;]*HTTP:\\\/\\\/.*?|\\\/(META|ABSTRACT|FULL|EPDF|PDF|SUMMARY)([>|\\)](LAST)?ACCESSED\\d+)?|[>|\\)](LAST)?ACCESSED\\d+|[\\.|\\(|,|;]?[A-Z]*\\.?SAGEPUB.*?|\\.{5}.*?|[\\.|,|<|&|\\(|;]+|[\\.|\\(|,|;]10.\\d{4}\\\/.*?|\\[DOI\\].*?|\\(\\d{4}\\)?|\\?.*?=.*?|#.*?)$\"<\/div><\/div>","created_on":0,"changed_on":0,"creator":{"name":" ","affiliation":null,"affiliations":[],"username":"","note":null,"link":null,"image":{"source":"","placeholder":""},"badges":[],"verified":0,"is_verified_user":false,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},"comments":[]}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#94EBFF","section_duration":0,"critical":null,"critical_id":null,"duration":0},{"id":1170491,"guid":"FD233068939D4B34A9C27E7CC159972E","previous_id":1170490,"previous_guid":"15C33EF0D18548C88A08BDB10D2933A3","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"12325670792A49979966ACF646BC7C3A","order_id":1,"type_id":6,"title":"Section","source":{"title":"DATA CLEANING PROCEDURE"}},{"id":1054724,"guid":"9D548C6B582A44B1B4EF23075E6614CA","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\">Here we describe the steps through which we carried out the cleaning procedure of factually invalid DOIs. <\/div><div class = \"text-block\">The workflow of the procedure is organized as follows:<\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/protocols-files\/files\/dfvkbjqap.jpg\" \/><\/div><div class = \"text-block\">We created a Python software for handling the whole procedure. The source code is publicly available on ISC license at <\/div><div class = \"text-block\"><a href=\"http:\/\/doi.org\/10.5281\/zenodo.4734013\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">http:\/\/doi.org\/10.5281\/zenodo.4734013<\/span><\/a><\/div><div class = \"text-block\">. <\/div><div class = \"text-block\"><span>For our data cleaning system, we devised two class objects: one called <\/span><span style = \"font-weight:bold;\">class<\/span><span style = \"font-weight:bold;font-weight:bold;\"> <\/span><span style = \"font-weight:bold;\">Clean_DOIs()<\/span><span>, with all the methods required to carry our cleaning\/validation procedure, and <\/span><span style = \"font-weight:bold;\">class Support()<\/span><span>, with all the methods required to process and dump CSV as well as carry HTTP requests. In this protocol, we describe the use of the different methods that can be imported from our classes. If you want to reproduce the procedure described in the next step, once you have verified to have Python 3.x installed and the required libraries, you can clone our Github repository and launch the <\/span><span style = \"font-style:italic;\">tutorial.py <\/span><span>file in a terminal inside the folder or you can execute the commands listed in the following sub-sections.<\/span><\/div><\/div>"}},{"id":1054725,"guid":"301057BE13164E548BD8E0D0B746B0EA","order_id":2,"type_id":1,"title":"description","source":{"description":"<div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/protocols-files\/files\/dfvkbjqap.jpg\" \/><\/div>"}},{"id":1054726,"guid":"5F1858B0A50611EB873D85B4C91630DC","order_id":3,"type_id":15,"title":"command","source":{"id":8767,"name":"python tutorial.py","command_name":"Launch tutorial.py","command":null,"os_name":"","os_version":null}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#84CE84","section_duration":0,"critical":null,"critical_id":null,"duration":0},{"id":1170492,"guid":"2FDF1F0FDBAC4573B6D59D9C3CA20A74","previous_id":1170491,"previous_guid":"FD233068939D4B34A9C27E7CC159972E","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"BFFF2DFDF00D46288844E4B28E2B3C81","order_id":1,"type_id":6,"title":"Section","source":{"title":"DATA CLEANING PROCEDURE"}},{"id":1054724,"guid":"8F2FF11120934463AC9D4FF88F4B54F0","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>First, with the function <\/span><span style = \"font-weight:bold;\">process_csv_input()<\/span><span> from our <\/span><span style = \"font-weight:bold;\">class<\/span><span style = \"font-weight:bold;font-weight:bold;\"> <\/span><span style = \"font-weight:bold;\">Support()<\/span><span> we import our data in CSV format and we store it in a <\/span><span style = \"font-style:italic;\">DictReader<\/span><span>, an<\/span><span style = \"font-weight:bold;\"> <\/span><span>object that maps the information in each row to a dictionary whose keys are given by the optional fieldnames parameter.<\/span><\/div><div class = \"text-block\"><span>On our procedure, we perform this method on our input csv and we store it as a <\/span><span style = \"font-weight:bold;\"><data><\/span><span> variable. Optionally, it is also possible to import in the <\/span><span style = \"font-weight:bold;\"><crossref_dois><\/span><span> variable the Crossref DOIs previously stored in a CSV file as shown in step 1.2. It is recommended to perform this step having at least 64GB of RAM, as over 120 million DOIs need to be stored in a set and kept on RAM throughout the entire process. You can reproduce the experiment by launching:<\/span><\/div><div class = \"text-block\"><span>After this procedure, the <\/span><span style = \"font-weight:bold;\"><data><\/span><span> variable (and, if included, the <\/span><span style = \"font-weight:bold;\"><crossref_dois><\/span><span> variable) will consist of a list of dictionaries, as shown below:<\/span><\/div><\/div>"}},{"id":1054725,"guid":"EB1ED650A9BA11EBA9B15D1C25CF0A4D","order_id":2,"type_id":15,"title":"command","source":{"id":8773,"name":"@staticmethod\ndef process_csv_input(path:str) -> list:\n    print(f\"[Support:INFO Proccessing csv at path {path}]\")\n    with open(path, 'r', encoding='utf-8') as csvfile:\n        reader = csv.DictReader(csvfile)\n        return list(reader)","command_name":"","command":null,"os_name":"","os_version":""}},{"id":1054726,"guid":"813AF6C0A76611EBB340838B39CCBF73","order_id":3,"type_id":15,"title":"command","source":{"id":8768,"name":"from support import Support\nfrom clean_dois import Clean_DOIs\n\ndata = Support.process_csv_input(\".\/dataset\/invalid_dois.csv\")\ncrossref_dois = Support.process_csv_input(\".\/dataset\/crossref_dois.csv\")","command_name":"","command":null,"os_name":null,"os_version":null}},{"id":1054727,"guid":"77E240631F994B13A843B5B08D27FBCE","order_id":4,"type_id":17,"title":"result","source":{"body":"<div class = \"text-blocks\"><div class = \"text-block\">[{\"Valid_citing_DOI\": \"10.14778\/1920841.1920954\", \"Invalid_cited_DOI\": \"10.5555\/646836.708343\"}, {\"Valid_citing_DOI\": \"10.5406\/ethnomusicology.59.2.0202\", \"Invalid_cited_DOI\": \"10.2307\/20184517\"}, <\/div><div class = \"text-block\">...]<\/div><\/div>"}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#84CE84","section_duration":0,"critical":null,"critical_id":null,"duration":0},{"id":1170493,"guid":"6FA5F486E7B54B829709EF69CC115003","previous_id":1170492,"previous_guid":"2FDF1F0FDBAC4573B6D59D9C3CA20A74","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"E3D27460F76544519E938C03C1B9F475","order_id":1,"type_id":6,"title":"Section","source":{"title":"DATA CLEANING PROCEDURE"}},{"id":1054724,"guid":"31241B41133A4098B0565B8D09522062","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>The procedure first checks for each cited DOI if it is factually invalid or it has become valid in the meanwhile. For this purpose, we use the function <\/span><span style = \"font-weight:bold;\">check_doi<\/span><span style = \"font-weight:bold;font-style:italic;\">s<\/span><span style = \"font-weight:bold;\">_validity()<\/span><span style = \"font-weight:bold;font-weight:bold;\">,<\/span><span style = \"font-weight:bold;\"> <\/span><span>from our <\/span><span style = \"font-weight:bold;\">class Clean_DOIs()<\/span><span>, which takes in input a list of dictionaries, like the one described above, and does a GET request to the DOI Proxy with the cited DOI (<\/span><\/div><div class = \"text-block\"><a href=\"https:\/\/www.doi.org\/factsheets\/DOIProxy.html\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">https:\/\/www.doi.org\/factsheets\/DOIProxy.html<\/span><\/a><\/div><div class = \"text-block\"><span>) of each row of the CSV: if the status code corresponding to that specific DOI is different from 1, it means that it is not valid; otherwise, that DOI has become valid in the meanwhile. The function outputs an enriched version of the list of dictionaries in input, with two additional fields: <\/span><span style = \"font-style:italic;\">\"Valid_DOI\"<\/span><span>, which stores a validated DOI, and <\/span><span style = \"font-style:italic;\">\"Already_valid\"<\/span><span>, which stores 1 if it has become valid in the meanwhile, 0 otherwise.<\/span><\/div><div class = \"text-block\"><span>In case the optional <\/span><span style = \"font-weight:bold;\"><crossref_dois><\/span><span> argument has been passed, a set containing all DOIs in the corresponding dictionaries list is generated. The set was chosen and not the list because, having all the values hashed, it makes the content check instantaneous, at the expense of more RAM. Therefore, the request to the DOI Proxy Server is made only if the current DOI is not found in the set, in which case it is registered as valid.<\/span><\/div><div class = \"text-block\"><span>For performing the HTTP request, the function uses the <\/span><span style = \"font-weight:bold;\">handle_request()<\/span><span> method of the <\/span><span style = \"font-weight:bold;\">class Support()<\/span><span style = \"font-weight:bold;\">.<\/span><\/div><div class = \"text-block\"><span>We can now instantiate the <\/span><span style = \"font-weight:bold;\">Clean_DOIs() <\/span><span>class, passing none or more of the optional parameters <\/span><span style = \"font-weight:bold;\"><crossref_dois><\/span><span>, <\/span><span style = \"font-weight:bold;\"><cache_path><\/span><span> and<\/span><span style = \"font-weight:bold;\"> <logs><\/span><span>,. Then, we run the <\/span><span style = \"font-weight:bold;\">check_dois_validity() <\/span><span>function passing the <\/span><span style = \"font-weight:bold;\"><data><\/span><span> and storing the results in a <\/span><span style = \"font-weight:bold;\"><checked_dois><\/span><span> variable:<\/span><\/div><div class = \"text-block\"><span>Depending on the dataset size, on the bandwidth and whether or not the <\/span><span style = \"font-weight:bold;\"><crossref_dois> <\/span><span>parameter has been used, this step can take a long time, up to a week in the case of the over one million DOIs of the input dataset mentioned at step 1 at an average bandwidth of 100 Mb\/s.<\/span><\/div><div class = \"text-block\"><span>Once the execution is complete, this is how the <\/span><span style = \"font-weight:bold;\"><checked_dois><\/span><span> variable appears:<\/span><\/div><\/div>"}},{"id":1054725,"guid":"0CFCE7F0A9C311EBA9B15D1C25CF0A4D","order_id":2,"type_id":15,"title":"command","source":{"id":8774,"name":"def check_dois_validity(self, data:list) -> list:\n        checked_dois = list()\n        pbar = tqdm(total=len(data))\n        for row in data:\n            valid_citing_doi = row[\"Valid_citing_DOI\"]\n            invalid_cited_doi = row[\"Invalid_cited_DOI\"]\n            invalid_dictionary = {\n                \"Valid_citing_DOI\": valid_citing_doi,\n                \"Invalid_cited_DOI\": invalid_cited_doi, \n                \"Valid_DOI\": \"\",\n                \"Already_valid\": 0\n            }\n            if invalid_cited_doi in self.crossref_dois:\n                handle = {\"responseCode\": 1}\n            else:\n                handle = Support().handle_request(url=f\"https:\/\/doi.org\/api\/handles\/{invalid_cited_doi}\", cache_path=self.cache_path, error_log_dict=self.logs)\n            if handle is not None:\n                if handle[\"responseCode\"] == 1:\n                    checked_dois.append(\n                        {\"Valid_citing_DOI\": valid_citing_doi,\n                        \"Invalid_cited_DOI\": invalid_cited_doi, \n                        \"Valid_DOI\": invalid_cited_doi,\n                        \"Already_valid\": 1\n                        })\n                else:\n                    checked_dois.append(invalid_dictionary)       \n            else:\n                checked_dois.append(invalid_dictionary)             \n            pbar.update(1)\n        pbar.close()\n        return checked_dois","command_name":"","command":null,"os_name":null,"os_version":null}},{"id":1054726,"guid":"96F88420A9C611EBA9B15D1C25CF0A4D","order_id":3,"type_id":15,"title":"command","source":{"id":8775,"name":"def _requests_retry_session(\n        self,\n        tries=2,\n        status_forcelist=(500, 502, 504, 520, 521),\n        session=None\n    ) -> Session:\n        session = session or requests.Session()\n        retry = Retry(\n            total=tries,\n            read=tries,\n            connect=tries,\n            status_forcelist=status_forcelist,\n        )\n        adapter = HTTPAdapter(max_retries=retry)\n        session.mount('http:\/\/', adapter)\n        session.mount('https:\/\/', adapter)\n        return session\n    \n    def handle_request(self, url:str, cache_path:str=\"\", error_log_dict:dict=dict()):\n        if cache_path != \"\":\n            requests_cache.install_cache(cache_path)\n        try:\n            data = self._requests_retry_session().get(url, timeout=10)\n            if data.status_code == 200:\n                return data.json()\n            else:\n                error_log_dict[url] = data.status_code\n        except Exception as e:\n            error_log_dict[url] = str(e)","command_name":"","command":null,"os_name":null,"os_version":null}},{"id":1054727,"guid":"CDEB1310A76611EBB340838B39CCBF73","order_id":4,"type_id":15,"title":"command","source":{"id":8769,"name":"clean_dois = Clean_DOIs(crossref_dois=crossref_dois, cache_path=\".\/cache\/doi_cache\", logs=doi_logs)\n\nchecked_dois=clean_dois.check_dois_validity(data=data)","command_name":"","command":null,"os_name":null,"os_version":null}},{"id":1054728,"guid":"93BD5300191A4EBDA465332848B8772F","order_id":5,"type_id":17,"title":"result","source":{"body":"<div class = \"text-blocks\"><div class = \"text-block\">[{\"Valid_citing_DOI\": \"10.1007\/s11771-020-4410-2\", \"Invalid_cited_DOI\": \"10.13745\/j.esf.2016.02.011\", \"Valid_DOI\": \"10.13745\/j.esf.2016.02.011\", Already_valid\": 1},<\/div><div class = \"text-block\">...]<\/div><\/div>"}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#84CE84","section_duration":0,"critical":null,"critical_id":null,"duration":604800},{"id":1170494,"guid":"A841D0C1D7074F84AAEA0C0A14849DC0","previous_id":1170493,"previous_guid":"6FA5F486E7B54B829709EF69CC115003","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"986C666A2B7C4F2CBE97577302808E04","order_id":1,"type_id":6,"title":"Section","source":{"title":"DATA CLEANING PROCEDURE"}},{"id":1054724,"guid":"6621B108D62148A2B4BB0F7BE90556A8","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>Afterwards, we pass the <\/span><span style = \"font-weight:bold;\"><checked_dois><\/span><span> variable obtained in the previous step to another function called <\/span><span style = \"font-weight:bold;\">procedure(),<\/span><span> aimed to clean the DOIs with the regular expressions described above, and to verify the validity of the cleaned DOIs.<\/span><\/div><div class = \"text-block\"><span>In our data cleaning procedure we store the output of the <\/span><span style = \"font-weight:bold;\">procedure() <\/span><span>function in the <\/span><span style = \"font-weight:bold;\"><output> <\/span><span>variable:<\/span><\/div><div class = \"text-block\"><span>For each cited DOI, the function <\/span><span style = \"font-weight:bold;\">procedure() <\/span><span>performs five actions:<\/span><\/div><div class = \"text-block\"><ol style = \"list-style-type: decimal;\"><li style = \"counter-reset:ol0;\">it removes white spaces in the string;<\/li><li style = \"counter-reset:ol0;\">it applies the prefix regular expression to catch eventual unwanted characters that precede a DOI. If there is a match, the regular expression removes the unwanted characters and the DOI is replaced with the cleaned one;<\/li><li style = \"counter-reset:ol0;\"> it applies the suffix regular expressions as a list of alternative unwanted patterns that may occur at the end of the string. If there is a match, the regular expression removes the unwanted characters and the DOI is replaced with the cleaned one;<\/li><li style = \"counter-reset:ol0;\">it removes eventual unwanted characters, like double slashes, double points, xml tags and backward slashes;<\/li><li style = \"counter-reset:ol0;\"><span>finally, the algorithm checks if the cited DOI after the cleaning procedure is different from the one in <\/span><span style = \"font-weight:bold;\"><checked_dois><\/span><span>, if it is contained in the <\/span><span style = \"font-weight:bold;\"><crossref_dois> <\/span><span>set or returned as valid by the DOI Handle API. If it is different and is validated, the algorithm stores the cleaned DOI in the \"<\/span><span style = \"font-style:italic;\">Valid_DOI\"  field<\/span><span> of the dictionary; if it is not different, the dictionary will store no new DOI.<\/span><\/li><\/ol><\/div><div class = \"text-block\"><span>By applying <\/span><span style = \"font-weight:bold;\">procedure() <\/span><span>to our <\/span><span style = \"font-weight:bold;\"><checked_dois><\/span><span> we obtain a list of dictionaries containing 7 fields: the four already present in the input list (\"<\/span><span style = \"font-style:italic;\">Valid_citing_DOI\"<\/span><span style = \"font-weight:bold;font-style:italic;font-style:italic;\">,<\/span><span style = \"font-weight:bold;font-style:italic;\"> <\/span><span style = \"font-style:italic;\">\"Invalid_cited_DOI\"<\/span><span>, <\/span><span style = \"font-style:italic;\">\"Valid_DOI\", <\/span><span style = \"font-style:italic;font-style:italic;\">\"<\/span><span style = \"font-style:italic;\">Already_Valid\"<\/span><span>) and, in addition, <\/span><span style = \"font-style:italic;\">\"prefix_error\"<\/span><span>, <\/span><span style = \"font-style:italic;\">\"suffix_error\"<\/span><span>, <\/span><span style = \"font-style:italic;\">\"other-type<\/span><span style = \"font-weight:bold;font-style:italic;\">\"<\/span><span style = \"font-weight:bold;\"> <\/span><span>fields, which contains, for each cleaned DOI, the number of errors that were cleaned. This is how the <\/span><span style = \"font-weight:bold;\"><output><\/span><span> variable looks like:<\/span><\/div><\/div>"}},{"id":1054725,"guid":"CF833D10A9C811EBA9B15D1C25CF0A4D","order_id":2,"type_id":15,"title":"command","source":{"id":8776,"name":"def procedure(self, data:list) -> list:\n        output = list()\n        pbar = tqdm(total=len(data))\n        for row in data:\n            valid_citing_doi = row[\"Valid_citing_DOI\"]\n            invalid_cited_doi = row[\"Invalid_cited_DOI\"]\n            unclean_dictionary = {\n                \"Valid_citing_DOI\": valid_citing_doi,\n                \"Invalid_cited_DOI\": invalid_cited_doi,\n                \"Valid_DOI\": row[\"Valid_DOI\"],\n                \"Already_valid\": row[\"Already_valid\"],\n                \"Prefix_error\": 0,\n                \"Suffix_error\": 0,\n                \"Other-type_error\": 0\n            }\n            if row[\"Already_valid\"] != 1:\n                new_doi, classes_of_errors = self.clean_doi(invalid_cited_doi)\n                clean_dictionary = {\n                    \"Valid_citing_DOI\": valid_citing_doi,\n                    \"Invalid_cited_DOI\": invalid_cited_doi,\n                    \"Valid_DOI\": new_doi,\n                    \"Already_valid\": row[\"Already_valid\"],\n                    \"Prefix_error\": classes_of_errors[\"prefix\"],\n                    \"Suffix_error\": classes_of_errors[\"suffix\"],\n                    \"Other-type_error\": classes_of_errors[\"other-type\"]\n                }\n                if new_doi != invalid_cited_doi:\n                    if new_doi in self.crossref_dois:\n                        handle = {\"responseCode\": 1}\n                    else:\n                        handle = Support().handle_request(url=f\"https:\/\/doi.org\/api\/handles\/{new_doi}\", cache_path=self.cache_path, error_log_dict=self.logs)\n                    if handle is not None:\n                        if handle[\"responseCode\"] == 1:\n                            output.append(clean_dictionary)\n                        else:\n                            output.append(unclean_dictionary)\n                    else:\n                        output.append(unclean_dictionary)\n                else:\n                    output.append(unclean_dictionary)\n            else:\n                output.append(unclean_dictionary)\n            pbar.update(1)\n        pbar.close()\n        return output","command_name":"","command":null,"os_name":null,"os_version":null}},{"id":1054726,"guid":"61909EF0A76711EBB340838B39CCBF73","order_id":3,"type_id":15,"title":"command","source":{"id":8770,"name":"output = clean_dois.procedure(data=checked_dois)","command_name":"","command":null,"os_name":null,"os_version":null}},{"id":1054727,"guid":"FD5D399FE37F4235B83B7FC18A828413","order_id":4,"type_id":17,"title":"result","source":{"body":"<div class = \"text-blocks\"><div class = \"text-block\">[{\"Valid_citing_DOI\": \"10.1007\/s40617-018-00299-1\", \"Invalid_cited_DOI\": \"10.1901\/jaba.2012.45-657\", \"Valid_DOI\": \"10.1901\/JABA.2012.45-657\", Already_valid\": 1, \"prefix_error\": 0, \"suffix_error\" 0:, \"other-type\": 0 }, <\/div><div class = \"text-block\">{\"Valid_citing_DOI\": \"10.1074\/jbc.m508416200\", \"Invalid_cited_DOI\": \"10.1059\/0003-4819-100-4-483\", \"Valid_DOI\": \"\", Already_valid\": 0, \"prefix_error\": 0, \"suffix_error\" 0:, \"other-type\": 0 }, <\/div><div class = \"text-block\">{\"Valid_citing_DOI\": \"10.1177\/2054358119836124\", \"Invalid_cited_DOI\": \"10.1016\/j.amepre.2015.07.017.\", \"Valid_DOI\": \"10.1016\/J.AMEPRE.2015.07.017\", Already_valid\": 0, \"prefix_error\": 0, \"suffix_error\" 1:, \"other-type\": 0 },<\/div><div class = \"text-block\">...]<\/div><\/div>"}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#84CE84","section_duration":0,"critical":null,"critical_id":null,"duration":432000},{"id":1170495,"guid":"02BDF56744C34AE9B86071E316C69CAB","previous_id":1170494,"previous_guid":"A841D0C1D7074F84AAEA0C0A14849DC0","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"25BD16892C2446F2A266E004D5DF49BB","order_id":1,"type_id":6,"title":"Section","source":{"title":"DATA CLEANING PROCEDURE"}},{"id":1054724,"guid":"E1D7AB72EBAC41308F7A953F074F7D12","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>At the end of our procedure we dump our <\/span><span style = \"font-weight:bold;\"><output> <\/span><span>in a csv file through the <\/span><span style = \"font-weight:bold;\">dump_csv()<\/span><span> method of the class <\/span><span style = \"font-weight:bold;\">Support()<\/span><span style = \"font-weight:bold;\">:<\/span><\/div><div class = \"text-block\">This is how the method is used in our data cleaning procedure:<\/div><div class = \"text-block\">The output of the function is then stored in a CSV file, organized in 7 fields as the dictionary mentioned above:<\/div><table border><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1007\/s40617-018-00299-1<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1901\/jaba.2012.45-657<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1901\/JABA.2012.45-657<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">1<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><\/tr><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1074\/jbc.m508416200<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1059\/0003-4819-100-4-483<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\"><\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><\/tr><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1177\/2054358119836124<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1016\/j.amepre.2015.07.017.<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1016\/J.AMEPRE.2015.07.017<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">1<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><\/tr><\/table><div class = \"text-block\">The output dataset is available at <\/div><div class = \"text-block\"><a href=\"http:\/\/doi.org\/10.5281\/zenodo.4733647\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">http:\/\/doi.org\/10.5281\/zenodo.4733647<\/span><\/a><\/div><div class = \"text-block\"><span style = \":UNDERLINE;\"> (Boente, R. et al., 2021)<\/span><\/div><div class = \"text-block\">Further information about the possible uses of research data generated by this project, and the support provided by the authors for reuse, are available in the Data Management Plan provided by the authors at <\/div><div class = \"text-block\"><a href=\"https:\/\/doi.org\/10.5281\/zenodo.4733920\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">https:\/\/doi.org\/10.5281\/zenodo.4733920<\/span><\/a><\/div><div class = \"text-block\"><span style = \":UNDERLINE;\">.<\/span><\/div><\/div>"}},{"id":1054725,"guid":"18C58D60A9CA11EBA9B15D1C25CF0A4D","order_id":2,"type_id":15,"title":"command","source":{"id":8777,"name":"@staticmethod\ndef dump_csv(data:list, path:str):\n    print(f\"[Support:INFO Writing csv at path {path}]\")\n    with open(path, 'w', newline='', encoding='utf8')  as output_file:\n        keys = data[0].keys()\n        dict_writer = csv.DictWriter(output_file, keys)\n        dict_writer.writeheader()\n        dict_writer.writerows(data)","command_name":"","command":null,"os_name":null,"os_version":null}},{"id":1054726,"guid":"7F4599F0A76711EBB340838B39CCBF73","order_id":3,"type_id":15,"title":"command","source":{"id":8771,"name":"Support.dump_csv(data=output, path=\".\/output.csv\")","command_name":"","command":null,"os_name":null,"os_version":null}},{"id":1054727,"guid":"47C7603A33B243ADBCEF284042C9CFC3","order_id":4,"type_id":1,"title":"description","source":{"description":"<table border><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1007\/s40617-018-00299-1<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1901\/jaba.2012.45-657<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1901\/JABA.2012.45-657<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">1<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><\/tr><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1074\/jbc.m508416200<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1059\/0003-4819-100-4-483<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\"><\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><\/tr><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1177\/2054358119836124<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1016\/j.amepre.2015.07.017.<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1016\/J.AMEPRE.2015.07.017<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">1<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><\/tr><\/table>"}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#84CE84","section_duration":0,"critical":null,"critical_id":null,"duration":0},{"id":1170496,"guid":"8395C8B38733471593093B7579480698","previous_id":1170495,"previous_guid":"02BDF56744C34AE9B86071E316C69CAB","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"58CB0914C2BB4D649FBE59953EBB3244","order_id":1,"type_id":6,"title":"Section","source":{"title":"STATISTICS AND VISUALIZATION"}},{"id":1054724,"guid":"47C4727FE1194A80863689307302A654","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\">Finally, two visualizations were produced from the output file. For this purpose, d3.js v6.7.0, an open source Javascript library for manipulating data-driven documents, was used. In particular, a barchart and a treemap have been created, which compare the number of DOIs still not valid after the cleaning procedure with those that have been corrected, and the number of those belonging to the four classes of errors considered, i.e. DOIs already valid and DOIs with prefix, suffix, or other-type errors. The bar chart can be sorted in ascending or descending order and it is possible to hover the mouse over both visualizations to highlight the represented value and the percentage of the total. It is possible to interact with the visualization at the following address: <\/div><div class = \"text-block\"><a href=\"https:\/\/open-sci.github.io\/2020-2021-grasshoppers-code\/\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">https:\/\/open-sci.github.io\/2020-2021-grasshoppers-code\/<\/span><\/a><\/div><div class = \"text-block\">.<\/div><\/div>"}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#FFED92","section_duration":0,"critical":null,"critical_id":null,"duration":0},{"id":1170499,"guid":"0DBC4CE0A9AE11EBA9B15D1C25CF0A4D","previous_id":1170487,"previous_guid":"E9635AD6F8564DC889662A07CA128C54","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"49743481285F44F18E6881E03E080444","order_id":1,"type_id":6,"title":"Section","source":{"title":""}},{"id":1054724,"guid":"FB48E5E063824EFE8D75E3ADBE94AB68","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\">In order to make the DOI validity check procedure dramatically more efficient, Crossref's public data file containing over 120 million metadata records can be used. The reason is that the DOIs reported by Crossref at the work level are assigned by Crossref itself and not by publishers and, therefore, they are necessarily correct. The records are provided on public licence by Crossref. (2021). January 2021 Public Data File from Crossref. <\/div><div class = \"text-block\"><a href=\"https:\/\/doi.org\/10.13003\/GU3DQMJVG4\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">https:\/\/doi.org\/10.13003\/GU3DQMJVG4<\/span><\/a><\/div><div class = \"text-block\"><span style = \":UNDERLINE;\">.<\/span><span>  <\/span><\/div><div class = \"text-block\">According to the statistics reported by Crossref at the indicated address, the compressed file weighs 102.51 GB and the download time at an average speed of 12.73 MB\/s is 2 hours.<\/div><\/div>"}},{"id":1054725,"guid":"17CA4A00A9B511EBA9B15D1C25CF0A4D","order_id":2,"type_id":9,"title":"dataset","source":{"name":"January 2021 Public Data File from Crossref","link":"https:\/\/doi.org\/10.13003\/GU3DQMJVG4"}},{"id":1054726,"guid":"AE40A762C46E4E768E836FCA744097FC","order_id":3,"type_id":19,"title":"safety","source":{"body":"<div class = \"text-blocks\"><div class = \"text-block\">This sub-step and the following one are completely optional and it is recommended to perform them only having at least 64 GB of RAM available, since over 120 million DOIs must be stored in a set and kept in RAM during the entire process shown at step 3. <\/div><\/div>","link":""}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"","section_duration":7200,"critical":null,"critical_id":null,"duration":7200},{"id":1170501,"guid":"751714F0A9AF11EBA9B15D1C25CF0A4D","previous_id":1170499,"previous_guid":"0DBC4CE0A9AE11EBA9B15D1C25CF0A4D","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"E92F2E9EAE8442B4B615043006A81088","order_id":1,"type_id":6,"title":"Section","source":{"title":""}},{"id":1054724,"guid":"BD9851D1CFF847D3A340A19556B77D65","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>Once downloaded, the file consists of a folder containing 40'228 JSON files. The function <\/span><span style = \"font-weight:bold;\">get_all_crossref_dois()<\/span><span> is used to extract the DOI names from each work.<\/span><\/div><div class = \"text-block\">The function cycles through all the JSON files in a folder, opens them, and parses them looking for works' DOIs. For this purpose the ijson library was used, that instead of loading the whole file into memory and parsing everything at once, lazily streams the data only responsing to specific events, similar to what SAX does for XML. <\/div><div class = \"text-block\">Finally, the list thus created is saved in a CSV file. The CSV file is preferred over JSON for this purpose because, as there is no hierarchy in the data, CSV takes up less space as it uses fewer characters than JSON. The CSV thus obtained consists of a list of all the DOIs of all the works found in Crossref, as shown in the table below:<\/div><\/div>"}},{"id":1054725,"guid":"91236800A9BA11EBA9B15D1C25CF0A4D","order_id":2,"type_id":15,"title":"command","source":{"id":8772,"name":"import ijson, os, tqdm as tqdm\n\n\ndef get_all_crossref_dois(folder_path:str) -> list:\n        json_files = [pos_json for pos_json in os.listdir(folder_path) if pos_json.endswith('.json')]\n        dois = list()\n        pbar = tqdm(total=len(json_files))\n        for json_file in json_files:\n            with open(os.path.join(folder_path, json_file)) as json_file:\n                parser = ijson.parse(json_file)\n                for prefix, event, value in parser:\n                    if (prefix, event) == ('items.item.DOI', 'string'):\n                        dois.append({\"crossref_doi\": value})\n            pbar.update(1)\n        pbar.close()\n        return dois","command_name":"","command":null,"os_name":"","os_version":null}},{"id":1054726,"guid":"6B4A37E39B774C12A4CABE0A8B730D83","order_id":3,"type_id":26,"title":"notes","source":{"id":0,"parent_id":0,"uri":"","title":"","body":"<div class = \"text-blocks\"><table border><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1001\/.389<\/td><\/tr><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1001\/.391<\/td><\/tr><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1001\/.405<\/td><\/tr><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1001\/.411<\/td><\/tr><\/table><\/div>","created_on":0,"changed_on":0,"creator":{"name":" ","affiliation":null,"affiliations":[],"username":"","note":null,"link":null,"image":{"source":"","placeholder":""},"badges":[],"verified":0,"is_verified_user":false,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},"comments":[]}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"","section_duration":0,"critical":null,"critical_id":null,"duration":28800}],"document":"","materials":[],"description":"<div class = \"text-blocks\"><div class = \"text-block\">The purpose of this protocol is to provide an automated process to repair invalid DOIs that have been collected by the OpenCitations Index Of Crossref Open DOI-To-DOI References (COCI) while processing data provided by Crossref.<\/div><div class = \"text-block\"><span>The data needed for this work is <\/span><a href=\"http:\/\/doi.org\/10.5281\/zenodo.4625300\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">provided by COCI as a CSV<\/span><\/a><span> containing pairs of valid citing DOIs and invalid cited DOIs. With the goal to determine an automated process, we first classified the errors that characterize the wrong DOIs in the list. The starting hypothesis is that there are two main classes of errors: factual errors, such as wrong characters, and DOIs that are not yet valid at the time of processing. The first class can be furtherly divided into three classes: errors due to irrelevant strings added to the beginning (prefix-type errors) or at the end (suffix-type errors) of the correct DOI, and errors due to unwanted characters in the middle (other-type errors). Once the classes of errors are addressed, we propose automatic processes to obtain correct DOIs from wrong ones. These processes involve the use of the information returned from <\/span><a href=\"https:\/\/www.doi.org\/factsheets\/DOIProxy.html\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">DOI API<\/span><\/a><span>, the <\/span><a href=\"https:\/\/doi.org\/10.13003\/GU3DQMJVG4\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">January 2021 Public Data File from Crossref<\/span><\/a><span>, as well as rule-based methods, including regular expressions to correct invalid DOIs.<\/span><\/div><div class = \"text-block\">The application of this methodology produced a CSV dataset containing all the pairs of citing and cited DOIs in the original dataset, each one enriched by 5 fields: \"Already_Valid\", which tells if the cited DOI was already valid before cleaning, \"New_DOI\", which contain a clean, valid DOI (if our procedure was able to produce one), and \"prefix_error\", \"suffix_error\" and \"other-type_error\" fields, which contain, for each cleaned DOI the number of errors that were cleaned.<\/div><\/div>","changed_on":1620053235}