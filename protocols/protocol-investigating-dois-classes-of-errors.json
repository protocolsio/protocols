{"id":49772,"title":"Protocol: Investigating DOIs classes of errors ","title_html":"<p>Protocol: Investigating DOIs classes of errors <\/p>","image":{"source":"https:\/\/www.protocols.io\/img\/default_protocol.png","placeholder":"https:\/\/www.protocols.io\/img\/default_protocol.png"},"doi":"dx.doi.org\/10.17504\/protocols.io.buuknwuw","doi_status":2,"uri":"protocol-investigating-dois-classes-of-errors-buuknwuw","type_id":1,"template_id":5,"published_on":1623174906,"parent_protocols":[],"parent_collections":[],"cited_protocols":[],"version_id":4,"version_data":{"id":"4","code":"buuknwuw","parent_id":49582,"parent_uri":"protocol-investigating-dois-classes-of-errors-bunnnvde","is_same_owner":true,"has_pending_merge_request":false,"has_approved_merge_request":true},"created_on":1620485713,"modified_on":null,"categories":null,"public":1,"is_unlisted":0,"creator":{"name":"Arcangelo  Massari","affiliation":null,"affiliations":[],"username":"arcangelo--massari","note":null,"link":null,"image":{"source":"\/img\/avatars\/010.png","placeholder":"\/img\/avatars\/010.png"},"badges":[{"id":2,"image":{"source":"\/img\/badges\/bronze.svg","placeholder":"\/img\/badges\/bronze.svg"},"name":"Author"}],"verified":0,"is_verified_user":false,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},"journal":null,"journal_name":null,"journal_link":null,"article_citation":null,"has_versions":0,"link":"","total_collections":0,"number_of_steps":14,"authors":[{"name":"Ricarda Boente","affiliation":"University of Bologna","affiliations":[],"username":"m4vle152w1s4vle1","note":"","link":null,"image":{"source":"\/img\/avatars\/005.png","placeholder":"\/img\/avatars\/005.png"},"badges":[],"verified":1,"is_verified_user":true,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},{"name":"Arcangelo  Massari","affiliation":"University of Bologna","affiliations":[],"username":"arcangelo--massari","note":"","link":null,"image":{"source":"\/img\/avatars\/010.png","placeholder":"\/img\/avatars\/010.png"},"badges":[],"verified":1,"is_verified_user":true,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},{"name":"Cristian Santini","affiliation":"University of Bologna","affiliations":[],"username":"m4vle152x1u4yle1","note":"","link":null,"image":{"source":"\/img\/avatars\/001.png","placeholder":"\/img\/avatars\/001.png"},"badges":[],"verified":1,"is_verified_user":true,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},{"name":"Deniz Tural","affiliation":"University of Bologna","affiliations":[],"username":"m4vle152x1v4qle1","note":"","link":null,"image":{"source":"\/img\/avatars\/008.png","placeholder":"\/img\/avatars\/008.png"},"badges":[],"verified":1,"is_verified_user":true,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false}],"versions":[],"groups":[{"id":22817,"uri":"open-science-20202021","title":"Open Science 2020\/2021","image":{"source":"https:\/\/www.protocols.io\/img\/group_placeholder.png","placeholder":"https:\/\/www.protocols.io\/img\/group_placeholder.png"},"tech_support":{"email":null,"phone":null,"hide_contact":0,"use_email":0,"url":null},"is_member":1,"request":{"id":22817,"uri":"open-science-20202021","title":"Open Science 2020\/2021","image":{"source":"https:\/\/www.protocols.io\/img\/group_placeholder.png","placeholder":"https:\/\/www.protocols.io\/img\/group_placeholder.png"},"tech_support":{"email":null,"phone":null,"hide_contact":0,"use_email":0,"url":null},"is_member":1,"description":null,"research_interests":null,"website":null,"location":null,"affiliation":null,"status":{"is_visible":true,"access_level":0},"stats":{"files":[],"total_members":0,"total_followers":0,"total_child_groups":0,"total_parent_groups":0,"has_collaborations":0},"user_status":{"is_member":true,"is_confirmed":true,"is_invited":false,"is_owner":false,"is_admin":false,"is_following":false},"join_link":null,"token":null,"owner":{"name":" ","affiliation":null,"affiliations":[],"username":null,"note":null,"link":null,"image":{"source":null,"placeholder":null},"badges":[],"verified":0,"is_verified_user":false,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},"is_protocol_requested":0,"is_group_requested":0,"is_my":false,"is_request":false,"is_confirmed":1,"is_declined":0,"requester":{"name":" ","affiliation":null,"affiliation_url":null,"username":null,"link":null},"protocol":{"id":0,"title":"Protocol: Investigating DOIs classes of errors ","title_html":"Protocol: Investigating DOIs classes of errors ","image":{"source":null,"placeholder":null},"doi":null,"doi_status":0,"uri":"protocol-investigating-dois-classes-of-errors-buuknwuw","type_id":1,"template_id":0,"published_on":null,"stats":{"number_of_views":0,"number_of_steps":0,"number_of_bookmarks":0,"number_of_comments":0,"number_of_exports":0,"number_of_runs":0,"number_of_votes":0,"is_voted":0},"parent_protocols":[],"parent_collections":[],"cited_protocols":[]},"created_on":1618332389,"resolve_on":0,"resolved_user":{"name":" ","affiliation":null,"affiliations":[],"username":null,"note":null,"link":null,"image":{"source":null,"placeholder":null},"badges":[],"verified":0,"is_verified_user":false,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},"shared":false}}],"is_owner":1,"has_subprotocols":0,"is_subprotocol":0,"is_bookmarked":0,"can_claim_authorship":0,"can_accept_authorship":0,"can_be_copied":1,"can_remove_fork":1,"fork_id":null,"url":"https:\/\/www.protocols.io\/view\/protocol-investigating-dois-classes-of-errors-buuknwuw","forks_count":{"private":0,"public":0},"access":{"can_view":1,"can_remove":0,"can_add":0,"can_edit":0,"can_publish":0,"can_get_doi":0,"can_share":1,"can_move":1,"can_move_outside":1,"can_transfer":1,"can_download":1,"is_locked":0},"guid":"E08DB1F9EDCB42959DD3F9F272C567C4","state_version_id":940,"steps":[{"id":1175662,"guid":"182D1060919F4EA1A5DA7AEDB2D13291","previous_id":null,"previous_guid":null,"modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"C1F2D3117C4C4E168C5D649C3EC1A6C7","order_id":1,"type_id":6,"title":"Section","source":{"title":"DATA IMPORT"}},{"id":1054724,"guid":"5A34EFFA11324FD994E47DC8EE643BCA","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>We import a CSV containing invalid DOI-to-DOI citations. The invalid DOIs were collected by Silvio Peroni while processing data provided by Crossref, and are provided on public licence in <\/span><span style = \"font-style:italic;\">Citations to invalid DOI-identified entities obtained from processing DOI-to-DOI citations to add in COCI<\/span><span> (Peroni, 2021).<\/span><\/div><div class = \"text-block\">The CSV is organized as follows: the first column contains valid citing DOIs and the second contains invalid cited DOIs.<\/div><\/div>"}},{"id":1054725,"guid":"AD3AB2009AE711EBB6547342859BDF69","order_id":2,"type_id":9,"title":"dataset","source":{"name":"Citations to invalid DOIs obtained from Crossref","link":"https:\/\/zenodo.org\/record\/4625300\/files\/invalid_dois.csv"}},{"id":1054726,"guid":"20506C29371848C998CD6B2F0D273831","order_id":3,"type_id":26,"title":"notes","source":{"id":0,"parent_id":0,"uri":"","title":"","body":"<div class = \"text-blocks\"><table border><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.14778\/1920841.1920954<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.5555\/646836.708343\t<\/td><\/tr><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.5406\/ethnomusicology.59.2.0202\t<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.2307\/20184517\t<\/td><\/tr><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1161\/01.cir.63.6.1391\t<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1161\/circ.37.4.509\t<\/td><\/tr><\/table><\/div>","created_on":0,"changed_on":0,"creator":{"name":" ","affiliation":null,"affiliations":[],"username":"","note":null,"link":null,"image":{"source":"","placeholder":""},"badges":[],"verified":0,"is_verified_user":false,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},"comments":[]}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#A492FF","section_duration":36000,"critical":null,"critical_id":null,"duration":0},{"id":1175663,"guid":"B8D4BC6C2952486C8F81A951EAF8DF0A","previous_id":1175662,"previous_guid":"182D1060919F4EA1A5DA7AEDB2D13291","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"0DA404C616644007BAFD610591B16177","order_id":1,"type_id":6,"title":"Section","source":{"title":""}},{"id":1054724,"guid":"67677B3AAB38438EB27ADCC99C484771","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>To make the DOI validity check procedure dramatically more efficient, Crossref's public data file containing over 120 million metadata records can be used. The reason is that the DOI names reported by Crossref at the work level are assigned by Crossref itself and not by publishers and, therefore, they are necessarily correct. The records are provided on public licence in <\/span><span style = \"font-style:italic;\">January 2021 Public Data File<\/span><span> (Crossref, 2021). <\/span><\/div><div class = \"text-block\">According to the statistics reported by Crossref at the indicated address, the compressed file weighs 102.51 GB. The download time at an average speed of 12.73 MB\/s is 2 hours.<\/div><\/div>"}},{"id":1054725,"guid":"17CA4A00A9B511EBA9B15D1C25CF0A4D","order_id":2,"type_id":9,"title":"dataset","source":{"name":"January 2021 Public Data File from Crossref","link":"https:\/\/doi.org\/10.13003\/GU3DQMJVG4"}},{"id":1054726,"guid":"21D91E3C931E402B9188A8178299D020","order_id":3,"type_id":19,"title":"safety","source":{"body":"<div class = \"text-blocks\"><div class = \"text-block\">This sub-step and the following one are completely optional and it is recommended to perform them only having at least 64 GB of RAM available since over 120 million DOIs must be stored in a set and kept in RAM during the entire process shown at step 3. <\/div><\/div>","link":""}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"","section_duration":7200,"critical":null,"critical_id":null,"duration":7200},{"id":1175664,"guid":"A00F5EA7993B4029A0BE96BB8A3D341A","previous_id":1175663,"previous_guid":"B8D4BC6C2952486C8F81A951EAF8DF0A","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"7CC13D4BFC474E9B9BDFAF17749348E5","order_id":1,"type_id":6,"title":"Section","source":{"title":""}},{"id":1054724,"guid":"FBEA4D2C66594B189E47B990A10E89C9","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>Once downloaded, the file consists of a folder containing 40,228 JSON files. The function <\/span><span style = \"font-weight:bold;\">get_all_crossref_dois()<\/span><span> is used to extract the DOI names from each work.<\/span><\/div><div class = \"text-block\">The function cycles through all the JSON files in a folder, opens them and parses them looking for works' DOIs. For this purpose the ijson library was used, that instead of loading the whole file into memory and parsing everything at once, lazily streams the data only responding to specific events, similar to what SAX does for XML. <\/div><div class = \"text-block\">Finally, the list thus created is saved in a CSV file. The CSV file is preferred over JSON for this purpose because, as there is no hierarchy in the data, CSV takes up less space as it uses fewer characters than JSON. The CSV thus obtained consists of a list of all the DOIs of all the works found in Crossref, as shown in the table below:<\/div><\/div>"}},{"id":1054725,"guid":"91236800A9BA11EBA9B15D1C25CF0A4D","order_id":2,"type_id":15,"title":"command","source":{"id":8778,"name":"import ijson, os, tqdm as tqdm\n\n\ndef get_all_crossref_dois(folder_path:str) -> list:\n        json_files = [pos_json for pos_json in os.listdir(folder_path) if pos_json.endswith('.json')]\n        dois = list()\n        pbar = tqdm(total=len(json_files))\n        for json_file in json_files:\n            with open(os.path.join(folder_path, json_file)) as json_file:\n                parser = ijson.parse(json_file)\n                for prefix, event, value in parser:\n                    if (prefix, event) == ('items.item.DOI', 'string'):\n                        dois.append({\"crossref_doi\": value})\n            pbar.update(1)\n        pbar.close()\n        return dois","command_name":"","command":null,"os_name":"","os_version":null}},{"id":1054726,"guid":"6EE1A2E052434011B982F29A43AD69B8","order_id":3,"type_id":26,"title":"notes","source":{"id":0,"parent_id":0,"uri":"","title":"","body":"<div class = \"text-blocks\"><table border><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1001\/.389<\/td><\/tr><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1001\/.391<\/td><\/tr><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1001\/.405<\/td><\/tr><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1001\/.411<\/td><\/tr><\/table><\/div>","created_on":0,"changed_on":0,"creator":{"name":" ","affiliation":null,"affiliations":[],"username":"","note":null,"link":null,"image":{"source":"","placeholder":""},"badges":[],"verified":0,"is_verified_user":false,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},"comments":[]}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"","section_duration":0,"critical":null,"critical_id":null,"duration":28800},{"id":1175665,"guid":"0AC48614228D4E8DB8BFA51DB018C582","previous_id":1175664,"previous_guid":"A00F5EA7993B4029A0BE96BB8A3D341A","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"A9276729D8D0419B90E7F01B329E76AB","order_id":1,"type_id":6,"title":"Section","source":{"title":"ERROR ANALYSIS"}},{"id":1054724,"guid":"5BB55C81761F4928B2D5C118F458A46C","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>The DOI error's taxonomy proposed in<\/span><span style = \"font-style:italic;\"> Errors in DOI indexing by bibliometric databases<\/span><span> (Franceschini et al., 2015) divides the errors in Digital Object Identifiers into two main classes: author errors, made by authors when creating the list of cited articles for their publication, and database mapping errors, related to a data-entry error. Other types of errors may be, more in general, classified as human-made errors (e.g. those introduced by editors when managing the references across publications). As proposed in<\/span><span style = \"font-style:italic;\"> Types of DOI errors of cited references in Web of Science with a cleaning method <\/span><span>(Xu et al., 2019), the errors in DOIs can be furtherly divided into prefix-type errors, suffix-type errors, and other-type errors. Our methodology uses the division proposed by Xu et al. (2019) to classify errors in DOIs, by identifying these classes of errors as belonging to the class of <\/span><span style = \"font-style:italic;\">factual errors<\/span><span>, distinguished from temporarily invalid DOIs, that should be excluded from the cleaning procedure.<\/span><\/div><div class = \"text-block\"><span>For further clarifications on this taxonomy consult the paper <\/span><span style = \"font-style:italic;\">Cleaning different types of DOI errors found in cited references on Crossref using automated methods<\/span><span> (Boente et al., 2021a).<\/span><\/div><\/div>"}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#94EBFF","section_duration":0,"critical":null,"critical_id":null,"duration":0},{"id":1175666,"guid":"3B0916EB3C2C4BC8A20CA2A13B6B6FF3","previous_id":1180858,"previous_guid":"50010120B74D11EBBC71914EE63AC4C0","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"95DA59265C0A4DD4AFF572E176E6E1BA","order_id":1,"type_id":6,"title":"Section","source":{"title":"ERROR ANALYSIS"}},{"id":1054724,"guid":"DFEE5D79F4014D97B7A7A273C15D699B","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>For cleaning <\/span><span style = \"font-weight:bold;\">prefix-type errors<\/span><span>, we took a modified version of the regular expression from Xu et al. (2019) in order to remove unwanted characters at the beginning of a DOI. The regular expression for matching prefix-type errors is the following:<\/span><\/div><div class = \"text-block\">It is worth noting that in this as in the following regular expressions the character \"O\" and the number \"0\" both count as a match, since, as demonstrated by Zhu et al. (2019), the two characters are often confused.<\/div><\/div>"}},{"id":1054725,"guid":"FDB643DC7DA445C495E3C88D3D8E7DA4","order_id":2,"type_id":26,"title":"notes","source":{"id":0,"parent_id":0,"uri":"","title":"","body":"<div class = \"text-blocks\"><div class = \"text-block\">\"(.*?)(?:\\.)?(?:HTTP:\\\/\\\/DX\\.D[0|O]I\\.[0|O]RG\\\/|HTTPS:\\\/\\\/D[0|O]I\\.[0|O]RG\\\/)(.*)\"<\/div><\/div>","created_on":0,"changed_on":0,"creator":{"name":" ","affiliation":null,"affiliations":[],"username":"","note":null,"link":null,"image":{"source":"","placeholder":""},"badges":[],"verified":0,"is_verified_user":false,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},"comments":[]}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#94EBFF","section_duration":0,"critical":null,"critical_id":null,"duration":0},{"id":1175667,"guid":"F46E2CF2F809419A893362578A660830","previous_id":1175666,"previous_guid":"3B0916EB3C2C4BC8A20CA2A13B6B6FF3","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"7D3F6003F3044822892FF1B2B8C2EAAF","order_id":1,"type_id":6,"title":"Section","source":{"title":"ERROR ANALYSIS"}},{"id":1054724,"guid":"8337F212BBFE412EA00E37F3CC4F4CD9","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>For capturing <\/span><span style = \"font-weight:bold;\">suffix-type errors<\/span><span>, we enriched the approach provided by Xu et al. (2019) and we devised a set of 17 alternative suffix regular expressions. The regular expression are the following:<\/span><\/div><div class = \"text-block\">All the suffix regular expressions can be combined in a single regular expression, by using alternatives within a non-capturing group. This is how the regular expression for suffix-type errors looks like:<\/div><\/div>"}},{"id":1054725,"guid":"19C517126F9943DAB4B73CD4C9E6ED1F","order_id":2,"type_id":26,"title":"notes","source":{"id":0,"parent_id":0,"uri":"","title":"","body":"<div class = \"text-blocks\"><div class = \"text-block\">1) \"(.*?)(?:\\\/-\\\/DCSUPPLEMENTAL)$\"<\/div><div class = \"text-block\">2) \"(.*?)(?:\\\/SUPPINF[0|O](\\.)?)$\"<\/div><div class = \"text-block\">3) \"(.*?)(?:[\\.|\\(|,|;]?PMID:\\d+.*?)$\"<\/div><div class = \"text-block\">4) \"(.*?)(?:[\\.|\\(|,|;]?PMCID:PMC\\d+.*?)$\"<\/div><div class = \"text-block\">5) \"(.*?)(?:[\\(|\\[]EPUBAHEADOFPRINT[\\)\\]])$\"<\/div><div class = \"text-block\">6) \"(.*?)(?:[\\.|\\(|,|;]?ARTICLEPUBLISHEDONLINE.*?\\d{4})$\"<\/div><div class = \"text-block\">7) \"(.*?)(?:[\\.|\\(|,|;]*HTTP:\\\/\\\/.*?)$\"<\/div><div class = \"text-block\">8) \"(.*?)(?:\\\/(META|ABSTRACT|FULL|EPDF|PDF|SUMMARY)([>|\\)](LAST)?ACCESSED\\d+)?)$\"<\/div><div class = \"text-block\">9) \"(.*?)(?:[>|)](LAST)?ACCESSED\\d+)$\"<\/div><div class = \"text-block\">10) \"(.*?)(?:[\\.|(|,|;]?[A-Z]*\\.?SAGEPUB.*)$?\"<\/div><div class = \"text-block\">11) \"(.*?)(?:\\.{5}.*?)$\"<\/div><div class = \"text-block\">12) \"(.*?)(?:[\\.|,|<|&|(|;])$\"<\/div><div class = \"text-block\">13) \"(.*?)(?:\\[DOI\\].*?)$\"<\/div><div class = \"text-block\">14) \"(.*?)(?:\\(\\d{4}\\)?)$\"<\/div><div class = \"text-block\">15) \"(.*?)(?:\\?.*?=.*?)$\"<\/div><div class = \"text-block\">16) \"(.*?)(?:#.*?)$\"<\/div><\/div>","created_on":0,"changed_on":0,"creator":{"name":" ","affiliation":null,"affiliations":[],"username":"","note":null,"link":null,"image":{"source":"","placeholder":""},"badges":[],"verified":0,"is_verified_user":false,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},"comments":[]}},{"id":1054726,"guid":"DC3A5FC05A3842A8B8C7AD94F0A23EDC","order_id":3,"type_id":26,"title":"notes","source":{"id":0,"parent_id":0,"uri":"","title":"","body":"<div class = \"text-blocks\"><div class = \"text-block\">\"(.*?)(?:\\\/-\\\/DCSUPPLEMENTAL|\\\/SUPPINF[0|O](\\.)?|[\\.|\\(|,|;]?PMID:\\d+.*?|[\\.|\\(|,|;]?PMCID:PMC\\d+.*?|[\\(|\\[]EPUBAHEADOFPRINT[\\)\\]]|[\\.|\\(|,|;]?ARTICLEPUBLISHEDONLINE.*?\\d{4}|[\\.|\\(|,|;]*HTTP:\\\/\\\/.*?|\\\/(META|ABSTRACT|FULL|EPDF|PDF|SUMMARY)([>|\\)](LAST)?ACCESSED\\d+)?|[>|\\)](LAST)?ACCESSED\\d+|[\\.|\\(|,|;]?[A-Z]*\\.?SAGEPUB.*?|\\.{5}.*?|[\\.|,|<|&|\\(|;]+|\\[DOI\\].*?|\\(\\d{4}\\)?|\\?.*?=.*?|#.*?)$\"<\/div><\/div>","created_on":0,"changed_on":0,"creator":{"name":" ","affiliation":null,"affiliations":[],"username":"","note":null,"link":null,"image":{"source":"","placeholder":""},"badges":[],"verified":0,"is_verified_user":false,"research_interests":null,"blocked_by_you":false,"blocked_you":false,"hide_following":false},"comments":[]}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#94EBFF","section_duration":0,"critical":null,"critical_id":null,"duration":0},{"id":1175668,"guid":"23E6D6A0FC7342078DE1FECA8A862CCD","previous_id":1175667,"previous_guid":"F46E2CF2F809419A893362578A660830","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"856CC7CBB60F4BFE914A43AA5066D833","order_id":1,"type_id":6,"title":"Section","source":{"title":"DATA CLEANING PROCEDURE"}},{"id":1054724,"guid":"A9625DD7A400489FB4B1A208A7EA322F","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\">Here we describe the steps through which we carried out the cleaning procedure of factually invalid DOIs. <\/div><div class = \"text-block\">The workflow of the procedure is organized as follows:<\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/protocols-files\/files\/dscybjqap.jpg\" \/><\/div><div class = \"text-block\">We created Python software for handling the whole procedure. The source code is publicly available on ISC license (Massari et al., 2021). <\/div><div class = \"text-block\"><span>For our data cleaning system, we devised two class objects: one called <\/span><span style = \"font-weight:bold;\">class<\/span><span style = \"font-weight:bold;font-weight:bold;\"> <\/span><span style = \"font-weight:bold;\">Clean_DOIs()<\/span><span>, with all the methods required to carry the cleaning\/validation procedure, and <\/span><span style = \"font-weight:bold;\">class Support()<\/span><span>, with all the methods required to process and dump CSV as well as carry HTTP requests. In this protocol, we describe the use of the different methods that can be imported from the two classes. If you want to reproduce the procedure described in the next step, once you have verified to have Python 3.x installed and the required libraries, you can clone our Github repository and launch the <\/span><span style = \"font-style:italic;\">tutorial.py <\/span><span>file in a terminal inside the folder or you can execute in a Python console the commands listed in the following sub-sections.<\/span><\/div><\/div>"}},{"id":1054725,"guid":"B2BA3C7EFD4344B0AC23C79A28A18FC6","order_id":2,"type_id":1,"title":"description","source":{"description":"<div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/protocols-files\/files\/dscybjqap.jpg\" \/><\/div>"}},{"id":1054726,"guid":"5F1858B0A50611EB873D85B4C91630DC","order_id":3,"type_id":15,"title":"command","source":{"id":8779,"name":"python tutorial.py","command_name":"Launch tutorial.py","command":null,"os_name":"","os_version":null}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#84CE84","section_duration":0,"critical":null,"critical_id":null,"duration":0},{"id":1175669,"guid":"CB8E71BEBF734E63BEC9779D0A2D76A7","previous_id":1175668,"previous_guid":"23E6D6A0FC7342078DE1FECA8A862CCD","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"6256020350324C369A26C52EC8D017E8","order_id":1,"type_id":6,"title":"Section","source":{"title":"DATA CLEANING PROCEDURE"}},{"id":1054724,"guid":"09FE8D5E0EB740C9AF5C1F51AC424C90","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>First, with the function <\/span><span style = \"font-weight:bold;\">process_csv_input()<\/span><span> from the <\/span><span style = \"font-weight:bold;\">class<\/span><span style = \"font-weight:bold;font-weight:bold;\"> <\/span><span style = \"font-weight:bold;\">Support()<\/span><span>, we import the data in CSV format and store it in a <\/span><span style = \"font-style:italic;\">DictReader<\/span><span>, an<\/span><span style = \"font-weight:bold;\"> <\/span><span>object that maps the information in each row to a dictionary whose keys are given by the optional <\/span><span style = \"font-style:italic;\">fieldnames <\/span><span>parameter.<\/span><\/div><div class = \"text-block\"><span>We perform this method on our input CSV and store it as a <\/span><span style = \"font-weight:bold;\"><data><\/span><span> variable. Optionally, it is also possible to import in the <\/span><span style = \"font-weight:bold;\"><crossref_dois><\/span><span> variable the Crossref DOIs previously stored in a CSV file as shown in step 1.2. It is recommended to perform this step having at least 64GB of RAM, as over 120 million DOIs need to be stored in a set and kept on RAM throughout the entire process. You can reproduce the experiment by launching:<\/span><\/div><div class = \"text-block\"><span>After this procedure, the <\/span><span style = \"font-weight:bold;\"><data><\/span><span> variable (and, if included, the <\/span><span style = \"font-weight:bold;\"><crossref_dois><\/span><span> variable) will consist of a list of dictionaries, as shown below:<\/span><\/div><\/div>"}},{"id":1054725,"guid":"EB1ED650A9BA11EBA9B15D1C25CF0A4D","order_id":2,"type_id":15,"title":"command","source":{"id":8780,"name":"@staticmethod\ndef process_csv_input(path:str) -> list:\n    print(f\"[Support:INFO Proccessing csv at path {path}]\")\n    with open(path, 'r', encoding='utf-8') as csvfile:\n        reader = csv.DictReader(csvfile)\n        return list(reader)","command_name":"","command":null,"os_name":"","os_version":""}},{"id":1054726,"guid":"813AF6C0A76611EBB340838B39CCBF73","order_id":3,"type_id":15,"title":"command","source":{"id":8781,"name":"from support import Support\nfrom clean_dois import Clean_DOIs\n\ndata = Support.process_csv_input(\".\/dataset\/invalid_dois.csv\")\ncrossref_dois = Support.process_csv_input(\".\/dataset\/crossref_dois.csv\")","command_name":"","command":null,"os_name":null,"os_version":null}},{"id":1054727,"guid":"A1372542F9994A02A858E8F4D42D633A","order_id":4,"type_id":17,"title":"result","source":{"body":"<div class = \"text-blocks\"><div class = \"text-block\">[{\"Valid_citing_DOI\": \"10.14778\/1920841.1920954\", \"Invalid_cited_DOI\": \"10.5555\/646836.708343\"}, {\"Valid_citing_DOI\": \"10.5406\/ethnomusicology.59.2.0202\", \"Invalid_cited_DOI\": \"10.2307\/20184517\"}, <\/div><div class = \"text-block\">...]<\/div><\/div>"}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#84CE84","section_duration":0,"critical":null,"critical_id":null,"duration":0},{"id":1175670,"guid":"2E11CDCC3C594EB08E81D1B621DCF995","previous_id":1175669,"previous_guid":"CB8E71BEBF734E63BEC9779D0A2D76A7","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"3B60F69D970A47EBA441508EB8436998","order_id":1,"type_id":6,"title":"Section","source":{"title":"DATA CLEANING PROCEDURE"}},{"id":1054724,"guid":"A8CE03E4D9EF45FF82DBB82F13B03C5C","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>The procedure first checks for each cited DOI if it is factually invalid or it has become valid in the meanwhile. For this purpose, the function <\/span><span style = \"font-weight:bold;\">check_dois_validity() <\/span><span>from the class <\/span><span style = \"font-weight:bold;\">Clean_DOIs() <\/span><span>is used, which takes in input a list of dictionaries, like the one described above, and does a GET request to the DOI Proxy with the cited DOI (<\/span><\/div><div class = \"text-block\"><a href=\"https:\/\/www.doi.org\/factsheets\/DOIProxy.html\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">https:\/\/www.doi.org\/factsheets\/DOIProxy.html<\/span><\/a><\/div><div class = \"text-block\"><span>) of each row of the CSV: if the status code corresponding to that specific DOI is different from 1, it means that it is not valid; otherwise, that DOI has become valid in the meanwhile. The function outputs an enriched version of the list of dictionaries in input, with two additional fields: <\/span><span style = \"font-style:italic;\">\"Valid_DOI\"<\/span><span>, which stores a validated DOI, and <\/span><span style = \"font-style:italic;\">\"Already_valid\"<\/span><span>, which stores 1 if it has become valid in the meanwhile, 0 otherwise.<\/span><\/div><div class = \"text-block\"><span>In case the optional <\/span><span style = \"font-weight:bold;\"><crossref_dois><\/span><span> argument has been passed, a set containing all DOIs in the corresponding dictionaries list is generated. The set was chosen and not the list because, having all the values hashed, makes the content check instantaneous, at the expense of more RAM. Therefore, the request to the DOI Proxy Server is made only if the current DOI is not found in the set, in which case it is registered as valid.<\/span><\/div><div class = \"text-block\"><span>We can now instantiate the <\/span><span style = \"font-weight:bold;\">Clean_DOIs() <\/span><span>class, passing none or more of the optional parameters <\/span><span style = \"font-weight:bold;\"><crossref_dois><\/span><span>, <\/span><span style = \"font-weight:bold;\"><cache_path><\/span><span> and<\/span><span style = \"font-weight:bold;\"> <logs><\/span><span>,. Then, we run the <\/span><span style = \"font-weight:bold;\">check_dois_validity() <\/span><span>function passing the <\/span><span style = \"font-weight:bold;\"><data><\/span><span> and storing the results in a <\/span><span style = \"font-weight:bold;\"><checked_dois><\/span><span> variable:<\/span><\/div><div class = \"text-block\"><span>Depending on the dataset size, on the bandwidth and whether or not the <\/span><span style = \"font-weight:bold;\"><crossref_dois> <\/span><span>parameter has been used, this step can take a long time, up to a week in the case of the over one million DOIs of the input dataset mentioned at step 1 at an average bandwidth of 100 Mb\/s.<\/span><\/div><div class = \"text-block\"><span>Once the execution is complete, this is how the <\/span><span style = \"font-weight:bold;\"><checked_dois><\/span><span> variable appears:<\/span><\/div><\/div>"}},{"id":1054725,"guid":"0CFCE7F0A9C311EBA9B15D1C25CF0A4D","order_id":2,"type_id":15,"title":"command","source":{"id":8782,"name":"def check_dois_validity(self, data:list, autosave_path:str=\"\", cache_every:int=100) -> list:\n        if autosave_path != \"\":\n            start_index, checked_dois = Support.read_cache(autosave_path=autosave_path)\n            pbar = tqdm(total=len(data)-start_index)\n            data = islice(data, start_index + 1, None)\n        else:\n            checked_dois = list()\n            pbar = tqdm(total=len(data))\n        i = 0\n        for row in data:\n            if autosave_path != \"\" and i == cache_every:\n                Support.dump_csv(data=checked_dois, path=autosave_path)\n                i = 0\n            valid_citing_doi = row[\"Valid_citing_DOI\"].lower()\n            invalid_cited_doi = row[\"Invalid_cited_DOI\"].lower()\n            invalid_dictionary = {\n                \"Valid_citing_DOI\": valid_citing_doi,\n                \"Invalid_cited_DOI\": invalid_cited_doi, \n                \"Valid_DOI\": \"\",\n                \"Already_valid\": 0\n            }\n            if invalid_cited_doi in self.crossref_dois:\n                handle = {\"responseCode\": 1}\n            else:\n                handle = Support().handle_request(url=f\"https:\/\/doi.org\/api\/handles\/{invalid_cited_doi}\", cache_path=self.request_cache, error_log_dict=self.logs)\n            if handle is not None:\n                if handle[\"responseCode\"] == 1:\n                    checked_dois.append(\n                        {\"Valid_citing_DOI\": valid_citing_doi,\n                        \"Invalid_cited_DOI\": invalid_cited_doi, \n                        \"Valid_DOI\": invalid_cited_doi,\n                        \"Already_valid\": 1\n                        })\n                else:\n                    checked_dois.append(invalid_dictionary)       \n            else:\n                checked_dois.append(invalid_dictionary)\n            i += 1             \n            pbar.update(1)\n        pbar.close()\n        return checked_dois","command_name":"","command":null,"os_name":null,"os_version":null}},{"id":1054726,"guid":"CDEB1310A76611EBB340838B39CCBF73","order_id":3,"type_id":15,"title":"command","source":{"id":8784,"name":"doi_logs = dict()\nclean_dois = Clean_DOIs(logs=doi_logs)\nchecked_dois = clean_dois.check_dois_validity(data=data, autosave_path=\".\/cache\/checked_dois.csv\")","command_name":"","command":null,"os_name":null,"os_version":null}},{"id":1054727,"guid":"A036053ADEC24B788A58847EFA1E9548","order_id":4,"type_id":17,"title":"result","source":{"body":"<div class = \"text-blocks\"><div class = \"text-block\">[{\"Valid_citing_DOI\": \"10.1007\/s11771-020-4410-2\", \"Invalid_cited_DOI\": \"10.13745\/j.esf.2016.02.011\", \"Valid_DOI\": \"10.13745\/j.esf.2016.02.011\", Already_valid\": 1},<\/div><div class = \"text-block\">...]<\/div><\/div>"}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#84CE84","section_duration":0,"critical":null,"critical_id":null,"duration":604800},{"id":1175671,"guid":"0D6D1A0FFAC04554A4B474E6D4E177DF","previous_id":1175670,"previous_guid":"2E11CDCC3C594EB08E81D1B621DCF995","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"C2ABEBC8CCF3435C8D1A8CCCA89B801B","order_id":1,"type_id":6,"title":"Section","source":{"title":"DATA CLEANING PROCEDURE"}},{"id":1054724,"guid":"6DFA587F854049829491C6FA46C19D2F","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>Afterwards, we pass the <\/span><span style = \"font-weight:bold;\"><checked_dois><\/span><span> variable obtained in the previous step to another function called <\/span><span style = \"font-weight:bold;\">procedure(),<\/span><span> aimed to clean the DOIs with the regular expressions described above and to verify the validity of the cleaned DOIs.<\/span><\/div><div class = \"text-block\"><span>In our data cleaning procedure we store the output of the <\/span><span style = \"font-weight:bold;\">procedure() <\/span><span>function in the <\/span><span style = \"font-weight:bold;\"><output> <\/span><span>variable:<\/span><\/div><div class = \"text-block\"><span>For each cited DOI, the function <\/span><span style = \"font-weight:bold;\">procedure() <\/span><span>performs five actions:<\/span><\/div><div class = \"text-block\"><ol style = \"list-style-type: decimal;\"><li style = \"counter-reset:ol0;\">it removes white spaces in the string;<\/li><li style = \"counter-reset:ol0;\">it applies the prefix regular expression to catch unwanted characters that precede a DOI. If there is a match, the regular expression removes the unwanted characters and the DOI is replaced with the cleaned one;<\/li><li style = \"counter-reset:ol0;\">it applies the suffix regular expressions as a list of alternative unwanted patterns that may occur at the end of the string. If there is a match, the regular expression removes the unwanted characters and the DOI is replaced with the cleaned one;<\/li><li style = \"counter-reset:ol0;\">it removes unwanted characters, like double slashes, double points, XML tags and backward slashes;<\/li><li style = \"counter-reset:ol0;\"><span>finally, after the cleaning procedure, the algorithm checks if the cited DOI is different from the one in <\/span><span style = \"font-weight:bold;\"><checked_dois><\/span><span>, if it is contained in the <\/span><span style = \"font-weight:bold;\"><crossref_dois> <\/span><span>set or returned as valid by the DOI Handle API. If it is different and is validated, the algorithm stores the cleaned DOI in the \"<\/span><span style = \"font-style:italic;\">Valid_DOI\"  field<\/span><span> of the dictionary; if it is not different, the dictionary will store no new DOI.<\/span><\/li><\/ol><\/div><div class = \"text-block\"><span>By applying <\/span><span style = \"font-weight:bold;\">procedure() <\/span><span>to our <\/span><span style = \"font-weight:bold;\"><checked_dois><\/span><span> we obtain a list of dictionaries containing 7 fields: the four already present in the input list (\"<\/span><span style = \"font-style:italic;\">Valid_citing_DOI\"<\/span><span style = \"font-weight:bold;font-style:italic;font-style:italic;\">,<\/span><span style = \"font-weight:bold;font-style:italic;\"> <\/span><span style = \"font-style:italic;\">\"Invalid_cited_DOI\"<\/span><span>, <\/span><span style = \"font-style:italic;\">\"Valid_DOI\", <\/span><span style = \"font-style:italic;font-style:italic;\">\"<\/span><span style = \"font-style:italic;\">Already_Valid\"<\/span><span>) and, in addition, <\/span><span style = \"font-style:italic;\">\"prefix_error\"<\/span><span>, <\/span><span style = \"font-style:italic;\">\"suffix_error\"<\/span><span>, <\/span><span style = \"font-style:italic;\">\"other-type<\/span><span style = \"font-weight:bold;font-style:italic;\">\"<\/span><span style = \"font-weight:bold;\"> <\/span><span>fields, which contains, for each cleaned DOI, the number of errors that were cleaned. This is how the <\/span><span style = \"font-weight:bold;\"><output><\/span><span> variable looks like:<\/span><\/div><div class = \"text-block\"><span>In order to handle the various exceptions that could occur during a request to a server, and to save the results already obtained and not have to repeat them in the event of a crash, blackout or other incidents, two different strategies have been employed. First, the <\/span><span style = \"font-weight:bold;\">handle_request ()<\/span><span> function handles HTTP 500 errors, sets a 10-second timeout, repeats the request a second time on an error, and caches the responses. On the other hand, if a cache file has been created, the <\/span><span style = \"font-weight:bold;\">read_cache () <\/span><span>function reads the data processed up to that moment, in order to restart from the last line read and not from the beginning. This function has been integrated into both <\/span><span style = \"font-weight:bold;\">check_dois_validity()<\/span><span> and <\/span><span style = \"font-weight:bold;\">procedures()<\/span><span>, that create or update a cache file every n DOIs. The number of DOIs after which to update the cache is customizable, as is<\/span><span style = \"font-weight:bold;\"> <\/span><span>the location of the cache file.<\/span><\/div><\/div>"}},{"id":1054725,"guid":"CF833D10A9C811EBA9B15D1C25CF0A4D","order_id":2,"type_id":15,"title":"command","source":{"id":8785,"name":"def procedure(self, data:list, autosave_path:str=\"\", cache_every:int=100) -> list:\n        if autosave_path != \"\":\n            start_index, output = Support.read_cache(autosave_path=autosave_path)\n            pbar = tqdm(total=len(data)-start_index)\n            data = islice(data, start_index + 1, None)\n        else:\n            output = list()\n            pbar = tqdm(total=len(data))\n        i = 0\n        for row in data:\n            if autosave_path != \"\" and i == cache_every:\n                Support.dump_csv(data=output, path=autosave_path)\n                i = 0\n            valid_citing_doi = row[\"Valid_citing_DOI\"].lower()\n            invalid_cited_doi = row[\"Invalid_cited_DOI\"].lower()\n            already_valid = row[\"Already_valid\"]\n            unclean_dictionary = {\n                \"Valid_citing_DOI\": valid_citing_doi,\n                \"Invalid_cited_DOI\": invalid_cited_doi,\n                \"Valid_DOI\": row[\"Valid_DOI\"].lower(),\n                \"Already_valid\": already_valid,\n                \"Prefix_error\": 0,\n                \"Suffix_error\": 0,\n                \"Other-type_error\": 0\n            }\n            if int(already_valid) == 0:\n                new_doi, classes_of_errors = self.clean_doi(invalid_cited_doi)\n                new_doi = new_doi.lower()\n                clean_dictionary = {\n                    \"Valid_citing_DOI\": valid_citing_doi,\n                    \"Invalid_cited_DOI\": invalid_cited_doi,\n                    \"Valid_DOI\": new_doi,\n                    \"Already_valid\": already_valid,\n                    \"Prefix_error\": classes_of_errors[\"prefix\"],\n                    \"Suffix_error\": classes_of_errors[\"suffix\"],\n                    \"Other-type_error\": classes_of_errors[\"other-type\"]\n                }\n                if new_doi != invalid_cited_doi:\n                    if new_doi in self.crossref_dois:\n                        handle = {\"responseCode\": 1}\n                    else:\n                        handle = Support().handle_request(url=f\"https:\/\/doi.org\/api\/handles\/{new_doi}\", cache_path=self.request_cache, error_log_dict=self.logs)\n                    if handle is not None:\n                        if handle[\"responseCode\"] == 1:\n                            output.append(clean_dictionary)\n                        else:\n                            output.append(unclean_dictionary)\n                    else:\n                        output.append(unclean_dictionary)\n                elif new_doi == invalid_cited_doi:\n                    output.append(unclean_dictionary)\n            elif int(already_valid) == 1:\n                output.append(unclean_dictionary)\n            pbar.update(1)\n            i += 1\n        pbar.close()\n        return output","command_name":"","command":null,"os_name":null,"os_version":null}},{"id":1054726,"guid":"61909EF0A76711EBB340838B39CCBF73","order_id":3,"type_id":15,"title":"command","source":{"id":8786,"name":"output = clean_dois.procedure(data=checked_dois, autosave_path=\".\/cache\/output.csv\")","command_name":"","command":null,"os_name":null,"os_version":null}},{"id":1054727,"guid":"40DEFCFDEC24479A867CFEB86023519B","order_id":4,"type_id":17,"title":"result","source":{"body":"<div class = \"text-blocks\"><div class = \"text-block\">[{\"Valid_citing_DOI\": \"10.1007\/s40617-018-00299-1\", \"Invalid_cited_DOI\": \"10.1901\/jaba.2012.45-657\", \"Valid_DOI\": \"10.1901\/JABA.2012.45-657\", Already_valid\": 1, \"prefix_error\": 0, \"suffix_error\" 0:, \"other-type\": 0 }, <\/div><div class = \"text-block\">{\"Valid_citing_DOI\": \"10.1074\/jbc.m508416200\", \"Invalid_cited_DOI\": \"10.1059\/0003-4819-100-4-483\", \"Valid_DOI\": \"\", Already_valid\": 0, \"prefix_error\": 0, \"suffix_error\" 0:, \"other-type\": 0 }, <\/div><div class = \"text-block\">{\"Valid_citing_DOI\": \"10.1177\/2054358119836124\", \"Invalid_cited_DOI\": \"10.1016\/j.amepre.2015.07.017.\", \"Valid_DOI\": \"10.1016\/J.AMEPRE.2015.07.017\", Already_valid\": 0, \"prefix_error\": 0, \"suffix_error\" 1:, \"other-type\": 0 },<\/div><div class = \"text-block\">...]<\/div><\/div>"}},{"id":1054728,"guid":"DD37834FB00F11EBB2730A58A9FEAC02","order_id":5,"type_id":15,"title":"command","source":{"id":8783,"name":"def _requests_retry_session(\n        self,\n        tries=2,\n        status_forcelist=(500, 502, 504, 520, 521),\n        session=None\n    ) -> Session:\n        session = session or requests.Session()\n        retry = Retry(\n            total=tries,\n            read=tries,\n            connect=tries,\n            status_forcelist=status_forcelist,\n        )\n        adapter = HTTPAdapter(max_retries=retry)\n        session.mount('http:\/\/', adapter)\n        session.mount('https:\/\/', adapter)\n        return session\n    \n    def handle_request(self, url:str, cache_path:str=\"\", error_log_dict:dict=dict()):\n        if cache_path != \"\":\n            requests_cache.install_cache(cache_path)\n        try:\n            data = self._requests_retry_session().get(url, timeout=10)\n            if data.status_code == 200:\n                return data.json()\n            else:\n                error_log_dict[url] = data.status_code\n        except Exception as e:\n            error_log_dict[url] = str(e)\n\n    @staticmethod\n    def read_cache(autosave_path:str) -> Tuple[int, list]:\n        num = 0\n        data = list()\n        if not os.path.exists(autosave_path):\n            return num, data\n        with open(autosave_path, 'r', encoding='utf8') as read_obj:\n            dict_reader = csv.DictReader(read_obj)\n            for row in dict_reader:\n                row_data = {\n                    \"Valid_citing_DOI\": row.get(\"Valid_citing_DOI\"),\n                    \"Invalid_cited_DOI\": row.get(\"Invalid_cited_DOI\"),\n                    \"Valid_DOI\": row.get(\"Valid_DOI\"),\n                    \"Already_valid\": row.get(\"Already_valid\"),\n                    \"Prefix_error\": row.get(\"Prefix_error\"),\n                    \"Suffix_error\": row.get(\"Suffix_error\"),\n                    \"Other-type_error\": row.get(\"Other-type_error\")\n                }\n                data.append(row_data)\n                num += 1\n            return num, data","command_name":"","command":null,"os_name":null,"os_version":null}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#84CE84","section_duration":0,"critical":null,"critical_id":null,"duration":86400},{"id":1175672,"guid":"4257576750E7472FAAF4162440144477","previous_id":1175671,"previous_guid":"0D6D1A0FFAC04554A4B474E6D4E177DF","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"A7491C52AE64448DB02EEEA2F48A15E0","order_id":1,"type_id":6,"title":"Section","source":{"title":"DATA CLEANING PROCEDURE"}},{"id":1054724,"guid":"899C27B3BBB94F2C8B7401EAB194350D","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>At the end of our procedure we dump our <\/span><span style = \"font-weight:bold;\"><output> <\/span><span>in a csv file through the <\/span><span style = \"font-weight:bold;\">dump_csv()<\/span><span> method of the class <\/span><span style = \"font-weight:bold;\">Support()<\/span><span style = \"font-weight:bold;\">:<\/span><\/div><div class = \"text-block\">This is how the method is used in our data cleaning procedure:<\/div><div class = \"text-block\">The output of the function is then stored in a CSV file, organized in 7 fields as the dictionary mentioned above:<\/div><table border><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1007\/s40617-018-00299-1<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1901\/jaba.2012.45-657<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1901\/JABA.2012.45-657<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">1<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><\/tr><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1074\/jbc.m508416200<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1059\/0003-4819-100-4-483<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\"><\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><\/tr><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1177\/2054358119836124<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1016\/j.amepre.2015.07.017.<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1016\/J.AMEPRE.2015.07.017<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">1<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><\/tr><\/table><div class = \"text-block\">The output dataset is available on Zenodo (Boente et al., 2021b).<\/div><div class = \"text-block\">Further information about the possible uses of research data generated by this project, and the support provided by the authors for reuse, are provided in our Data Management Plan, available on Zenodo (Boente et al., 2021c).<\/div><\/div>"}},{"id":1054725,"guid":"18C58D60A9CA11EBA9B15D1C25CF0A4D","order_id":2,"type_id":15,"title":"command","source":{"id":8787,"name":"@staticmethod\ndef dump_csv(data:list, path:str):\n    print(f\"[Support:INFO Writing csv at path {path}]\")\n    with open(path, 'w', newline='', encoding='utf8')  as output_file:\n        keys = data[0].keys()\n        dict_writer = csv.DictWriter(output_file, keys)\n        dict_writer.writeheader()\n        dict_writer.writerows(data)","command_name":"","command":null,"os_name":null,"os_version":null}},{"id":1054726,"guid":"7F4599F0A76711EBB340838B39CCBF73","order_id":3,"type_id":15,"title":"command","source":{"id":8788,"name":"Support.dump_csv(data=output, path=\".\/output.csv\")","command_name":"","command":null,"os_name":null,"os_version":null}},{"id":1054727,"guid":"13B1F6774F3D4345BED72FA1A74CDC4F","order_id":4,"type_id":1,"title":"description","source":{"description":"<table border><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1007\/s40617-018-00299-1<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1901\/jaba.2012.45-657<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1901\/JABA.2012.45-657<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">1<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><\/tr><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1074\/jbc.m508416200<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1059\/0003-4819-100-4-483<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\"><\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><\/tr><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1177\/2054358119836124<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1016\/j.amepre.2015.07.017.<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">10.1016\/J.AMEPRE.2015.07.017<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">1<\/td><td rowspan = \"\" colspan = \"\" style =\"display : table-cell;\">0<\/td><\/tr><\/table>"}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#84CE84","section_duration":0,"critical":null,"critical_id":null,"duration":0},{"id":1175673,"guid":"2050DC446409491297399A21D342BEE3","previous_id":1175672,"previous_guid":"4257576750E7472FAAF4162440144477","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"92322E2FE70C423186B8B3D3A7062999","order_id":1,"type_id":6,"title":"Section","source":{"title":"STATISTICS AND VISUALIZATION"}},{"id":1054724,"guid":"1F80FE62B8E244DCB57ED97FE876E951","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\">Finally, two visualizations were produced from the output file. For this purpose, d3.js v6.7.0, an open-source Javascript library for manipulating data-driven documents, was used (Bostock, 2021). In particular, a bar chart and a treemap have been created, which compare the number of DOIs still not valid after the cleaning procedure with those that have been corrected, and the number of those belonging to the four classes of errors considered, i.e. DOIs already valid and DOIs with prefix, suffix, or other-type errors. In addition, the bar chart allows comparing the results obtained through this study with those that would have been obtained using the regular expressions suggested by Xu et al. (2019). Finally, the bar chart can be sorted in ascending or descending order and it is possible to hover the mouse over both visualizations to highlight the represented value and the percentage of the total. It is possible to interact with the visualization at the following address: <\/div><div class = \"text-block\"><a href=\"https:\/\/open-sci.github.io\/2020-2021-grasshoppers-code\/\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">https:\/\/open-sci.github.io\/2020-2021-grasshoppers-code\/<\/span><\/a><\/div><div class = \"text-block\">.<\/div><\/div>"}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#FFED92","section_duration":0,"critical":null,"critical_id":null,"duration":0},{"id":1180858,"guid":"50010120B74D11EBBC71914EE63AC4C0","previous_id":1175665,"previous_guid":"0AC48614228D4E8DB8BFA51DB018C582","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"24FB5F65691247CA87024054E74507C6","order_id":1,"type_id":6,"title":"Section","source":{"title":"ERROR ANALYSIS"}},{"id":1054724,"guid":"71A1B1157E24440D9ED7A44EB212631D","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>To find recurrent errors to automatically clean, we manually isolated from the dataset a subset of strings at the beginning and at the end of factually invalid DOIs, which contained prefix or suffix errors. In addition, we found other types of errors, like double underscores, double periods, XML tags, spaces, and forward slashes that could be removed at the end of the data cleaning process. A sample of this subset of corrupting string is provided in Table 3 of the article <\/span><span style = \"font-style:italic;\">Cleaning different types of DOI errors found in cited references on Crossref using automated methods<\/span><span> (Boente et al., 2021a).<\/span><\/div><\/div>"}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#94EBFF","section_duration":0,"critical":null,"critical_id":null,"duration":0},{"id":1190923,"guid":"66D2D640C48511EBAC161FFF03B7E57B","previous_id":1175673,"previous_guid":"2050DC446409491297399A21D342BEE3","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"D1EC9B16433F4555BF41025DB5C49A73","order_id":1,"type_id":6,"title":"Section","source":{"title":"REFERENCES"}},{"id":1054724,"guid":"CFDEA513F0FE4D688693840113F972D6","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>Boente, R., Massari, A., Santini, C., & Tural, D. (2021a). Cleaning different types of DOI errors found in cited references on Crossref using automated methods<\/span><span style = \"font-style:italic;\"> <\/span><span>(Version 2). Zenodo. <\/span><\/div><div class = \"text-block\"><a href=\"http:\/\/doi.org\/10.5281\/zenodo.4734513\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">http:\/\/doi.org\/10.5281\/zenodo.4734513<\/span><\/a><\/div><div class = \"text-block\"><span>Boente, R., Massari, A., Santini, C., & Tural, D. (2021b). Classes of errors in DOI names: output dataset (Version v1.0.0-alpha) [Data set].<\/span><span style = \"font-style:italic;\"> <\/span><span>Zenodo. <\/span><\/div><div class = \"text-block\"><a href=\"http:\/\/doi.org\/10.5281\/zenodo.4733647\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">http:\/\/doi.org\/10.5281\/zenodo.4733647<\/span><\/a><\/div><div class = \"text-block\">Boente, Ricarda, Massari, Arcangelo, Santini, Cristian, & Tural, Deniz. (2021c). Classes of errors in DOI names (Data Management Plan) (Version 5). Zenodo. <\/div><div class = \"text-block\"><a href=\"https:\/\/doi.org\/10.5281\/zenodo.4733919\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">10.5281\/zenodo.4733919<\/span><\/a><\/div><div class = \"text-block\">Bostock, M. (2021). D3: Data-Driven Documents. Software Heritage. https:\/\/archive.softwareheritage.org\/swh:1:dir:35fe697ae5a21e96d9fc01d890b30010e23c16dd<\/div><div class = \"text-block\">Crossref. (2021). January 2021 Public Data File. <\/div><div class = \"text-block\"><a href=\"https:\/\/doi.org\/10.13003\/GU3DQMJVG4\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">https:\/\/doi.org\/10.13003\/GU3DQMJVG4<\/span><\/a><\/div><div class = \"text-block\"><span>Franceschini, F., Maisano, D., & Mastrogiacomo, L. (2015).<\/span><span style = \"font-style:italic;\"> <\/span><span>Errors in DOI indexing by bibliometric databases. Scientometrics, 102(3), 2181\u20132186. <\/span><\/div><div class = \"text-block\"><a href=\"https:\/\/doi.org\/10.1007\/s11192-014-1503-4\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">https:\/\/doi.org\/10.1007\/s11192-014-1503-4<\/span><\/a><\/div><div class = \"text-block\">Massari, A., Santini, C., & Boente, R.. (2021). open-sci\/2020-2021-grasshoppers-code: Classes of errors in DOI names (Version v1.1.0). Zenodo. <\/div><div class = \"text-block\"><a href=\"http:\/\/doi.org\/10.5281\/zenodo.4734013\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">http:\/\/doi.org\/10.5281\/zenodo.4734013<\/span><\/a><\/div><div class = \"text-block\">Peroni, S. (2021). Citations to invalid DOI-identified entities obtained from processing DOI-to-DOI citations to add in COCI (Version 1.0) [Data set]. Zenodo. <\/div><div class = \"text-block\"><a href=\"http:\/\/doi.org\/10.5281\/zenodo.4625300\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">http:\/\/doi.org\/10.5281\/zenodo.4625300<\/span><\/a><\/div><div class = \"text-block\"><span>Xu, S., Hao, L., An, X., Zhai, D., & Pang, H. (2019). Types of DOI errors of cited references in Web of Science with a cleaning method. Scientometrics<\/span><span style = \"font-style:italic;\"> <\/span><span>120, 1427\u20131437. <\/span><\/div><div class = \"text-block\"><a href=\"https:\/\/doi.org\/10.1007\/s11192-019-03162-4\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">https:\/\/doi.org\/10.1007\/s11192-019-03162-4<\/span><\/a><\/div><div class = \"text-block\"><span>Zhu, J., Hu, G. & Liu, W. (2019). DOI errors and possible solutions for Web of Science<\/span><span style = \"font-style:italic;\">. <\/span><span>Scientometrics 118, 709\u2013718. <\/span><\/div><div class = \"text-block\"><a href=\"https:\/\/doi.org\/10.1007\/s11192-018-2980-7\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">https:\/\/doi.org\/10.1007\/s11192-018-2980-7<\/span><\/a><\/div><\/div>"}}],"cases":[],"data":null,"html":null,"section":null,"section_color":"#EA9F6C","section_duration":0,"critical":null,"critical_id":null,"duration":0}],"document":"","materials":[],"description":"<div class = \"text-blocks\"><div class = \"text-block\">The purpose of this protocol is to provide an automated process to repair invalid DOIs that have been collected by the OpenCitations Index Of Crossref Open DOI-To-DOI References (COCI) while processing data provided by Crossref.<\/div><div class = \"text-block\"><span>The data needed for this work is <\/span><a href=\"http:\/\/doi.org\/10.5281\/zenodo.4625300\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">provided by Silvio Peroni as a CSV<\/span><\/a><span> containing pairs of valid citing DOIs and invalid cited DOIs. With the goal to determine an automated process, we first classified the errors that characterize the wrong DOIs in the list. The starting hypothesis is that there are two main classes of errors: factual errors, such as wrong characters, and DOIs that are not yet valid at the time of processing. The first class can be furtherly divided into three classes: errors due to irrelevant strings added to the beginning (prefix-type errors) or at the end (suffix-type errors) of the correct DOI, and errors due to unwanted characters in the middle (other-type errors). Once the classes of errors are addressed, we propose automatic processes to obtain correct DOIs from wrong ones. These processes involve the use of the information returned from <\/span><a href=\"https:\/\/www.doi.org\/factsheets\/DOIProxy.html\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">DOI API<\/span><\/a><span>, the <\/span><a href=\"https:\/\/doi.org\/10.13003\/GU3DQMJVG4\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">January 2021 Public Data File from Crossref<\/span><\/a><span>, as well as rule-based methods, including regular expressions to correct invalid DOIs.<\/span><\/div><div class = \"text-block\">The application of this methodology produced a CSV dataset containing all the pairs of citing and cited DOIs in the original dataset, each one enriched by 5 fields: \"Already_Valid\", which tells if the cited DOI was already valid before cleaning, \"New_DOI\", which contain a clean, valid DOI (if our procedure was able to produce one), and \"prefix_error\", \"suffix_error\" and \"other-type_error\" fields, which contain, for each cleaned DOI the number of errors that were cleaned.<\/div><\/div>","changed_on":1623174906}