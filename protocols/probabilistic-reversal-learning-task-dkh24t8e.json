{"access":{"can_view":true,"can_edit":false,"can_remove":false,"can_add":false,"can_publish":false,"can_get_doi":false,"can_share":true,"can_move":true,"can_move_outside":true,"can_transfer":true,"can_download":true,"limited_run":false,"limited_private_links":false,"limited_blind_links":false,"is_locked":false},"authors":[{"name":"Alexandra Nelson","affiliation":"UCSF","affiliation_url":null,"username":"n4qle1w1s115wle1","link":null,"user_image_file":{"guid":"C37265DF5B2B11EFA2EF0A58A9FEAC02","file_name":"AItbvmkZsbo6pKPuo0WHe6GYowH5G6IQkVnSyhZ3Vc11=s96-c.png","url":"https://files.protocols.io/external/AItbvmkZsbo6pKPuo0WHe6GYowH5G6IQkVnSyhZ3Vc11s96c-C37265DF5B2B11EFA2EF0A58A9FEAC02.png","mime":"image/png","size":1335,"width":0,"height":0,"avg_color":"","scan_status":0,"created_at":1723742899},"note":"","is_verified_user":true},{"name":"Berenice Coutant","affiliation":"UCSF","affiliation_url":null,"username":"berenice-coutant","link":null,"user_image_file":{"guid":"00000000000000000000000000000001","file_name":"avatar.png","url":"https://www.protocols.io/img/avatars/001.png","mime":"image/png","size":0,"width":100,"height":100,"avg_color":"","scan_status":0,"created_at":0},"note":"","is_verified_user":true},{"name":"Xiaowen Zhuang","affiliation":"UCSF","affiliation_url":null,"username":"n4vle152z1t4vle1","link":null,"user_image_file":{"guid":"00000000000000000000000000000001","file_name":"avatar.png","url":"https://www.protocols.io/img/avatars/001.png","mime":"image/png","size":0,"width":100,"height":100,"avg_color":"","scan_status":0,"created_at":0},"note":"","is_verified_user":false}],"before_start":"","book_chapter":null,"can_accept_authorship":false,"can_be_copied":true,"can_claim_authorship":false,"can_manage_keywords":true,"can_remove_fork":false,"can_sign":false,"child_steps":{},"cited_protocols":[],"collection_items":[],"created_on":1725056313,"creator":{"name":"Alexandra Nelson","affiliation":"UCSF","affiliation_url":null,"username":"n4qle1w1s115wle1","link":null,"user_image_file":{"guid":"C37265DF5B2B11EFA2EF0A58A9FEAC02","file_name":"AItbvmkZsbo6pKPuo0WHe6GYowH5G6IQkVnSyhZ3Vc11=s96-c.png","url":"https://files.protocols.io/external/AItbvmkZsbo6pKPuo0WHe6GYowH5G6IQkVnSyhZ3Vc11s96c-C37265DF5B2B11EFA2EF0A58A9FEAC02.png","mime":"image/png","size":1335,"width":0,"height":0,"avg_color":"","scan_status":0,"created_at":1723742899},"badges":[{"id":3,"name":"Power author!","image":{"source":"/img/badges/silver.svg","placeholder":"/img/badges/silver.svg"}}],"affiliations":[{"affiliation":"UCSF","url":null,"job_title":null,"is_default":true}]},"cross_cloud_origin":null,"description":"{\"blocks\":[{\"key\":\"bc2gr\",\"text\":\"This is a protocol used to assess mouse reversal learning in an operant box. Food-deprived mice are initially trained in the operant box (milk as the reward) using the related \\\"Basic Operant Behavioral Training\\\" protocol in the same folder on protocols.io. See that protocol for materials and operant box design. Mice are then trained and tested on the protocol included here to assess reversal learning with probabilistic reward delivery. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{}}","disclaimer":"","document":"","documents":null,"doi":"dx.doi.org/10.17504/protocols.io.261ge51eog47/v1","doi_status":2,"ethics_statement":"","fork_id":null,"fork_info":null,"fork_info_status":"not_fork","forks":[],"funders":[{"funder_name":"Aligning Science Across Parkinson's","grant_id":"ASAP-020529"}],"groups":[{"id":8831,"uri":"asap-network","title":"ASAP Collaborative Research Network","is_public":false,"image":{"source":"https://s3.amazonaws.com/protocols-files/public/6f37aae407d1c3261f56707cf7d8e3a47c4d284af7e8d2ae19cb40776fda17a7/cj6mcz36.jpg","placeholder":"https://s3.amazonaws.com/protocols-files/public/6f37aae407d1c3261f56707cf7d8e3a47c4d284af7e8d2ae19cb40776fda17a7/cj6mcz36.jpg"},"tech_support":{"email":null,"phone":null,"use_email":false,"hide_contact":false,"url":null}},{"id":62724,"uri":"nelson-lab3","title":"Nelson Lab","is_public":false,"image":{"source":"https://s3.amazonaws.com/protocols-files/files/0AA4207AFEFD11ECB2920A58A9FEAC02-placeholder.png","placeholder":"https://s3.amazonaws.com/protocols-files/files/0AA4207AFEFD11ECB2920A58A9FEAC02-placeholder.png"},"tech_support":{"email":null,"phone":null,"use_email":false,"hide_contact":false,"url":null}}],"guid":"FEADC86C60DA4D8EB4AA2A4EC545CBF2","guidelines":"","has_references":false,"has_step_reagents":false,"has_versions":false,"id":106778,"image":{"source":"https://www.protocols.io/img/default_protocol.png","webp_source":null,"placeholder":"https://www.protocols.io/img/default_protocol.png","webp_placeholder":null},"image_attribution":"","in_trash":false,"is_bookmarked":false,"is_contact_suspended":false,"is_content_confidential":false,"is_content_warning":false,"is_doi_reserved":false,"is_in_pending_publishing":false,"is_in_transfer":false,"is_owner":true,"is_research":true,"is_retracted":false,"is_shared_directly":false,"is_subprotocol":null,"is_unlisted":false,"item_id":1292138,"journal":null,"journals":[],"keywords":"ASAPCRN","last_modified":1725057706,"link":"","location":null,"manuscript_citation":"","materials":[],"materials_text":"{\"blocks\":[{\"key\":\"6s6qk\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{}}","ownership_history":null,"parent_collections":[],"parent_protocols":[],"peer_reviewed":false,"protocol_references":"","public":true,"public_fork_note":"","published_on":1725057706,"references":[],"related_equipments":[],"related_materials":[],"reserved_doi":"","retraction_reason":null,"samples":{},"shared_access_id":265,"show_comparison":false,"sign_info":null,"space_access":{"can_view":false,"can_edit":false,"can_remove":false,"can_add":false,"can_publish":true,"can_get_doi":true,"can_share":false,"can_move":false,"can_move_outside":false,"can_transfer":false,"can_download":false,"limited_run":false,"limited_private_links":false,"limited_blind_links":false,"is_locked":false},"space_id":8782,"state_version_id":223,"stats":{"is_voted":false,"number_of_views":3,"number_of_steps":5,"number_of_bookmarks":0,"number_of_comments":0,"number_of_bookmarked_comments":0,"number_of_steps_comments":0,"number_of_protocol_comments":0,"number_of_exports":0,"number_of_runs":0,"number_of_votes":0,"number_of_reagents":0,"number_of_equipments":0,"number_of_collections":0,"number_of_forks":{"private":0,"public":0},"number_of_accessible_forks":0},"status":{"id":1,"info":"We use this protocol and it's working"},"steps":[{"id":2212045,"guid":"2D1878D3ED534BD2944C632AE0D2B00D","previous_id":0,"previous_guid":null,"section":"\u003cp\u003eOverview/Setup\u003c/p\u003e","section_color":"#A492FF","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"m172\",\"text\":\"Setup: This protocol can be used after following the Basic Operant Training protocol listed separately in the same folder on protocols.io. This related protocol includes information about operant box design and implementation, as well as training/shaping that is used prior to using the Probabilistic Reversal Learning Task detailed here. \",\"type\":\"align-justify\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{}}","data":null,"protocol_id":106778,"case_id":0,"critical_ids":"","duration":0,"original_id":0,"number":"1","cases":[],"critical":null},{"id":2212047,"guid":"D7F5CE694BE9484E8332B928B4D33803","previous_id":2212045,"previous_guid":"2D1878D3ED534BD2944C632AE0D2B00D","section":"\u003cp\u003eOverview/Setup\u003c/p\u003e","section_color":"#A492FF","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"96rq0\",\"text\":\"Probabilistic Reversal Learning contains two phases. In the initial phase, the two choice ports are associated with 80 (high) and 0% (low) probability of reward, and contingencies are switched every 7-23 rewarded trials (Phase5.1). In the second phase, the two choice ports are associated with 80 (high) and 20% (low) probability of reward, with a switch in contingencies occurring when a mouse chooses the high-probability reward port in \\u003e80% of the last 15 trials (Phase5.2). There is a maximum of 250 trials per session. In a given mouse, the left and right sides are randomly assigned to either the “high” or “low” probability outcomes.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7f3jn\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"6tk6t\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":0,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"msnu\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4vpac\",\"text\":\"\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{\"0\":{\"type\":\"image\",\"mutability\":\"IMMUTABLE\",\"data\":{\"alt\":\"\",\"guid\":\"FAC1C8D4671E11EF8FCD0A58A9FEAC02\",\"height\":309,\"id\":496128,\"is_video\":false,\"legend\":\"{\\\"blocks\\\":[{\\\"key\\\":\\\"e84tg\\\",\\\"text\\\":\\\"Figure 1: Phase5.2/Probabilistic Reversal Learning. Schematic of evaluation\\\\nphase5.2 with left port being an 80% chance to gain of reward and the right\\\\nport assigned to a 20% change to gain a reward. At the start of each trial, the\\\\ncenter port light is illuminated, and the mouse must poke in the central port\\\\nto initiate the trial. Then both side cue lights illuminate. Depending on the\\\\nchoice of the side port, mice receive a reward based on the probability chance.\\\\nThe switch between ports happens after \\\\u003e80% of rewarded trials on the “high”\\\\nprobability port during the last 15 trials. Each session contains 250 trials. \\\",\\\"type\\\":\\\"unstyled\\\",\\\"depth\\\":0,\\\"inlineStyleRanges\\\":[{\\\"style\\\":\\\"bold\\\",\\\"offset\\\":0,\\\"length\\\":52}],\\\"entityRanges\\\":[],\\\"data\\\":{}}],\\\"entityMap\\\":{}}\",\"mime\":\"image/png\",\"nestedSelector\":null,\"original_name\":\"Slide1.png\",\"placeholder\":\"https://content.protocols.io/files/rerubvk8p.png\",\"shadow\":false,\"source\":\"https://content.protocols.io/files/resabvk8p.jpg\",\"webp_source\":\"https://content.protocols.io/files/rer9bvk8p.webp\",\"width\":550}}}}","data":null,"protocol_id":106778,"case_id":0,"critical_ids":"","duration":0,"original_id":0,"number":"2","cases":[],"critical":null},{"id":2212048,"guid":"688A41F54FBA4C04AA3734E9304FE652","previous_id":2212047,"previous_guid":"D7F5CE694BE9484E8332B928B4D33803","section":"\u003cp\u003eTask Design\u003c/p\u003e","section_color":"#94EBFF","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"f2o98\",\"text\":\"Mice are trained with one session per day consisting of 250 trials. “High” and “low” probability ports are randomly assigned to the left and right port at the beginning of the session. All trials start with a 10-second illumination of the central port light. A central nosepoke within 10 seconds extinguishes the central nosepoke light and initiates the next phase of the trial. Trials in which the mouse fails to nosepoke during this window are recorded as center omissions. After the center nosepoke, cue lights on both side ports are illuminated for 10 seconds, and mice can nosepoke in either side port. Trials in which mice fail to nosepoke at a side port during this 10-second window are recorded as side omissions. A side nosepoke at the “high” probability port results in reward delivery (10uL) in the central port 80% of the time in both phase 5.1 and phase 5.2. A side nosepoke at the “low” probability port results in reward delivery in the central port 0% of the time in phase 5.1 or 20% of the time in phase 5.2. A side nosepoke at the “low” probability port results in reward delivery in the central port 0% of the time in phase 5.1 or 20% of the time in phase 5.2. Once a side port is selected, reward is delivered at the central port based on these probabilities and the central port light stays illuminated for 10 seconds. Once the central port light turns off, the intertrial interval (ITI) begins (a randomly selected period of time between 20-30 seconds). In Phase 5.1, a switch in the pairing of left and right ports with “high” (80%) and “low” (0%) probability of reward occurs periodically. Contingency reversals  occur at a random frequency (every 7-23 rewarded trials) throughout the session.\\nMice continue these sessions for a minimum of 6 days and maximum of 10 days; mice reaching a total of \\u003e80 rewarded trials over 3 cumulative days are classified as ‘learners’ and go to Phase5.2. If a mouse does not reach the \\u003e80 rewarded trials criterion after 10 days of training, it is labelled a ‘non-learner’, but still goes on to Phase5.2. In Phase5.2, trials have a similar structure, but the contingency switch only occurs when mice choose the high reward-probability port on \\u003e80% of the last 15 trials. All mice undergo 3 Phase5.2 sessions. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{}}","data":null,"protocol_id":106778,"case_id":0,"critical_ids":"","duration":0,"original_id":0,"number":"3","cases":[],"critical":null},{"id":2212057,"guid":"A117564384064C5899A54F369E424DAD","previous_id":2212048,"previous_guid":"688A41F54FBA4C04AA3734E9304FE652","section":"\u003cp\u003eData Collection and Analysis\u003c/p\u003e","section_color":"#84CE84","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"eksf5\",\"text\":\"During experimental sessions, the following information and events (timestamps) are\\nrecorded for subsequent analysis:\",\"type\":\"align-justify\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"bposr\",\"text\":\"-Initial random assignment of either the “high” or “low” probability-associated port.\",\"type\":\"align-justify\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4g4ud\",\"text\":\"-Trial start.\",\"type\":\"align-justify\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8qdjc\",\"text\":\"-First center nosepoke (beam break).  The interval between trial start and center nosepoke is the reaction time to initiate a trial. \",\"type\":\"align-justify\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"707m2\",\"text\":\"-Side port (left or right) nosepoke (beam break). The interval between center nosepoke and side port nosepoke is the reaction time to choose a side. \",\"type\":\"align-justify\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"85ioj\",\"text\":\"-Second center port nosepoke (beam break), for reward retrieval. \",\"type\":\"align-justify\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"c90sn\",\"text\":\"-End of trial, time-out. End of trial may occur at the time of reward retrieval, or when an animal does not perform the next nosepoke within the prescribed period (omission).\",\"type\":\"align-justify\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{}}","data":null,"protocol_id":106778,"case_id":0,"critical_ids":"","duration":0,"original_id":0,"number":"4","cases":[],"critical":null},{"id":2212085,"guid":"C1D3D678FB104E54B96E0E6E790C026D","previous_id":2212057,"previous_guid":"A117564384064C5899A54F369E424DAD","section":"\u003cp\u003eData Collection and Analysis\u003c/p\u003e","section_color":"#84CE84","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"21gdm\",\"text\":\"Using the above information and timestamps, software/code can extract for each session: \",\"type\":\"align-justify\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"5j391\",\"text\":\"-Choice of port (high or low-probability of reward).\",\"type\":\"align-justify\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7a2gp\",\"text\":\"-Reaction time to initiate a trial, which may be used as a readout of attention and/or motivation.\",\"type\":\"align-justify\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"313a8\",\"text\":\"-Reaction time to choose a side, which may reflect motivation regarding specific choices within blocks. \",\"type\":\"align-justify\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"a0igi\",\"text\":\"-Number of contingency switches within a session.\",\"type\":\"align-justify\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"c73bm\",\"text\":\"-Center and Side omissions. Omissions can reflect motivation. \",\"type\":\"align-justify\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{}}","data":null,"protocol_id":106778,"case_id":0,"critical_ids":"","duration":0,"original_id":0,"number":"5","cases":[],"critical":null}],"template_id":0,"title":"Probabilistic Reversal Learning Task","title_html":"Probabilistic Reversal Learning Task","type_id":1,"units":[{"id":1,"type_id":3,"name":"µL","can_manage":0,"read_only":0},{"id":2,"type_id":3,"name":"mL","can_manage":0,"read_only":0},{"id":3,"type_id":3,"name":"L","can_manage":0,"read_only":0},{"id":4,"type_id":3,"name":"µg","can_manage":0,"read_only":0},{"id":5,"type_id":3,"name":"mg","can_manage":0,"read_only":0},{"id":6,"type_id":3,"name":"g","can_manage":0,"read_only":0},{"id":7,"type_id":3,"name":"kg","can_manage":0,"read_only":0},{"id":8,"type_id":3,"name":"ng","can_manage":0,"read_only":0},{"id":9,"type_id":3,"name":"Hz","can_manage":0,"read_only":0},{"id":10,"type_id":24,"name":"°C","can_manage":0,"read_only":0},{"id":11,"type_id":24,"name":"°К","can_manage":0,"read_only":0},{"id":12,"type_id":24,"name":"°F","can_manage":0,"read_only":0},{"id":13,"type_id":25,"name":"Mass Percent","can_manage":0,"read_only":0},{"id":14,"type_id":25,"name":"% volume","can_manage":0,"read_only":0},{"id":15,"type_id":25,"name":"Mass / % volume","can_manage":0,"read_only":0},{"id":16,"type_id":25,"name":"Parts per Million (PPM)","can_manage":0,"read_only":0},{"id":17,"type_id":25,"name":"Parts per Billion (PPB)","can_manage":0,"read_only":0},{"id":18,"type_id":25,"name":"Parts per Trillion (PPT)","can_manage":0,"read_only":0},{"id":19,"type_id":25,"name":"Mole Fraction","can_manage":0,"read_only":0},{"id":20,"type_id":25,"name":"Mole Percent","can_manage":0,"read_only":0},{"id":21,"type_id":25,"name":"Molarity (M)","can_manage":0,"read_only":1},{"id":22,"type_id":25,"name":"Molarity (M)","can_manage":0,"read_only":0},{"id":23,"type_id":25,"name":"Genome copies per ml","can_manage":0,"read_only":0},{"id":24,"type_id":3,"name":"μV","can_manage":0,"read_only":0},{"id":25,"type_id":3,"name":"ms","can_manage":0,"read_only":0},{"id":26,"type_id":3,"name":"pg","can_manage":0,"read_only":0},{"id":27,"type_id":25,"name":"Molarity dilutions","can_manage":0,"read_only":0},{"id":28,"type_id":25,"name":"millimolar (mM)","can_manage":0,"read_only":0},{"id":29,"type_id":25,"name":"micromolar (µM)","can_manage":0,"read_only":0},{"id":30,"type_id":25,"name":"nanomolar (nM)","can_manage":0,"read_only":0},{"id":31,"type_id":25,"name":"picomolar (pM)","can_manage":0,"read_only":0},{"id":32,"type_id":24,"name":"Room temperature","can_manage":0,"read_only":0},{"id":33,"type_id":30,"name":"rpm","can_manage":0,"read_only":0},{"id":34,"type_id":30,"name":"x g","can_manage":0,"read_only":0},{"id":165,"type_id":24,"name":"On ice","can_manage":0,"read_only":0},{"id":200,"type_id":32,"name":"cm","can_manage":0,"read_only":0},{"id":201,"type_id":32,"name":"mm","can_manage":0,"read_only":0},{"id":202,"type_id":32,"name":"µm","can_manage":0,"read_only":0},{"id":203,"type_id":32,"name":"nm","can_manage":0,"read_only":0},{"id":204,"type_id":25,"name":"mg/mL","can_manage":0,"read_only":0},{"id":205,"type_id":25,"name":"µg/µL","can_manage":0,"read_only":0},{"id":206,"type_id":25,"name":"% (v/v)","can_manage":0,"read_only":0},{"id":207,"type_id":3,"name":"V","can_manage":0,"read_only":0},{"id":1324,"type_id":30,"name":"rcf","can_manage":0,"read_only":0},{"id":1359,"type_id":35,"name":"Bar","can_manage":0,"read_only":0},{"id":1360,"type_id":35,"name":"Pa","can_manage":0,"read_only":0}],"uri":"probabilistic-reversal-learning-task-dkh24t8e","url":"https://www.protocols.io/view/probabilistic-reversal-learning-task-dkh24t8e","version_class":106778,"version_data":{"id":0,"code":"dkh24t8e","version_class":106778,"parent_id":null,"parent_uri":null,"is_same_owner":false,"is_parent_public":false,"has_pending_merge_request":false,"has_approved_merge_request":false,"merge_request":null},"version_id":0,"version_uri":"probabilistic-reversal-learning-task-261ge51eog47/v1","versions":[{"id":106778,"title":"Probabilistic Reversal Learning Task","title_html":"Probabilistic Reversal Learning Task","image":{"source":"https://www.protocols.io/img/default_protocol.png","webp_source":null,"placeholder":"https://www.protocols.io/img/default_protocol.png","webp_placeholder":null},"doi":"dx.doi.org/10.17504/protocols.io.261ge51eog47/v1","uri":"probabilistic-reversal-learning-task-dkh24t8e","published_on":1725057706,"modified_on":1725057706,"version_class":106778,"version_id":0,"version_code":"dkh24t8e","version_uri":"probabilistic-reversal-learning-task-261ge51eog47/v1","created_on":1725056313,"categories":null,"type_id":1,"creator":{"name":"Alexandra Nelson","affiliation":"UCSF","affiliation_url":null,"username":"n4qle1w1s115wle1","link":null,"user_image_file":{"guid":"C37265DF5B2B11EFA2EF0A58A9FEAC02","file_name":"AItbvmkZsbo6pKPuo0WHe6GYowH5G6IQkVnSyhZ3Vc11=s96-c.png","url":"https://files.protocols.io/external/AItbvmkZsbo6pKPuo0WHe6GYowH5G6IQkVnSyhZ3Vc11s96c-C37265DF5B2B11EFA2EF0A58A9FEAC02.png","mime":"image/png","size":1335,"width":0,"height":0,"avg_color":"","scan_status":0,"created_at":1723742899}},"stats":{"number_of_comments":0,"last_comment_time":0}}],"warning":""}