{"access":{"can_view":true,"can_edit":false,"can_remove":false,"can_add":false,"can_publish":false,"can_get_doi":false,"can_share":true,"can_move":true,"can_move_outside":true,"can_transfer":true,"can_download":true,"limited_run":false,"limited_private_links":false,"limited_blind_links":false,"is_locked":false},"acknowledgements":null,"authors":[{"name":"Nauras Daraghmeh","affiliation":"Department of Marine Sciences, University of Gothenburg, Sweden","affiliation_url":"","username":"nauras-daraghmeh","link":"https://www.researchgate.net/profile/Nauras-Daraghmeh","user_image_file":{"guid":"36D872605B2B11EFA2EF0A58A9FEAC02","file_name":"prs2bimqf.jpg","url":"https://files.protocols.io/external/prs2bimqf-36D872605B2B11EFA2EF0A58A9FEAC02.jpg","mime":"image/jpeg","size":300289,"width":0,"height":0,"avg_color":"","scan_status":0,"created_at":1723742663},"note":"","is_verified_user":true}],"before_start":"","book_chapter":null,"can_accept_authorship":false,"can_be_copied":true,"can_claim_authorship":false,"can_manage_keywords":true,"can_remove_fork":false,"can_sign":false,"child_steps":{},"cited_protocols":[],"collection_items":[],"created_on":1692204573,"creator":{"name":"Nauras Daraghmeh","affiliation":"Department of Marine Sciences, University of Gothenburg, Sweden","affiliation_url":"","username":"nauras-daraghmeh","link":"https://www.researchgate.net/profile/Nauras-Daraghmeh","user_image_file":{"guid":"36D872605B2B11EFA2EF0A58A9FEAC02","file_name":"prs2bimqf.jpg","url":"https://files.protocols.io/external/prs2bimqf-36D872605B2B11EFA2EF0A58A9FEAC02.jpg","mime":"image/jpeg","size":300289,"width":0,"height":0,"avg_color":"","scan_status":0,"created_at":1723742663},"badges":[{"id":2,"name":"Author","image":{"source":"/img/badges/bronze.svg","placeholder":"/img/badges/bronze.svg"}},{"id":6,"name":"Science accelerator","image":{"source":"/img/badges/accelerator.svg","placeholder":"/img/badges/accelerator.svg"}}],"affiliations":[{"affiliation":"Department of Marine Sciences, University of Gothenburg, Sweden","url":"","job_title":null,"is_default":true}]},"cross_cloud_origin":null,"description":"{\"blocks\":[{\"key\":\"70dog\",\"text\":\"This workflow details how COI and 18S rRNA gene raw amplicon sequencing data from the European ARMS programme (ARMS-MBON) can be processed bioinformatically to generate read count and taxonomy tables of molecular operational taxonomic units (mOTUs) for the identification of (marine) non-indigenuous species (NIS). However, the end products may also be used for any other diversity analyses that suit the user. The pipeline may also be adjusted to work with amplicon sequence variants (ASVs) instead of mOTU by omitting and ajdusting certain steps.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4ljof\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"6vmd9\",\"text\":\"The data used here comprise all publicly available COI and 18S sequencing data from ARMS-MBON as of February 2024. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"erl2l\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"cm24a\",\"text\":\"Processes described in this pipeline were executed on Unix and Windows OS. Certain steps (especially software installations etc.) may differ when run on different operating systems. Some computationally intensive steps were run on a high-performance computing cluster. This is noted in the respective section of this workflow.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"e9fj8\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"9kq2e\",\"text\":\"Note that in this workflow, separate directories were created for each marker gene. Make sure that the input files required (i.e., the files produced in the correspoding preceding step) are in the respective directory.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"eo3kd\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"1nmno\",\"text\":\"References to all data, software, packages and databases used in this workflow (please cite any of the tools used in your analysis):\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":132}],\"entityRanges\":[],\"data\":{}},{\"key\":\"doloc\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"a2ciu\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"1spis\",\"text\":\"ARMS-MBON (Obst et al., 2020)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":9},{\"style\":\"italic\",\"offset\":16,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"46q2n\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"24je7\",\"text\":\"R v4.1.0 and v4.3.1 (R Core Team, 2021, 2023) (v4.3.1 was used for dada2 processing and COI numt-removal) \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":19},{\"style\":\"italic\",\"offset\":66,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"7jtgm\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"5gonq\",\"text\":\"RStudio 2022.07.1 (RStudio Team, 2022)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":17}],\"entityRanges\":[],\"data\":{}},{\"key\":\"cd9c3\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"9o4tq\",\"text\":\"cutadapt v4.5 (Martin, 2011) \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":13}],\"entityRanges\":[],\"data\":{}},{\"key\":\"8ljq7\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"c1891\",\"text\":\"Git v2.37.3 (Chacon \\u0026 Straub, 2014)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":11}],\"entityRanges\":[],\"data\":{}},{\"key\":\"2gdso\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4tbdf\",\"text\":\"Python v3.11.4 (Van Rossum \\u0026 Drake, 2009)\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":14}],\"entityRanges\":[],\"data\":{}},{\"key\":\"3fm83\",\"text\":\"MACSE v2.05 (Ranwez et al., 2018)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":11},{\"style\":\"italic\",\"offset\":20,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"54kd\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3cr9d\",\"text\":\"swarm v3.0.0 (Mahé et al., 2015)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":12}],\"entityRanges\":[],\"data\":{}},{\"key\":\"d9smm\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8mn3n\",\"text\":\"NCBI BLAST (Johnson et al., 2008)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":10},{\"style\":\"italic\",\"offset\":20,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"furp2\",\"text\":\"\\nBLAST+ release 2.11.0 (Camacho et al., 2009)\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":1,\"length\":21},{\"style\":\"italic\",\"offset\":32,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"4g8pq\",\"text\":\"BOLD (Ratnasingham \\u0026 Hebert, 2007)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":4},{\"style\":\"italic\",\"offset\":4,\"length\":2}],\"entityRanges\":[],\"data\":{}},{\"key\":\"eavsh\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"af55i\",\"text\":\"BOLDigger-commandline v2.2.1 (Buchner \\u0026 Leese, 2020)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":21}],\"entityRanges\":[],\"data\":{}},{\"key\":\"9dsqs\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"14rat\",\"text\":\"SeqKit v2.5.1 (Shen et al., 2016)\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":13},{\"style\":\"italic\",\"offset\":20,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"86r4t\",\"text\":\"MIDORI2 (Leray et al., 2022)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":7},{\"style\":\"italic\",\"offset\":15,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"fkj1n\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3uikr\",\"text\":\"MIDORI2 webserver (Leray et al., 2018)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":7},{\"style\":\"bold\",\"offset\":8,\"length\":9},{\"style\":\"italic\",\"offset\":25,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"8rakh\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"5dpmp\",\"text\":\"GenBank release 257 (Benson et al., 2012)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":19},{\"style\":\"italic\",\"offset\":27,\"length\":7}],\"entityRanges\":[],\"data\":{}},{\"key\":\"330lg\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"60smh\",\"text\":\"RDP classifier (Wang et al., 2007)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":14},{\"style\":\"italic\",\"offset\":21,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"90s4\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"aus63\",\"text\":\"Silva taxonomic training data formatted for DADA2 (Callahan, 2018) (Silva v132; Quast et al., 2013)\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":50},{\"style\":\"bold\",\"offset\":68,\"length\":10},{\"style\":\"bold\",\"offset\":79,\"length\":1},{\"style\":\"italic\",\"offset\":86,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"4khet\",\"text\":\"SILVA v128 and v132 dada2 formatted 18s 'train sets' (Morien \\u0026 Parfrey, 2018) (Silva v128 and v132; Quast et al., 2013)\\n\\nProtist Ribosomal Reference database (PR2) v5.0.0 (Guillou et al., 2013)\\n\\nWorld Register of Marine Species (WoRMS) (Ahyong et al., 2023)\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":53},{\"style\":\"bold\",\"offset\":79,\"length\":19},{\"style\":\"bold\",\"offset\":99,\"length\":1},{\"style\":\"bold\",\"offset\":121,\"length\":49},{\"style\":\"bold\",\"offset\":195,\"length\":40},{\"style\":\"italic\",\"offset\":106,\"length\":6},{\"style\":\"italic\",\"offset\":180,\"length\":6},{\"style\":\"italic\",\"offset\":244,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"9sj8f\",\"text\":\"World Register of Introduced Marine Species (WRiMS) (Rius et al., 2023)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":51},{\"style\":\"italic\",\"offset\":58,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"dgspp\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"djctj\",\"text\":\"Microsoft Excel 2016 (Microsoft Corporation, 2016) \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":21},{\"style\":\"bold\",\"offset\":50,\"length\":1},{\"style\":\"italic\",\"offset\":15,\"length\":6},{\"style\":\"italic\",\"offset\":50,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"249uq\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7b3jf\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7nhhg\",\"text\":\"R packages:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":11}],\"entityRanges\":[],\"data\":{}},{\"key\":\"6hjds\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3dicj\",\"text\":\"argparse v2.2.2 (Davis, 2023)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":16}],\"entityRanges\":[],\"data\":{}},{\"key\":\"d6m44\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"6scqd\",\"text\":\"dada2 v1.28.0 (Callahan et al., 2016)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":13},{\"style\":\"italic\",\"offset\":24,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"56tph\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"bru7m\",\"text\":\"ShortRead v1.58.0 (Morgan et al., 2009)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":17},{\"style\":\"italic\",\"offset\":26,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"4mab2\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7jo9h\",\"text\":\"Biostrings v2.68.1 (Pagés et al., 2020)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":18},{\"style\":\"italic\",\"offset\":25,\"length\":7}],\"entityRanges\":[],\"data\":{}},{\"key\":\"bq85r\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"1bei0\",\"text\":\"ggplot2 v3.4.2 and v3.4.3 (Wickham, 2016) (v3.4.3 was used in the dada2 workflow to plot read quality profiles)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":25},{\"style\":\"italic\",\"offset\":66,\"length\":5}],\"entityRanges\":[],\"data\":{}},{\"key\":\"fh8p1\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"12d2\",\"text\":\"ensembleTax v1.2.2 (Catlett et al., 2023)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":18},{\"style\":\"italic\",\"offset\":28,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"2osf\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"b3v0\",\"text\":\"tidyr v1.3.0 (Wickham et al., 2023)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":12},{\"style\":\"italic\",\"offset\":22,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"5smp3\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3rgg2\",\"text\":\"dplyr v1.0.9 and v1.1.3 (Wickham et al., 2022, 2023) (v1.1.3 was used during COI numt-removal)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":23},{\"style\":\"italic\",\"offset\":33,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"39i6o\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"2u0nm\",\"text\":\"stringr v1.5.0 (Wickham, 2022)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":14}],\"entityRanges\":[],\"data\":{}},{\"key\":\"5q5s6\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"c81vl\",\"text\":\"devtools v2.4.3 (Wickham et al., 2021)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":15},{\"style\":\"italic\",\"offset\":25,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"f84jl\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3i0fe\",\"text\":\"hiReadsProcessor v1.29.1 and v1.36.0 (Malani, 2021) (v1.36.0 was used during COI numt-removal)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":36}],\"entityRanges\":[],\"data\":{}},{\"key\":\"bf000\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"es2sj\",\"text\":\"seqinr v4.2.30 (Charif \\u0026 Lobry, 2007)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":14}],\"entityRanges\":[],\"data\":{}},{\"key\":\"bqbff\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"bl1jt\",\"text\":\"remotes v2.4.2 (Csárdi et al., 2021)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":14},{\"style\":\"italic\",\"offset\":23,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"oobq\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3mjie\",\"text\":\"LULU v0.1.0 (Frøslev et al., 2017)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":11},{\"style\":\"italic\",\"offset\":21,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"84n11\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"29vck\",\"text\":\"readxl v1.4.0 (Wickham \\u0026 Bryan, 2022)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":13}],\"entityRanges\":[],\"data\":{}},{\"key\":\"3noc3\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"fvgpf\",\"text\":\"phyloseq v1.36.0 (McMurdie \\u0026 Holmes, 2013)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":16}],\"entityRanges\":[],\"data\":{}},{\"key\":\"5q14l\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7teu2\",\"text\":\"vegan v2.6.2 (Oksanen et al., 2023)\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":12},{\"style\":\"italic\",\"offset\":22,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"b0pjk\",\"text\":\"ggpubr v0.4.0 (Kassambara, 2020)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":13}],\"entityRanges\":[],\"data\":{}},{\"key\":\"5jevd\",\"text\":\"\\ndata.table v1.14.2 (Dowle \\u0026 Srinivasan, 2021)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":1,\"length\":18}],\"entityRanges\":[],\"data\":{}},{\"key\":\"cjip1\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"6891c\",\"text\":\"xlsx v0.6.5 (Dragulescu \\u0026 Arendt, 2020)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":11}],\"entityRanges\":[],\"data\":{}},{\"key\":\"e6ufl\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"55f13\",\"text\":\"plyr v1.8.7 (Wickham, 2011)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":11}],\"entityRanges\":[],\"data\":{}},{\"key\":\"2ouvc\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8k27s\",\"text\":\"geosphere v1.5.18 (Hijmans 2022)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":17}],\"entityRanges\":[],\"data\":{}},{\"key\":\"9mhn4\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"2j39j\",\"text\":\"\\nBibliography:\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":1,\"length\":13}],\"entityRanges\":[],\"data\":{}},{\"key\":\"de17j\",\"text\":\"Ahyong, S., Boyko, C. B., Bailly, N., Bernot, J., Bieler, R., Brandão, S. N., Daly, M., De Grave, S., Gofas, S., Hernandez, F., Hughes, L., Neubauer, T. A., Paulay, G., Boydens, B., Decock, W., Dekeyzer, S., Vandepitte, L., Vanhoorne, B., Adlard, R., … Zullini, A. (2023). World Register of Marine Species (WoRMS). WoRMS Editorial Board. https://www.marinespecies.org\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":273,\"length\":40}],\"entityRanges\":[],\"data\":{}},{\"key\":\"4bnch\",\"text\":\"\\nBenson, D. A., Cavanaugh, M., Clark, K., Karsch-Mizrachi, I., Lipman, D. J., Ostell, J., \\u0026 Sayers, E. W. (2012). GenBank. Nucleic Acids Research, 41(D1), D36–D42. https://doi.org/10.1093/nar/gks1195\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":123,\"length\":22},{\"style\":\"italic\",\"offset\":147,\"length\":2}],\"entityRanges\":[],\"data\":{}},{\"key\":\"8d87p\",\"text\":\"\\nBuchner, D., \\u0026 Leese, F. (2020). BOLDigger – a Python package to identify and organise sequences with the Barcode of Life Data systems. Metabarcoding and Metagenomics 4: E53535, 4, e53535-. https://doi.org/10.3897/MBMG.4.53535\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":137,\"length\":40},{\"style\":\"italic\",\"offset\":179,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"4vdm5\",\"text\":\"\\nCallahan, B. (2018). Silva taxonomic training data formatted for DADA2 (Silva version 132). Zenodo. https://doi.org/10.5281/zenodo.1172783\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":22,\"length\":69}],\"entityRanges\":[],\"data\":{}},{\"key\":\"c80kv\",\"text\":\"\\nCallahan, B. J., McMurdie, P. J., Rosen, M. J., Han, A. W., Johnson, A. J. A., \\u0026 Holmes, S. P. (2016). DADA2: High-resolution sample inference from Illumina amplicon data. Nature Methods 2016 13:7, 13(7), 581–583. https://doi.org/10.1038/nmeth.3869\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":173,\"length\":24},{\"style\":\"italic\",\"offset\":199,\"length\":2}],\"entityRanges\":[],\"data\":{}},{\"key\":\"7m4v\",\"text\":\"\\nCamacho, C., Coulouris, G., Avagyan, V., Ma, N., Papadopoulos, J., Bealer, K., \\u0026 Madden, T. L. (2009). BLAST+: Architecture and applications. BMC Bioinformatics, 10(1), 1–9. https://doi.org/10.1186/1471-2105-10-421/FIGURES/4\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":143,\"length\":18},{\"style\":\"italic\",\"offset\":163,\"length\":2}],\"entityRanges\":[],\"data\":{}},{\"key\":\"aa4f5\",\"text\":\"\\nCatlett, D., Son, K., \\u0026 Liang, C. (2023). ensembleTax: Ensemble Taxonomic Assignments of Amplicon Sequencing Data.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":43,\"length\":71}],\"entityRanges\":[],\"data\":{}},{\"key\":\"8l6o3\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7rpsp\",\"text\":\"Chacon, S., \\u0026 Straub, B. (2014). Pro git. Apress.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":33,\"length\":7}],\"entityRanges\":[],\"data\":{}},{\"key\":\"9fa30\",\"text\":\"\\nCharif, D., \\u0026 Lobry, J. R. (2007). SeqinR 1.0-2: a contributed package to the R project for statistical computing devoted to biological sequences retrieval and analysis. In U. Bastolla, M. Porto, H. E. Roman, \\u0026 M. Vendruscolo (Eds.), Structural\\napproaches to sequence evolution: Molecules, networks, populations (pp. 207–232). Springer Verlag.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":235,\"length\":77}],\"entityRanges\":[],\"data\":{}},{\"key\":\"ku4s\",\"text\":\"\\nCsárdi, G., Hester, J., Wickham, H., Chang, W., Morgan, M., \\u0026 Tenenbaum, D. (2021). remotes: R Package Installation from Remote Repositories, Including “GitHub.” https://cran.r-project.org/package=remotes\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":85,\"length\":77}],\"entityRanges\":[],\"data\":{}},{\"key\":\"6kt0u\",\"text\":\"\\nDavis, T. L. (2023). argparse: Command Line Optional and Positional Argument Parser. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":22,\"length\":62}],\"entityRanges\":[],\"data\":{}},{\"key\":\"f639a\",\"text\":\"https://cran.r-project.org/package=argparse\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"5us50\",\"text\":\"\\nDowle, M., \\u0026 Srinivasan, A. (2021). data.table: Extension of `data.frame`. https://cran.r-project.org/package=data.table\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":37,\"length\":37}],\"entityRanges\":[],\"data\":{}},{\"key\":\"c076s\",\"text\":\"\\nDragulescu, A., \\u0026 Arendt, C. (2020). xlsx: Read, Write, Format Excel 2007 and Excel 97/2000/XP/2003 Files. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":38,\"length\":68}],\"entityRanges\":[],\"data\":{}},{\"key\":\"sf3t\",\"text\":\"https://cran.r-project.org/package=xlsx\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4qo61\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"2or91\",\"text\":\"Frøslev, T. G., Kjøller, R., Bruun, H. H., Ejrnæs, R., Brunbjerg, A. K., Pietroni, C., \\u0026 Hansen, A. J. (2017). Algorithm for post-clustering curation of DNA amplicon data yields reliable biodiversity estimates. Nature Communications 2017 8:1, 8(1),\\n1–11. https://doi.org/10.1038/s41467-017-01312-x\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":211,\"length\":30},{\"style\":\"italic\",\"offset\":243,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"30air\",\"text\":\"\\nGuillou, L., Bachar, D., Audic, S., Bass, D., Berney, C., Bittner, L., Boutte, C., Burgaud, G., De Vargas, C., Decelle, J., Del Campo, J., Dolan, J. R., Dunthorn, M., Edvardsen, B., Holzmann, M., Kooistra, W. H. C. F., Lara, E., Le Bescot, N., Logares, R., …Christen, R. (2013). The Protist Ribosomal Reference database (PR2): a catalog of unicellular eukaryote Small Sub-Unit rRNA sequences with curated taxonomy. Nucleic Acids Research, 41(D1), D597–D604. https://doi.org/10.1093/NAR/GKS1160\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":416,\"length\":22},{\"style\":\"italic\",\"offset\":440,\"length\":2}],\"entityRanges\":[],\"data\":{}},{\"key\":\"3oppc\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"1ok5f\",\"text\":\"Hijmans, R. J. (2022). geosphere: Spherical Trigonometry. https://cran.r-project.org/package=geosphere\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":23,\"length\":33}],\"entityRanges\":[],\"data\":{}},{\"key\":\"du8nm\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"1a94a\",\"text\":\"Johnson, M., Zaretskaya, I., Raytselis, Y., Merezhuk, Y., McGinnis, S., \\u0026 Madden, T. L. (2008). NCBI BLAST: a better web interface. Nucleic Acids Research, 36(Web Server), W5–W9. https://doi.org/10.1093/nar/gkn201\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":132,\"length\":22},{\"style\":\"italic\",\"offset\":156,\"length\":2}],\"entityRanges\":[],\"data\":{}},{\"key\":\"9rfb8\",\"text\":\"\\nKassambara, A. (2020). ggpubr:\\n“ggplot2” Based Publication Ready Plots. R package version 0.4.0. https://cran.r-project.org/package=ggpubr\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":24,\"length\":47}],\"entityRanges\":[],\"data\":{}},{\"key\":\"dmsp0\",\"text\":\"\\nLeray, M., Ho, S. L., Lin, I. J., \\u0026 Machida, R. J. (2018). MIDORI server: a webserver for taxonomic assignment of unknown metazoan mitochondrial-encoded sequences using a curated database. Bioinformatics, 34(21), 3753–3754.\\nhttps://doi.org/10.1093/BIOINFORMATICS/BTY454\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":190,\"length\":14},{\"style\":\"italic\",\"offset\":206,\"length\":2}],\"entityRanges\":[],\"data\":{}},{\"key\":\"cjvvv\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"f03st\",\"text\":\"Leray, M., Knowlton, N., \\u0026 Machida, R. J. (2022). MIDORI2: A collection of quality controlled, preformatted, and regularly updated reference databases for taxonomic assignment of eukaryotic mitochondrial sequences. Environmental DNA, 4(4), 894–907. https://doi.org/10.1002/EDN3.303\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":215,\"length\":17},{\"style\":\"italic\",\"offset\":234,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"f2p83\",\"text\":\"\\nMahé, F., Rognes, T., Quince, C., de Vargas, C., \\u0026 Dunthorn, M. (2015). Swarmv2: Highly-scalable and high-resolution amplicon clustering. PeerJ, 2015(12), e1420. https://doi.org/10.7717/PEERJ.1420/SUPP-1\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":139,\"length\":5},{\"style\":\"italic\",\"offset\":146,\"length\":4}],\"entityRanges\":[],\"data\":{}},{\"key\":\"5l2dk\",\"text\":\"\\nMalani, N. V. (2021). hiReadsProcessor: Functions to process LM-PCR reads from 454/Illumina data.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":23,\"length\":74}],\"entityRanges\":[],\"data\":{}},{\"key\":\"97pav\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"ban5u\",\"text\":\"Martin, M. (2011). Cutadapt removes adapter sequences from high-throughput sequencing reads. EMBnet.Journal, 17(1), 10–12. https://doi.org/10.14806/ej.17.1.200\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":93,\"length\":14},{\"style\":\"italic\",\"offset\":109,\"length\":2}],\"entityRanges\":[],\"data\":{}},{\"key\":\"a94hn\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"flvd1\",\"text\":\"McMurdie, P. J., \\u0026 Holmes, S. (2013). phyloseq: An R Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data. PLOS ONE, 8(4), e61217. https://doi.org/10.1371/JOURNAL.PONE.0061217\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":139,\"length\":8},{\"style\":\"italic\",\"offset\":149,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"9gh7m\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"707l4\",\"text\":\"Microsoft Corporation (2016). Microsoft Excel, Available at: https://office.microsoft.com/excel\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":30,\"length\":15},{\"style\":\"UNDERLINE\",\"offset\":61,\"length\":34}],\"entityRanges\":[{\"key\":0,\"offset\":61,\"length\":34}],\"data\":{}},{\"key\":\"3n34i\",\"text\":\"\\nMorgan, M., Anders, S., Lawrence, M., Aboyoun, P., Pagès, H., \\u0026 Gentleman, R. (2009). ShortRead: a Bioconductor package for input, quality assessment and exploration of high-throughput sequence data. Bioinformatics, 25, 2607–2608.\\nhttps://doi.org/10.1093/bioinformatics/btp450\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":201,\"length\":14},{\"style\":\"italic\",\"offset\":217,\"length\":2}],\"entityRanges\":[],\"data\":{}},{\"key\":\"8k3m5\",\"text\":\"\\nMorien, E., \\u0026 Parfrey, L. W. (2018). SILVA v128 and v132 dada2 formatted 18s “train sets.” Zenodo. https://doi.org/10.5281/zenodo.1447330\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":38,\"length\":54}],\"entityRanges\":[],\"data\":{}},{\"key\":\"42j68\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"97qo2\",\"text\":\"Obst, M., Exter, K., Allcock, A. L., Arvanitidis, C., Axberg, A., Bustamante, M., Cancio, I., Carreira-Flores, D., Chatzinikolaou, E., Chatzigeorgiou, G., Chrismas, N., Clark, M. S., Comtet, T., Dailianis, T., Davies, N., Deneudt, K., de Cerio, O. D., Fortič, A., Gerovasileiou, V., … Pavloudi, C. (2020). A Marine Biodiversity Observation Network for Genetic Monitoring of Hard-Bottom Communities (ARMS-MBON). Frontiers in Marine Science, 7, 1031. https://doi.org/10.3389/FMARS.2020.572680/BIBTEX\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":411,\"length\":27},{\"style\":\"italic\",\"offset\":440,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"4pe4o\",\"text\":\"\\nPagès, H., Aboyoun, P., Gentleman, R., \\u0026 DebRoy, S. (2020). Biostrings: Efficient manipulation of biological strings. https://bioconductor.org/packages/Biostrings\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":61,\"length\":56}],\"entityRanges\":[],\"data\":{}},{\"key\":\"f4p32\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"f1r9v\",\"text\":\"Quast, C., Pruesse, E., Yilmaz, P., Gerken, J., Schweer, T., Yarza, P., Peplies, J., \\u0026 Glöckner, F. O. (2013). The SILVA ribosomal RNA gene database project: improved data processing and web-based tools. Nucleic Acids Research, 41(D1), D590–D596. https://doi.org/10.1093/NAR/GKS1219\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":204,\"length\":22},{\"style\":\"italic\",\"offset\":228,\"length\":2}],\"entityRanges\":[],\"data\":{}},{\"key\":\"4lpqp\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"67q3\",\"text\":\"R Core Team. (2020, 2021). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.r-project.org/\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":27,\"length\":55}],\"entityRanges\":[],\"data\":{}},{\"key\":\"ioac\",\"text\":\"\\nRanwez, V., Douzery, E. J. P., Cambon, C., Chantret, N., \\u0026 Delsuc, F. (2018). MACSE v2: Toolkit for the Alignment of Coding Sequences Accounting for Frameshifts and Stop Codons. Molecular Biology and Evolution, 35(10), 2582–2584.\\nhttps://doi.org/10.1093/MOLBEV/MSY159\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":179,\"length\":31},{\"style\":\"italic\",\"offset\":212,\"length\":2}],\"entityRanges\":[],\"data\":{}},{\"key\":\"251p2\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4dqek\",\"text\":\"Ratnasingham, S., \\u0026 Hebert, P. D. N. (2007). bold: The Barcode of Life Data System (http://www.barcodinglife.org). Molecular Ecology Notes, 7(3), 355–364. https://doi.org/10.1111/J.1471-8286.2007.01678.X\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":115,\"length\":23},{\"style\":\"italic\",\"offset\":140,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"a3gat\",\"text\":\"\\nRius, M., Ahyong, S., Bieler, R., Boudouresque, C., Costello, M. J., Downey, R., Galil, B. S., Gollasch, S., Hutchings, P., Kamburska, L., Katsanevakis, S., Kupriyanova, E., Lejeusne, C., Marchini, A., Occhipinti, A., Pagad, S., Panov, V. E., Poore, G.\\nC. B., Robinson, T. B., … Zhan, A. (2023). World Register of Introduced Marine Species (WRiMS). WoRMS Editorial Board. https://www.marinespecies.org/introduced\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":297,\"length\":51}],\"entityRanges\":[],\"data\":{}},{\"key\":\"39lfi\",\"text\":\"\\nRStudio Team. (2022). RStudio: Integrated Development Environment for R. RStudio, PBC. http://www.rstudio.com/\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":23,\"length\":49}],\"entityRanges\":[],\"data\":{}},{\"key\":\"9pp20\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"d0dbh\",\"text\":\"Shen, W., Le, S., Li, Y., \\u0026 Hu, F. (2016). SeqKit: A Cross-Platform and Ultrafast Toolkit for FASTA/Q File Manipulation. PLOS ONE, 11(10), e0163962. https://doi.org/10.1371/journal.pone.0163962\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":121,\"length\":8},{\"style\":\"italic\",\"offset\":131,\"length\":2}],\"entityRanges\":[],\"data\":{}},{\"key\":\"8dnoe\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"89ji1\",\"text\":\"Van Rossum, G., \\u0026 Drake, F. L. (2009). Python 3 Reference Manual. CreateSpace.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":39,\"length\":25}],\"entityRanges\":[],\"data\":{}},{\"key\":\"7b6jg\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"62fgf\",\"text\":\"Wang, Q., Garrity, G. M., Tiedje, J. M., \\u0026 Cole, J. R. (2007). Naïve Bayesian classifier for rapid assignment of rRNA sequences into the new bacterial taxonomy. Applied and Environmental Microbiology, 73(16), 5261–5267.\\nhttps://doi.org/10.1128/AEM.00062-07/SUPPL_FILE/SUMMARY_BYHIERARCHY.ZIP\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":161,\"length\":38},{\"style\":\"italic\",\"offset\":201,\"length\":2}],\"entityRanges\":[],\"data\":{}},{\"key\":\"7dhh9\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"b0bc8\",\"text\":\"Wickham, H. (2011). The Split-Apply-Combine Strategy for Data Analysis. Journal of Statistical Software, 40(1), 1–29. https://www.jstatsoft.org/v40/i01/\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":72,\"length\":31},{\"style\":\"italic\",\"offset\":105,\"length\":2}],\"entityRanges\":[],\"data\":{}},{\"key\":\"15gce\",\"text\":\"\\nWickham, H. (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":21,\"length\":43}],\"entityRanges\":[],\"data\":{}},{\"key\":\"34hum\",\"text\":\"\\nWickham, H. (2022). stringr: Simple, Consistent Wrappers for Common String Operations.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":21,\"length\":65}],\"entityRanges\":[],\"data\":{}},{\"key\":\"fsrep\",\"text\":\"https://cran.r-project.org/package=stringr\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"d5i80\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7rgp0\",\"text\":\"Wickham, H., \\u0026 Bryan, J. (2022). readxl: Read Excel Files. https://cran.r-project.org/package=readxl\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":33,\"length\":24}],\"entityRanges\":[],\"data\":{}},{\"key\":\"eh2sk\",\"text\":\"\\nWickham, H., François, R., Henry, L., \\u0026 Müller, K. (2022, 2023). dplyr: A Grammar of Data Manipulation. R package version 1.1.2. https://cran.r-project.org/package=dplyr\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":66,\"length\":37}],\"entityRanges\":[],\"data\":{}},{\"key\":\"flr32\",\"text\":\"\\nWickham, H., Hester, J., Chang, W., \\u0026 Bryan, J. (2021). devtools: Tools to Make Developing R Packages Easier. https://cran.r-project.org/package=devtools\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":57,\"length\":52}],\"entityRanges\":[],\"data\":{}},{\"key\":\"7tqen\",\"text\":\"\\nWickham, H., Vaughan, D., \\u0026 Girlich, M. (2023). tidyr: Tidy Messy Data. https://cran.r-project.org/package=tidyr\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":49,\"length\":22}],\"entityRanges\":[],\"data\":{}},{\"key\":\"drspk\",\"text\":\"\\n\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{\"0\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"48179156676911EE836F0A58A9FEAC02\",\"url\":\"https://www.researchgate.net/deref/https%3A%2F%2Foffice.microsoft.com%2Fexcel?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6InF1ZXN0aW9uIiwicGFnZSI6InF1ZXN0aW9uIn19\"}}}}","disclaimer":"","document":"","documents":null,"doi":"dx.doi.org/10.17504/protocols.io.n92ldmmmnl5b/v1","doi_status":2,"ethics_statement":"","fork_id":null,"fork_info":null,"fork_info_status":"not_fork","forks":[],"funders":[{"funder_name":"MARCO BOLO","grant_id":"101082021"}],"groups":[{"id":22013,"uri":"swedna-swedish-edna-lab","title":"SWEDNA Swedish eDNA Lab","is_public":true,"image":{"source":"https://s3.amazonaws.com/protocols-files/files/189A86842A9311EC96350A58A9FEAC02-placeholder.png","placeholder":"https://s3.amazonaws.com/protocols-files/files/189A86842A9311EC96350A58A9FEAC02-placeholder.png"},"tech_support":{"email":null,"phone":null,"use_email":false,"hide_contact":false,"url":null}}],"guid":"E0369C433C5411EE882A0A58A9FEAC02","guidelines":"","has_references":false,"has_step_reagents":false,"has_versions":false,"id":86559,"image":{"source":"https://www.protocols.io/img/default_protocol.png","webp_source":null,"placeholder":"https://www.protocols.io/img/default_protocol.png","webp_placeholder":null},"image_attribution":"","in_trash":false,"is_bookmarked":false,"is_contact_suspended":false,"is_content_confidential":false,"is_content_warning":false,"is_doi_reserved":true,"is_in_pending_publishing":false,"is_in_transfer":false,"is_owner":true,"is_research":true,"is_retracted":false,"is_shared_directly":false,"is_subprotocol":null,"is_unlisted":false,"item_id":1151662,"journal":null,"journals":[],"keywords":null,"last_modified":1730104865,"link":"","location":null,"manuscript_citation":"","materials":[],"materials_text":"","ownership_history":null,"parent_collections":[],"parent_protocols":[],"peer_reviewed":false,"protocol_references":"","public":true,"public_fork_note":"","published_on":1730104865,"references":[],"related_equipments":[],"related_materials":[],"reserved_doi":"10.17504/protocols.io.n92ldmmmnl5b/v1","retraction_reason":null,"samples":{},"shared_access_id":0,"show_comparison":false,"sign_info":null,"space_access":{"can_view":false,"can_edit":false,"can_remove":false,"can_add":false,"can_publish":true,"can_get_doi":true,"can_share":false,"can_move":false,"can_move_outside":false,"can_transfer":false,"can_download":false,"limited_run":false,"limited_private_links":false,"limited_blind_links":false,"is_locked":false},"space_id":21831,"state_version_id":11747,"stats":{"is_voted":false,"number_of_views":42,"number_of_steps":15,"number_of_bookmarks":1,"number_of_comments":0,"number_of_bookmarked_comments":0,"number_of_steps_comments":0,"number_of_protocol_comments":0,"number_of_exports":0,"number_of_runs":0,"number_of_votes":0,"number_of_reagents":0,"number_of_equipments":0,"number_of_collections":0,"number_of_forks":{"private":0,"public":0},"number_of_accessible_forks":0},"status":{"id":1,"info":"We use this protocol and it's working"},"steps":[{"id":1766859,"guid":"E0406CC83C5411EE882A0A58A9FEAC02","previous_id":1766868,"previous_guid":"F8FCA3903C5411EEA943654F37AFF3AC","section":"\u003cp\u003ePrimer trimming and amplicon sequence variant (ASV) inference using \u003cem\u003ecutadapt\u003c/em\u003e and \u003cem\u003edada2\u003c/em\u003e \u003c/p\u003e","section_color":"#94EBFF","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"53q5h\",\"text\":\"The downloaded fastq.gz files contain reads which were demultiplexed with two different strategies after MiSeq sequencing. The combined_OmicsData.csv file provided above holds information on this in the Gene_COI_demultiplexed and Gene_18S_demultiplexed columns. Sequencing reads of some runs were demultiplexed based on the Illumina MiSeq library indices, while others were demultiplexed based on these indices as well as with cutadapt based on the respective PCR primers sequences. In the former case, reads still contain the PCR primer sequences, while in the latter case, reads are already devoid of these sequences. See the table below for information on which demultiplexing strategy was applied on the reads of each sequencing run. Note *: according to the combined_OmicsData.csv file, 18S reads of Run_1 and Run_3 were demultiplexed based on MiSeq indices as well as PCR primer sequences. However, after checking these reads, they still contained primer sequences in certain orientations. So reads of those runs were considered as being demultiplexed based on MiSeq indices only.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":105,\"length\":5},{\"style\":\"italic\",\"offset\":324,\"length\":14},{\"style\":\"italic\",\"offset\":427,\"length\":8},{\"style\":\"italic\",\"offset\":849,\"length\":5},{\"style\":\"italic\",\"offset\":1067,\"length\":5}],\"entityRanges\":[],\"data\":{}},{\"key\":\"e61f9\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8r90v\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":0,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"a0hv8\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7k92s\",\"text\":\"Subsequently, the directories containing the downloaded fastq.gz files of each marker gene were placed in separate directories based on the demultiplexing applied (i.e., fastqs_normal and fastqs_cutadapt).\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4gpti\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"1hd9m\",\"text\":\"We ran a pipeline using cutadapt and dada2 in R on an HPC cluster for primer trimming (or length-filtering of reads previously demultiplexed with cutadapt) and read filtering, denoising, merging, chimera \\u0026 singleton removal and taxonomy assignment (for 18S ASVs, COI ASVs were not classified using dada2). It should also be feasible to run the pipeline on a personal machine with a good amount of RAM, although multithreading for certain steps is not enabled in dada2 for Windows OS and the pipeline will take a while to run. The 18S and COI data sets were processed separately. We mainly followed the publicly available dada2 workflows, with some alterations. See here: \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":24,\"length\":8},{\"style\":\"italic\",\"offset\":37,\"length\":5},{\"style\":\"italic\",\"offset\":46,\"length\":1},{\"style\":\"italic\",\"offset\":146,\"length\":8},{\"style\":\"italic\",\"offset\":298,\"length\":5},{\"style\":\"italic\",\"offset\":462,\"length\":5},{\"style\":\"italic\",\"offset\":621,\"length\":5}],\"entityRanges\":[],\"data\":{}},{\"key\":\"fh45g\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"gdem\",\"text\":\"https://benjjneb.github.io/dada2/tutorial.html\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":1,\"offset\":0,\"length\":46}],\"data\":{}},{\"key\":\"bp2mo\",\"text\":\"https://benjjneb.github.io/dada2/ITS_workflow.html (for a workflow incorporating cutadapt)\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":81,\"length\":8}],\"entityRanges\":[{\"key\":2,\"offset\":0,\"length\":50}],\"data\":{}},{\"key\":\"dgnjs\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"c8kh8\",\"text\":\"Cutadapt needs to be installed on your system prior running the R workflow, see cutadapt documentation https://cutadapt.readthedocs.io/en/stable/index.html. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":0,\"length\":8},{\"style\":\"italic\",\"offset\":64,\"length\":1},{\"style\":\"italic\",\"offset\":80,\"length\":8}],\"entityRanges\":[{\"key\":3,\"offset\":103,\"length\":52}],\"data\":{}},{\"key\":\"4i9js\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"b02es\",\"text\":\"To avoid potentially resulting biases during dada2's error model estimation, sequence reads were processed separately per sequencing run up until (and including) the merging of paired reads. Subsequently, inferred amplicon sequence variants (ASVs) from all sequencing runs were merged prior to chimera removal. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":45,\"length\":5}],\"entityRanges\":[],\"data\":{}},{\"key\":\"alkla\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"f1t5e\",\"text\":\"For reads which still contained primer sequences, cutadapt was first applied with maximum mismatch of 1 and 2 bp (i.e., a maximum error rate of e = 0.05 and e = 0.1) for 18S and COI reads, respectively. A higher mismatch was allowed for COI reads due to the longer primer sequences compared to 18S and the fact that COI is a more variable protein-coding gene. Given the expected length of the COI amplicons (313 bp), primers were only trimmed from the forward and reverse reads when being present at the 5'-end of the reads in their forward orientation. Given the length variability of the 18S amplicons and potential read-through during sequencing, primers were trimmed from the forward and reverse reads when being present in their forward orientation, as well as when being present in their reverse-complement orientation. Untrimmed reads were discarded. For reads which were already demultiplexed with cutadapt and therefore did not contain any primer sequences, cutadapt was only applied to filter out reads with a length of zero bp (can occur during demultiplexing when a read was only made up of its primer sequence and was fully trimmed to zero bp).\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":50,\"length\":8},{\"style\":\"italic\",\"offset\":906,\"length\":8},{\"style\":\"italic\",\"offset\":967,\"length\":9}],\"entityRanges\":[],\"data\":{}},{\"key\":\"8sa9s\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"9il89\",\"text\":\"Quality profiles were generated for a maximum of (randomly chosen) 20 samples of each sequencing  run. For dada2's filterAndTrim function, maxEE was set to 2 for forward and 4 for reverse reads for all sequencing datasets. Based on the quality profiles, COI reads were trimmed with truncLen to a length of 200 and 130 bp (forward and reverse reads, respectively) to on average retain base calls with a minimum Phred quality score of 30. Note that the truncLen needs to be set to values so that the minimum overlap during the merging of paired reads further down the pipleine is still possible. No truncating was applied for 18S due to the length variation of amplicons of this marker gene. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":107,\"length\":5},{\"style\":\"italic\",\"offset\":115,\"length\":14},{\"style\":\"italic\",\"offset\":139,\"length\":5},{\"style\":\"italic\",\"offset\":282,\"length\":8},{\"style\":\"italic\",\"offset\":451,\"length\":8}],\"entityRanges\":[],\"data\":{}},{\"key\":\"4mtn4\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4oq69\",\"text\":\"10^8 bp were used for error model calculation (default value, fewer bp were used automatically if a dataset did consist of fewer than 10^8 bp in total). An overlap of 10 bp with a maximum mismatch of 1 bp was applied during merging of paired reads. The amount of reads which \\\"survived\\\" each step of the pipleine was recorded for each sequencing run. For all randomized steps,  R's base function set.seed was applied to allow for reproducibility (see scripts). \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":377,\"length\":1},{\"style\":\"italic\",\"offset\":395,\"length\":8}],\"entityRanges\":[],\"data\":{}},{\"key\":\"eim6f\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"cjd5c\",\"text\":\"Below are several example scripts. For both COI and 18S, there are examples for the processing of sequencing runs with \\\"regular\\\" reads which still have primer sequences present (Run_1 example for 18S and COI). This script was applied for all sequencing runs with this kind of reads (see table above). When running this script for other runs, just change the name of the respective runs in the script accordingly. For COI and 18S, there also examples below for the processing of sequencing runs which are already devoid of primer sequences (see example Run_3 for COI and example Run_4 for 18S). This script was applied for all sequencing runs with this kind of reads (see table above). When running this script for other runs, just change the name of the respective runs in the script accordingly. To summarise, the following table shows which example script was applied for each run:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"aeb52\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"ccb5s\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":4,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"6b8mo\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3em1m\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":5,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"4q7td\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7vidq\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":6,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"57c7e\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"58pr2\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":7,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"3edn9\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"72u8e\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":8,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"e061j\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"1ut4\",\"text\":\"Subsequently, individual ASV tables of each sequencing run were merged (COI ASVs were additionally subjected to a stringent length filtering to only retain sequences with a length of of 310, 313 or 316 bp (so the expected length of 313 bp ± one codon triplet); this was not done for 18S due to the length variation of amplicons of this marker gene) and chimeric and singleton sequences were removed. For COI, a .fasta file containing the non-singleton ASV sequences with corresponding headers, as well as an ASV count table (read abundances per sample) were generated. Taxonomy was assigned later on in the workflow (see sections below). For 18S, ASVs were classified with dada2's assignTaxonomy function using three different reference sets (with varying taxonomic ranks) formatted for use in dada2: a) official Silva v132 containing prokayrote and eukaryote sequences (silva_nr_v132_train_set.fa.gz, https://zenodo.org/record/1172783); b) a subset of Silva v132 containing eukaryote sequences clustered at 99% similarity only (silva_132.18s.99_rep_set.dada2.fa.gz, https://zenodo.org/record/1447330); and c) PR2 v5.0.0 (pr2_version_5.0.0_SSU_dada2.fasta.gz, https://github.com/pr2database/pr2database/releases). A minimum bootstrap threshold of 70 was applied. An ensemble taxonomy, ASV fasta file and ASV count table for 18S ASVs were generated in the next section of the workflow (see below). For both COI and 18S, a final read tracking table was generated showing the amount of reads (total and as percentage compared to initial input) \\\"surviving\\\" each step of the cutadapt-dada2-pipeline. The following scripts were executed in R:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":673,\"length\":5},{\"style\":\"italic\",\"offset\":681,\"length\":14},{\"style\":\"italic\",\"offset\":794,\"length\":5},{\"style\":\"italic\",\"offset\":813,\"length\":5},{\"style\":\"italic\",\"offset\":953,\"length\":5},{\"style\":\"italic\",\"offset\":1110,\"length\":3},{\"style\":\"italic\",\"offset\":1570,\"length\":8},{\"style\":\"italic\",\"offset\":1579,\"length\":5},{\"style\":\"italic\",\"offset\":1634,\"length\":1}],\"entityRanges\":[{\"key\":9,\"offset\":902,\"length\":33},{\"key\":10,\"offset\":1067,\"length\":33},{\"key\":11,\"offset\":1160,\"length\":51}],\"data\":{}},{\"key\":\"1a5ea\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"707l5\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"k7r4\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":12,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"tjea\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"1btph\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":13,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"cgka8\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"887cv\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{\"0\":{\"type\":\"tables\",\"mutability\":\"IMMUTABLE\",\"data\":{\"cellsMeta\":{},\"colTitles\":[\"Run\",\"COI\",\"18S\"],\"colWidths\":[\"58\",\"251\",\"250\"],\"data\":[[\"Run_1\",\"MiSeq (indices)\",\"MiSeq (indices)*\"],[\"Run_2\",\"MiSeq (indices)\",\"MiSeq (indices)\"],[\"Run_3\",\"MiSeq (indices) / cutadapt (primers)\",\"MiSeq (indices)*\"],[\"Run_4\",\"MiSeq (indices) / cutadapt (primers)\",\"MiSeq (indices) / cutadapt (primers)\"],[\"Run_5\",\"MiSeq (indices)\",\"MiSeq (indices)\"],[\"Run_6\",\"MiSeq (indices)\",\"MiSeq (indices)\"],[\"Run_7\",\"MiSeq (indices) / cutadapt (primers)\",\"MiSeq (indices) / cutadapt (primers)\"]],\"guid\":\"D09A90E0406111EEAF6E310D6FB9CA7C\",\"isJexcelDataFormat\":true,\"legend\":{\"blocks\":[{\"data\":{},\"depth\":0,\"entityRanges\":[],\"inlineStyleRanges\":[],\"key\":\"dmotq\",\"text\":\"Demultiplexing strategies applied on the reads of the different sequencing runs of each marker gene. For *: see explanation above.\",\"type\":\"unstyled\"}],\"entityMap\":{}},\"mergeCells\":{},\"printData\":[[\"Run_1\",\"MiSeq (indices)\",\"MiSeq (indices)*\"],[\"Run_2\",\"MiSeq (indices)\",\"MiSeq (indices)\"],[\"Run_3\",\"MiSeq (indices) / cutadapt (primers)\",\"MiSeq (indices)*\"],[\"Run_4\",\"MiSeq (indices) / cutadapt (primers)\",\"MiSeq (indices) / cutadapt (primers)\"],[\"Run_5\",\"MiSeq (indices)\",\"MiSeq (indices)\"],[\"Run_6\",\"MiSeq (indices)\",\"MiSeq (indices)\"],[\"Run_7\",\"MiSeq (indices) / cutadapt (primers)\",\"MiSeq (indices) / cutadapt (primers)\"]],\"rowHeights\":[23,23,23,23,23,23,23]}},\"1\":{\"type\":\"link\",\"mutability\":\"\",\"data\":{\"guid\":\"F04C0E678E1A11EB9D8A0A58A9FEAC02\",\"url\":\"https://benjjneb.github.io/dada2/tutorial.html\"}},\"10\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"6B3C8931428911EEB8630A58A9FEAC02\",\"url\":\"https://zenodo.org/record/1447330\"}},\"11\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"B1C2C850428911EEB8630A58A9FEAC02\",\"url\":\"https://github.com/pr2database/pr2database/releases\"}},\"12\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"COI chimera \\u0026 singleton removal with dada2 in R\",\"description\":\"\",\"guid\":\"3B983B96428B11EEB8630A58A9FEAC02\",\"name\":\"library(dada2)\\n\\nsetwd(\\\"~/COI\\\")\\n\\n# Load the sequence table of the different sequence runs\\n\\nRun1\\u003c-readRDS(\\\"seqtab_Run1.rds\\\")\\nRun2\\u003c-readRDS(\\\"seqtab_Run2.rds\\\")\\nRun3\\u003c-readRDS(\\\"seqtab_Run3.rds\\\")\\nRun4\\u003c-readRDS(\\\"seqtab_Run4.rds\\\")\\nRun5\\u003c-readRDS(\\\"seqtab_Run5.rds\\\")\\nRun6\\u003c-readRDS(\\\"seqtab_Run6.rds\\\")\\nRun7\\u003c-readRDS(\\\"seqtab_Run7.rds\\\")\\n\\n# Merge sequence tables \\n\\nmerged\\u003c-mergeSequenceTables(Run1,Run2,Run3,Run4,Run5,Run6,Run7)\\n\\nsaveRDS(merged,\\\"merged_seqtab_coi.rds\\\")\\n\\n# Keep sequence reads with a length of 310, 313 or 316 bp only.\\n\\nseqtab.filtered \\u003c- merged[,nchar(colnames(merged)) %in% c(310,313,316)]\\n\\nsaveRDS(seqtab.filtered,\\\"seqtab_filtered_coi.rds\\\")\\n\\n# Remove chimeras #\\n\\nseqtab.nochim \\u003c- removeBimeraDenovo(seqtab.filtered, multithread=T, verbose=TRUE)\\n\\n# Save sequence table with the non-chimeric sequences as RDS file:\\n\\nsaveRDS(seqtab.nochim, \\\"seqtab_nochim_coi.rds\\\")\\n\\n# It is possible that a large fraction of the total number of UNIQUE SEQUENCES will be chimeras.\\n# However, this is usually not the case for the majority of the READS.\\n# Calculate percentage of the reads that were non-chimeric.\\n\\nsum(seqtab.nochim)/sum(merged)\\n\\n# Remove singletons from the non-chimeric ASVs\\n\\n#Transform counts to numeric (as they will most likely be integers)\\nmode(seqtab.nochim) = \\\"numeric\\\"\\n\\n# Subset columns with counts of \\u003e 1 and save to file\\nseqtab.nochim.nosingle\\u003c-seqtab.nochim[,colSums(seqtab.nochim) \\u003e 1]\\nsaveRDS(seqtab.nochim.nosingle,\\\"seqtab_nochim_nosingle_coi.rds\\\")\\n\\n# Write a fasta file of the final, non-chimeric , non-singleton sequences with short \\u003eASV... type headers\\n\\nasv_seqs \\u003c- colnames(seqtab.nochim.nosingle)\\nasv_headers \\u003c- vector(dim(seqtab.nochim.nosingle)[2], mode=\\\"character\\\")\\nfor (i in 1:dim(seqtab.nochim.nosingle)[2]) {\\n  asv_headers[i] \\u003c- paste(\\\"\\u003eASV\\\", i, sep=\\\"\\\")\\n}\\n\\nasv_fasta \\u003c- c(rbind(asv_headers, asv_seqs))\\nwrite(asv_fasta, \\\"COI_nochim_nosingle_ASVs.fa\\\")\\n\\n# Write an ASV count table of the final, non-chimeric, non-singleton sequences with short \\u003eASV... type names\\n\\ncolnames(seqtab.nochim.nosingle) \\u003c- paste0(\\\"ASV\\\", seq(ncol(seqtab.nochim.nosingle)))\\n\\nASV_counts\\u003c-t(seqtab.nochim.nosingle) # transposing table\\n\\nwrite.table(ASV_counts,file=\\\"COI_ASV_counts_nosingle.txt\\\",sep=\\\"\\\\t\\\", quote=F,col.names=NA)\\n\\n## Track reads through the entire dada2 pipeline ##\\n\\n# Track reads through the chimera and singleton removal step.\\n\\n# Read non-chimeric and non-singleton table again, as it has been modified\\nseqtab.nochim.nosingle\\u003c-readRDS(\\\"seqtab_nochim_nosingle_coi.rds\\\")\\n\\ntrack_nochim_nosingle\\u003c-cbind(rowSums(seqtab.filtered),rowSums(seqtab.nochim),rowSums(seqtab.nochim.nosingle))\\ncolnames(track_nochim_nosingle) \\u003c- c(\\\"length_filt\\\",\\\"nonchim\\\",\\\"nosingle\\\")\\n\\n# Read tracking tables of the single runs and combine\\n\\ntrack1\\u003c-read.table(\\\"track_Run1.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1)\\ntrack2\\u003c-read.table(\\\"track_Run2.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1)\\ntrack3\\u003c-read.table(\\\"track_Run3.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1)\\ntrack4\\u003c-read.table(\\\"track_Run4.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1)\\ntrack5\\u003c-read.table(\\\"track_Run5.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1)\\ntrack6\\u003c-read.table(\\\"track_Run6.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1)\\ntrack7\\u003c-read.table(\\\"track_Run7.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1)\\n\\ntracks\\u003c-rbind(track1,track2,track3,track4,track5,track6,track7)\\n\\n# Combine all tracking tables\\n\\ntracks\\u003c-tracks[order(match(rownames(tracks),rownames(track_nochim_nosingle))),]\\ntrack_coi\\u003c-cbind(tracks,track_nochim_nosingle)\\n\\n# Calculate percentages for each step compared to input\\n\\ntrack_coi$cutadapt_perc\\u003c-(track_coi$cutadapt / track_coi$input)*100\\ntrack_coi$filtered_perc\\u003c-(track_coi$filtered / track_coi$input)*100\\ntrack_coi$denoisedF_perc\\u003c-(track_coi$denoisedF / track_coi$input)*100\\ntrack_coi$denoisedR_perc\\u003c-(track_coi$denoisedR / track_coi$input)*100\\ntrack_coi$merged_perc\\u003c-(track_coi$merged / track_coi$input)*100\\ntrack_coi$length_filt_perc\\u003c-(track_coi$length / track_coi$input)*100\\ntrack_coi$nonchim_perc\\u003c-(track_coi$nonchim / track_coi$input)*100\\ntrack_coi$nosingle_perc\\u003c-(track_coi$nosingle / track_coi$input)*100\\n\\n# Save final read tracking table to file\\n\\nwrite.table(track_coi,\\\"track_COI.txt\\\",sep=\\\"\\\\t\\\",col.names = NA)\\n\",\"os_name\":\"\",\"os_version\":\"\"}},\"13\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"18S chimera \\u0026 singleton removal and taxonomic classification with dada2 in R\",\"description\":\"\",\"guid\":\"019FF278428B11EEB8630A58A9FEAC02\",\"name\":\"library(dada2)\\n\\nsetwd(\\\"~/18S\\\")\\n\\n# Load the sequence table of the different sequence runs\\n\\nRun1\\u003c-readRDS(\\\"seqtab_Run1.rds\\\")\\nRun2\\u003c-readRDS(\\\"seqtab_Run2.rds\\\")\\nRun3\\u003c-readRDS(\\\"seqtab_Run3.rds\\\")\\nRun4\\u003c-readRDS(\\\"seqtab_Run4.rds\\\")\\nRun5\\u003c-readRDS(\\\"seqtab_Run5.rds\\\")\\nRun6\\u003c-readRDS(\\\"seqtab_Run6.rds\\\")\\nRun7\\u003c-readRDS(\\\"seqtab_Run7.rds\\\")\\n\\n# Merge sequence tables \\n\\nmerged\\u003c-mergeSequenceTables(Run1,Run2,Run3,Run4,Run5,Run6,Run7)\\n\\nsaveRDS(merged,\\\"merged_seqtab_18S.rds\\\")\\n\\n# Do not apply length filtering due to length variability\\n\\n# Remove chimeras #\\n\\nseqtab.nochim \\u003c- removeBimeraDenovo(merged, multithread=T, verbose=TRUE)\\n\\n# Save sequence table with the non-chimeric sequences as RDS file:\\n\\nsaveRDS(seqtab.nochim, \\\"seqtab_nochim_18S.rds\\\")\\n\\n# It is possible that a large fraction of the total number of UNIQUE SEQUENCES will be chimeras.\\n# However, this is usually not the case for the majority of the READS.\\n# Calculate percentage of the reads that were non-chimeric.\\n\\nsum(seqtab.nochim)/sum(merged)\\n\\n# Remove singletons from the non-chimeric ASVs\\n\\n#Transform counts to numeric (as they will most likely be integers)\\n\\nmode(seqtab.nochim) = \\\"numeric\\\"\\n\\n# Subset columns with counts of \\u003e 1\\n\\nseqtab.nochim.nosingle\\u003c-seqtab.nochim[,colSums(seqtab.nochim) \\u003e 1]\\n\\nsaveRDS(seqtab.nochim.nosingle,\\\"seqtab_nochim_nosingle_18S.rds\\\")\\n\\n## Track reads through the entire dada2 pipeline ##\\n\\n# Track reads through the chimera and singleton removal step.\\n\\ntrack_nochim_nosingle\\u003c-cbind(rowSums(seqtab.nochim),rowSums(seqtab.nochim.nosingle))\\ncolnames(track_nochim_nosingle) \\u003c- c(\\\"nonchim\\\",\\\"nosingle\\\")\\n\\n# Read tracking tables of the single runs and combine\\n\\ntrack1\\u003c-read.table(\\\"track_Run1.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1)\\ntrack2\\u003c-read.table(\\\"track_Run2.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1)\\ntrack3\\u003c-read.table(\\\"track_Run3.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1)\\ntrack4\\u003c-read.table(\\\"track_Run4.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1)\\ntrack5\\u003c-read.table(\\\"track_Run5.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1)\\ntrack6\\u003c-read.table(\\\"track_Run6.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1)\\ntrack7\\u003c-read.table(\\\"track_Run7.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1)\\n\\ntracks\\u003c-rbind(track1,track2,track3,track4,track5,track6,track7)\\n\\n# Combine all tracking tables\\n\\ntracks\\u003c-tracks[order(match(rownames(tracks),rownames(track_nochim_nosingle))),]\\ntrack_18S\\u003c-cbind(tracks,track_nochim_nosingle)\\n\\n# Calculate percentages for each step compared to input\\n\\ntrack_18S$cutadapt_perc\\u003c-(track_18S$cutadapt / track_18S$input)*100\\ntrack_18S$filtered_perc\\u003c-(track_18S$filtered / track_18S$input)*100\\ntrack_18S$denoisedF_perc\\u003c-(track_18S$denoisedF / track_18S$input)*100\\ntrack_18S$denoisedR_perc\\u003c-(track_18S$denoisedR / track_18S$input)*100\\ntrack_18S$merged_perc\\u003c-(track_18S$merged / track_18S$input)*100\\ntrack_18S$nonchim_perc\\u003c-(track_18S$nonchim / track_18S$input)*100\\ntrack_18S$nosingle_perc\\u003c-(track_18S$nosingle / track_18S$input)*100\\n\\n# Save final read tracking table to file\\n\\nwrite.table(track_18S,\\\"track_18S.txt\\\",sep=\\\"\\\\t\\\",col.names = NA)\\n\\n## Assign taxonomy ##\\n\\n# Read non-chimeric, no-singleton sequence table again (in case the processes above have altered it)\\n\\nseqtab.nochim.nosingle\\u003c-readRDS(\\\"seqtab_nochim_nosingle_18S.rds\\\")\\n\\n## Official Silva v132 (prokaryote and eukaryote reference set)\\n\\n# Download here: https://zenodo.org/record/1172783\\n\\nsilva.ref\\u003c-\\\"silva_nr_v132_train_set.fa.gz\\\"\\n\\n# Set minBoot to 70\\n\\ntaxa_silva \\u003c- assignTaxonomy(seqtab.nochim.nosingle, silva.ref, minBoot=70, multithread=T,outputBootstraps = T)\\n\\nsaveRDS(taxa_silva,\\\"taxa_18S_silva.rds\\\")\\n\\n## Contributed Silva Eukaryote v132 99% clustered reference set\\n\\n# Download here: https://zenodo.org/record/1447330\\n\\nsilva.euk.ref \\u003c- \\\"silva_132.18s.99_rep_set.dada2.fa.gz\\\" \\n\\n# Set minBoot to 70.\\n# Define taxonomic ranks for this specific reference set\\n\\ntaxa_silva_euk \\u003c- assignTaxonomy(seqtab.nochim.nosingle, silva.euk.ref, minBoot=70, multithread=T,outputBootstraps = T,taxLevels=c(\\\"Domain\\\",\\\"Division\\\",\\\"Division_X\\\",\\\"Subdivision\\\",\\\"Class\\\",\\\"Order_Family\\\",\\\"Species\\\",\\\"Strain\\\"))\\n\\nsaveRDS(taxa_silva_euk,\\\"taxa_18S_silva_euk.rds\\\")\\n\\n## PR2 v5.0.0\\n\\n# Download here: https://github.com/pr2database/pr2database/releases\\n\\npr2.ref \\u003c- \\\"pr2_version_5.0.0_SSU_dada2.fasta.gz\\\" \\n\\n# Set minBoot to 70.\\n# Define taxonomic ranks for this specific reference set\\n\\ntaxa_pr2 \\u003c- assignTaxonomy(seqtab.nochim.nosingle, pr2.ref, minBoot=70, multithread=T,outputBootstraps = T, taxLevels = c(\\\"Domain\\\",\\\"Supergroup\\\",\\\"Division\\\",\\\"Subdivision\\\",\\\"Phylum\\\",\\\"Class_X\\\",\\\"Class_Order_Family\\\",\\\"Genus\\\",\\\"Species\\\"))\\n\\nsaveRDS(taxa_pr2,\\\"taxa_18S_pr2.rds\\\")\\n\\n### Continue with next script for 18S ensemble taxonomy ###\\n\\n# ASV count and taxonomy tables, as well as the fasta file with ASV sequences will be produced there #\\n\",\"os_name\":\"\",\"os_version\":\"\"}},\"2\":{\"type\":\"link\",\"mutability\":\"\",\"data\":{\"guid\":\"0AE8D9398E1B11EB9D8A0A58A9FEAC02\",\"url\":\"https://benjjneb.github.io/dada2/ITS_workflow.html\"}},\"3\":{\"type\":\"link\",\"mutability\":\"\",\"data\":{\"guid\":\"C674C7498E1711EB9D8A0A58A9FEAC02\",\"url\":\"https://cutadapt.readthedocs.io/en/stable/index.html\"}},\"4\":{\"type\":\"tables\",\"mutability\":\"IMMUTABLE\",\"data\":{\"cellsMeta\":{},\"colTitles\":[\"Run\",\"COI\",\"18S\"],\"colWidths\":[\"56\",\"139\",\"138\"],\"data\":[[\"Run_1\",\"COI example Run_1\",\"18S example Run_1\"],[\"Run_2\",\"COI example Run_1\",\"18S example Run_1\"],[\"Run_3\",\"COI example Run_3\",\"18S example Run_1\"],[\"Run_4\",\"COI example Run_3\",\"18S example Run_4\"],[\"Run_5\",\"COI example Run_1\",\"18S example Run_1\"],[\"Run_6\",\"COI example Run_1\",\"18S example Run_1\"],[\"Run_7\",\"COI example Run_3\",\"18S example Run_4\"]],\"guid\":\"0DBDAC60428211EEA62F8762B2601509\",\"isJexcelDataFormat\":true,\"legend\":{\"blocks\":[{\"data\":{},\"depth\":0,\"entityRanges\":[],\"inlineStyleRanges\":[],\"key\":\"e6nb3\",\"text\":\"This table shows which of each marker gene's run was processed with what kind of example script provided below.\",\"type\":\"unstyled\"}],\"entityMap\":{}},\"mergeCells\":{},\"printData\":[[\"Run_1\",\"COI example Run_1\",\"18S example Run_1\"],[\"Run_2\",\"COI example Run_1\",\"18S example Run_1\"],[\"Run_3\",\"COI example Run_3\",\"18S example Run_1\"],[\"Run_4\",\"COI example Run_3\",\"18S example Run_4\"],[\"Run_5\",\"COI example Run_1\",\"18S example Run_1\"],[\"Run_6\",\"COI example Run_1\",\"18S example Run_1\"],[\"Run_7\",\"COI example Run_3\",\"18S example Run_4\"]],\"rowHeights\":[23,23,23,23,23,23,23]}},\"5\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"COI: Run_1 example - primer trimming and ASV inference with cutadapt and dada2 in R until read merging\",\"description\":\"\",\"guid\":\"B475FDE4427311EEB8630A58A9FEAC02\",\"name\":\"### dada2 COI workflow with cutadapt primer removal ###\\n\\n# load / install necessary packages\\n\\nif (!requireNamespace(\\\"BiocManager\\\", quietly = TRUE))\\n  install.packages(\\\"BiocManager\\\")\\n\\nBiocManager::install(\\\"dada2\\\") # if this does not work, try to install via devtools (requires prior installation of devtools)\\nBiocManager::install(\\\"ShortRead\\\")\\nBiocManager::install(\\\"Biostrings\\\")\\n\\nlibrary(dada2)\\nlibrary(ShortRead)\\nlibrary(Biostrings)\\nlibrary(ggplot2)\\n\\n# directory containing the fastq.gz files\\n\\npath \\u003c- \\\"~/COI/fastqs_normal/Run_1\\\"\\n\\nlist.files(path)\\n\\n# generate matched lists of the forward and reverse read files, as well as parsing out the sample name\\n\\nfnFs \\u003c- sort(list.files(path, pattern = \\\"_1.fastq.gz\\\", full.names = TRUE))\\nfnRs \\u003c- sort(list.files(path, pattern = \\\"_2.fastq.gz\\\", full.names = TRUE))\\n\\n# Designate sequences [including ambiguous nucleotides (base = N, Y, W, etc.) if present) of the primers used\\n# The reverse COI primer jgHCO2198 contains Inosine nucleotides.\\n# These \\\"I\\\" bases are not part of IUPAC convention and are not recognized by the packages used here. Change \\\"I\\\"s to \\\"N\\\"s.\\n\\nFWD \\u003c- \\\"GGWACWGGWTGAACWGTWTAYCCYCC\\\"  ## forward primer sequence\\nREV \\u003c- \\\"TANACYTCNGGRTGNCCRAARAAYCA\\\"  ## reverse primer sequence\\n\\n# Verify the presence and orientation of these primers in the data\\n\\nallOrients \\u003c- function(primer) {\\n  # Create all orientations of the input sequence\\n  require(Biostrings)\\n  dna \\u003c- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors\\n  orients \\u003c- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), \\n               RevComp = reverseComplement(dna))\\n  return(sapply(orients, toString))  # Convert back to character vector\\n}\\nFWD.orients \\u003c- allOrients(FWD)\\nREV.orients \\u003c- allOrients(REV)\\nFWD.orients\\nREV.orients\\n\\n# Calculate number of reads containing forward and reverse primer sequences (considering all possible primer orientations. Only exact matches are found.).\\n# Only one set of paired end fastq.gz files will be checked (second sample in this case).\\n# This is is sufficient, assuming all the files were created using the same library preparation.\\n\\nprimerHits \\u003c- function(primer, fn) {\\n  # Counts number of reads in which the primer is found\\n  nhits \\u003c- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)\\n  return(sum(nhits \\u003e 0))\\n}\\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs[[2]]), \\n      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs[[2]]))\\n\\n# Output:\\n# FWD primer should mainly be found in the forward reads in its forward orientation.\\n# REV primer should mainly be found in the reverse reads in its forward orientation.\\n\\n# Use cutadapt for primer removal (prior installation of cutadapt on your machine via python, anaconda, etc. required)\\n# Tell R the path to cutadapt.\\n# Check installed version of cutadapt.\\n\\ncutadapt \\u003c- \\\"/sw/bioinfo/cutadapt/4.5/rackham/bin/cutadapt\\\" # CHANGE ME to the cutadapt path on your machine\\nsystem2(cutadapt, args = \\\"--version\\\") # see if R recognizes cutadapt and shows its version\\n\\n# Create output filenames for the cutadapt-ed files.\\n# Define the parameters for the cutadapt command.\\n# See here for a detailed explanation of paramter settings: https://cutadapt.readthedocs.io/en/stable/guide.html#\\n\\npath.cut \\u003c- file.path(path, \\\"cutadapt\\\")\\nif(!dir.exists(path.cut)) dir.create(path.cut)\\nfnFs.cut \\u003c- file.path(path.cut, basename(fnFs))\\nfnRs.cut \\u003c- file.path(path.cut, basename(fnRs))\\n\\n# Trim FWD off of R1 (forward reads) - \\nR1.flags \\u003c- paste0(\\\"-g\\\", \\\" ^\\\", FWD) \\n# Trim REV off of R2 (reverse reads)\\nR2.flags \\u003c- paste0(\\\"-G\\\", \\\" ^\\\", REV) \\n# Run Cutadapt\\nfor(i in seq_along(fnFs)) {\\n  system2(cutadapt, args = c(\\\"-e 0.1 --discard-untrimmed\\\", R1.flags, R2.flags,\\n                             \\\"-o\\\", fnFs.cut[i], \\\"-p\\\", fnRs.cut[i], # output files\\n                             fnFs[i], fnRs[i])) # input files\\n}\\n\\n# see here for a detailed explanation of the output:\\n# https://cutadapt.readthedocs.io/en/stable/guide.html#cutadapt-s-output\\n# Sometimes, you will see this: \\\"WARNING: One or more of your adapter sequences may be incomplete. Please see the detailed output above.\\\"\\n# This usually refers to: \\\"WARNING: The adapter is preceded by \\\"T\\\" (or any other base) extremely often. The provided adapter sequence could be incomplete at its 3' end.\\\"\\n# The amplified regions and primer binding sites are usually highly conserved, so primer sequences are often preceded by the same base.\\n# Cutadapt just warns us that this is the case and tells us to check if the preceding base is indeed not part of the primer. \\n\\n# Count the presence of primers in the first cutadapt-ed sample to check if cutadapt worked as intended:\\n\\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[2]]), \\n      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[2]]))\\n\\n# The primer-free sequence read files are now ready to be analyzed.\\n# Similar to the earlier steps of reading in FASTQ files, read in the names of the cutadapt-ed FASTQ files. \\n# Apply some string manipulation to get the matched lists of forward and reverse fastq files.\\n\\n# Forward and reverse fastq filenames have the format:\\ncutFs \\u003c- sort(list.files(path.cut, pattern = \\\"_1.fastq.gz\\\", full.names = TRUE))\\ncutRs \\u003c- sort(list.files(path.cut, pattern = \\\"_2.fastq.gz\\\", full.names = TRUE))\\n\\n# Check if forward and reverse files match:\\n\\nif(length(cutFs) == length(cutRs)) print(\\\"Forward and reverse files match. Go forth and explore\\\")\\nif (length(cutFs) != length(cutRs)) stop(\\\"Forward and reverse files do not match. Better go back and have a check\\\")\\n\\n# Extract sample names, assuming filenames have format:\\nget.sample.name \\u003c- function(fname) strsplit(basename(fname), \\\"_\\\")[[1]][1]\\nsample.names \\u003c- unname(sapply(cutFs, get.sample.name))\\nhead(sample.names)\\n\\n# Inspect read quality profiles. \\n# If there are more than 20 samples, grab 20 randomly\\n\\nset.seed(1)\\n\\nif(length(cutFs) \\u003c= 20) {\\n  fwd_qual_plots\\u003c-plotQualityProfile(cutFs) + \\n    scale_x_continuous(breaks=seq(0,300,20)) + \\n    scale_y_continuous(breaks=seq(0,40,5)) +\\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))+\\n    geom_hline(yintercept = 30)\\n  rev_qual_plots\\u003c-plotQualityProfile(cutRs) + \\n    scale_x_continuous(breaks=seq(0,300,20)) + \\n    scale_y_continuous(breaks=seq(0,40,5)) +\\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))+\\n    geom_hline(yintercept = 30)\\n} else {\\n  rand_samples \\u003c- sample(size = 20, 1:length(cutFs)) # grab 20 random samples to plot\\n  fwd_qual_plots \\u003c- plotQualityProfile(cutFs[rand_samples]) + \\n    scale_x_continuous(breaks=seq(0,300,20)) + \\n    scale_y_continuous(breaks=seq(0,40,5)) +\\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))+\\n    geom_hline(yintercept = 30)\\n  rev_qual_plots \\u003c- plotQualityProfile(cutRs[rand_samples]) + \\n    scale_x_continuous(breaks=seq(0,300,20)) + \\n    scale_y_continuous(breaks=seq(0,40,5)) +\\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))+\\n    geom_hline(yintercept = 30)\\n}\\nfwd_qual_plots\\nrev_qual_plots\\n\\n# Print out the forward quality plot\\n\\nsetwd(\\\"~/COI\\\")\\n\\njpeg(file=\\\"COI_Run1_quality_forward.jpg\\\",res=300, width=15, height=8, units=\\\"in\\\")\\nfwd_qual_plots\\ndev.off()\\n\\n# Print out the reverse quality plot\\n\\njpeg(file=\\\"COI_Run1_quality_reverse.jpg\\\",res=300, width=15, height=8, units=\\\"in\\\")\\nrev_qual_plots\\ndev.off()\\n\\n## Filter and trim ##\\n\\n# Assign filenames to the fastq.gz files of filtered and trimmed reads.\\n\\nfiltFs \\u003c- file.path(path.cut, \\\"filtered\\\", basename(cutFs))\\nfiltRs \\u003c- file.path(path.cut, \\\"filtered\\\", basename(cutRs))\\n\\n# Set filter and trim parameters.\\n\\nout \\u003c- filterAndTrim(cutFs, filtFs, cutRs, filtRs, truncLen =c(200,130),maxN = 0, maxEE = c(2,4), \\n                     truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = T) \\n\\n# Save this output as RDS file for the read tracking table created downstream:\\nsaveRDS(out, \\\"filter_and_trim_out_Run1.rds\\\")\\n\\n# check how many reads remain after filtering\\n\\nout\\n\\n# Check if file names match\\n\\nsample.names \\u003c- sapply(strsplit(basename(filtFs), \\\"_\\\"), `[`, 1) # Assumes filename = samplename_XXX.fastq.gz\\nsample.namesR \\u003c- sapply(strsplit(basename(filtRs), \\\"_\\\"), `[`, 1) # Assumes filename = samplename_XXX.fastq.gz\\nif(identical(sample.names, sample.namesR)) {print(\\\"Files are still matching.....congratulations\\\")\\n} else {stop(\\\"Forward and reverse files do not match.\\\")}\\nnames(filtFs) \\u003c- sample.names\\nnames(filtRs) \\u003c- sample.namesR\\n\\n# Estimate error models of the amplicon dataset. \\n\\nset.seed(100) # set seed to ensure that randomized steps are replicatable\\nerrF \\u003c- learnErrors(filtFs, multithread=T)\\nerrR \\u003c- learnErrors(filtRs, multithread=T)\\n\\n# save error calculation as RDS files:\\n\\nsaveRDS(errF, \\\"errF_Run1.rds\\\")\\nsaveRDS(errR, \\\"errR_Run1.rds\\\")\\n\\n# As a sanity check, visualize the estimated error rates and write to file:\\n\\nplot_err_F\\u003c-plotErrors(errF, nominalQ = TRUE)\\nplot_err_R\\u003c-plotErrors(errR, nominalQ = TRUE)\\n\\njpeg(file=\\\"COI_Run1_error_forward.jpg\\\")\\nplot_err_F\\ndev.off()\\n\\njpeg(file=\\\"COI_Run1_error_reverse.jpg\\\")\\nplot_err_R\\ndev.off()\\n\\n### The dada2 tutorial implements a dereplication step at this point. \\n### This does not seem to be necessary any more with the newer dada2 versions, according to what the developers stated in the dada2 github forum.\\n\\n# Apply the dada2's core sequence-variant inference algorithm:\\n\\n# Set pool = pseudo\\\", see https://benjjneb.github.io/dada2/pool.html\\n\\ndadaFs \\u003c- dada(filtFs, err=errF, multithread=T,pool=\\\"pseudo\\\")\\ndadaRs \\u003c- dada(filtRs, err=errR, multithread=T,pool=\\\"pseudo\\\")\\n\\n# Apply the sample names extracted earlier (see above) to remove the long fastq.gz file names\\nnames(dadaFs) \\u003c- sample.names\\nnames(dadaRs) \\u003c- sample.names\\n\\n# Save sequence-variant inference output as RDS files: \\n\\nsaveRDS(dadaFs, \\\"dadaFs_Run1.rds\\\")\\nsaveRDS(dadaRs, \\\"dadaRs_Run1.rds\\\")\\n\\n# Merge the forward and reverse reads.\\n# Adjust the minimum overlap (default = 12) and maximum mismatch allowed if necessary.\\n\\nmergers \\u003c- mergePairs(dadaFs, filtFs, dadaRs,filtRs,minOverlap = 10,maxMismatch = 1,verbose=TRUE)\\n\\nsaveRDS(mergers,\\\"mergers_Run1.rds\\\")\\n\\n# Construct an amplicon sequence variant table (ASV) table\\n# If maxMismatch \\u003e 0 has been allowed in the mergePairs step,\\n# \\\"Duplicate sequences detected and merged\\\" may appear as output during the sequence table creation\\n# This is not a problem, just ignore it.\\n\\nseqtab \\u003c- makeSequenceTable(mergers)\\n\\n# How many sequence variants were inferred?\\ndim(seqtab)\\n\\n# Save sequence table\\n\\nsaveRDS(seqtab, \\\"seqtab_Run1.rds\\\")\\n\\n## Track reads throughout the pipeline ##\\n\\n# Get number of reads in files prior to cutadapt application\\n\\ninput\\u003c-countFastq(path,pattern=\\\".gz\\\") # get statistics from input files\\ninput$Sample\\u003c-rownames(input)\\ninput$Sample\\u003c-gsub(\\\"_.*\\\",\\\"\\\",input$Sample) # Remove all characters after _ (incl. _) in file names\\ninput\\u003c-aggregate(.~Sample,input,FUN=\\\"mean\\\") # Aggregate forward and reverse read files\\n\\n# Get number of reads from each step of dada2 pipeline\\n\\ngetN \\u003c- function(x) sum(getUniques(x))\\ntrack \\u003c- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers,getN))\\ncolnames(track) \\u003c- c(\\\"cutadapt\\\", \\\"filtered\\\", \\\"denoisedF\\\", \\\"denoisedR\\\", \\\"merged\\\")\\nrownames(track) \\u003c- sample.names\\n\\n# Combine with read numbers from input files\\n\\ninput\\u003c-input[order(match(input[,1],rownames(track))),]\\ntrack\\u003c-cbind(input$records,track)\\ncolnames(track)[1]\\u003c-\\\"input\\\"\\n\\n# Save to file\\n\\nwrite.table(track,\\\"track_Run1.txt\\\",sep=\\\"\\\\t\\\",col.names = NA)\\n\",\"os_name\":\"\",\"os_version\":\"\"}},\"6\":{\"type\":\"command\",\"mutability\":\"\",\"data\":{\"can_edit\":true,\"command_name\":\"18S: Run_1 example - primer trimming and ASV inference with cutadapt and dada2 in R until read merging\",\"description\":null,\"guid\":\"47BFFBC0341F11EE9D4302C0B41BC903\",\"name\":\"### dada2 18S workflow with cutadapt primer removal ###\\n\\n# load / install necessary packages\\n\\nif (!requireNamespace(\\\"BiocManager\\\", quietly = TRUE))\\n  install.packages(\\\"BiocManager\\\")\\n\\nBiocManager::install(\\\"dada2\\\") # if this does not work, try to install via devtools (requires prior installation of devtools)\\nBiocManager::install(\\\"ShortRead\\\")\\nBiocManager::install(\\\"Biostrings\\\")\\n\\nlibrary(dada2)\\nlibrary(ShortRead)\\nlibrary(Biostrings)\\nlibrary(ggplot2)\\n\\n# directory containing the fastq.gz files\\n\\npath \\u003c- \\\"~/18S/fastqs_normal/Run_1\\\"\\n\\nlist.files(path)\\n\\n# generate matched lists of the forward and reverse read files, as well as parsing out the sample name\\n\\nfnFs \\u003c- sort(list.files(path, pattern = \\\"_1.fastq.gz\\\", full.names = TRUE))\\nfnRs \\u003c- sort(list.files(path, pattern = \\\"_2.fastq.gz\\\", full.names = TRUE))\\n\\n# Designate sequences [including ambiguous nucleotides (base = N, Y, W, etc.) if present) of the primers used\\n\\nFWD \\u003c- \\\"TGGTGCATGGCCGTTCTTAGT\\\"  ## forward primer sequence\\nREV \\u003c- \\\"CATCTAAGGGCATCACAGACC\\\"  ## reverse primer sequence\\n\\n# Verify the presence and orientation of these primers in the data\\n\\nallOrients \\u003c- function(primer) {\\n  # Create all orientations of the input sequence\\n  require(Biostrings)\\n  dna \\u003c- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors\\n  orients \\u003c- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), \\n               RevComp = reverseComplement(dna))\\n  return(sapply(orients, toString))  # Convert back to character vector\\n}\\nFWD.orients \\u003c- allOrients(FWD)\\nREV.orients \\u003c- allOrients(REV)\\nFWD.orients\\nREV.orients\\n\\n# Calculate number of reads containing forward and reverse primer sequences (considering all possible primer orientations. Only exact matches are found.).\\n# Only one set of paired end fastq.gz files will be checked (firstsample in this case).\\n# This is is sufficient, assuming all the files were created using the same library preparation.\\n\\nprimerHits \\u003c- function(primer, fn) {\\n  # Counts number of reads in which the primer is found\\n  nhits \\u003c- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)\\n  return(sum(nhits \\u003e 0))\\n}\\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs[[1]]), \\n      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs[[1]]), \\n      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs[[1]]), \\n      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs[[1]]))\\n\\n# Output:\\n# FWD primer should mainly be found in the forward reads in its forward orientation.\\n# FWD primer may also be found in some of the reverse reads in its reverse-complement orientation (due to read-through when amplicons are short).\\n# REV primer may also be found in the forward reads in its reverse complement orientation (due to read-through when amplicons are short).\\n# REV primer should mainly be found in the reverse reads in its forward orientation.\\n\\n# Use cutadapt for primer removal (prior installation of cutadapt on your machine via python, anaconda, etc. required)\\n# Tell R the path to cutadapt.\\n# Check installed version of cutadapt.\\n\\ncutadapt \\u003c- \\\"/sw/bioinfo/cutadapt/4.5/rackham/bin/cutadapt\\\" # CHANGE ME to the cutadapt path on your machine\\nsystem2(cutadapt, args = \\\"--version\\\") # see if R recognizes cutadapt and shows its version\\n\\n# Create output filenames for the cutadapt-ed files.\\n# Define the parameters for the cutadapt command.\\n# See here for a detailed explanation of paramter settings: https://cutadapt.readthedocs.io/en/stable/guide.html#\\n\\npath.cut \\u003c- file.path(path, \\\"cutadapt\\\")\\nif(!dir.exists(path.cut)) dir.create(path.cut)\\nfnFs.cut \\u003c- file.path(path.cut, basename(fnFs))\\nfnRs.cut \\u003c- file.path(path.cut, basename(fnRs))\\n\\nFWD.RC \\u003c- dada2:::rc(FWD)\\nREV.RC \\u003c- dada2:::rc(REV)\\n# Trim FWD and the reverse-complement of REV off of R1 (forward reads)\\nR1.flags \\u003c- paste(\\\"-g\\\", FWD, \\\"-a\\\", REV.RC) \\n# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)\\nR2.flags \\u003c- paste(\\\"-G\\\", REV, \\\"-A\\\", FWD.RC) \\n# Run Cutadapt\\nfor(i in seq_along(fnFs)) {\\n  system2(cutadapt, args = c(\\\"-e 0.05 --discard-untrimmed\\\",R1.flags, R2.flags, \\\"-m\\\",1, # -e sets the allowed error, -m 1 discards sequences of length zero after cutadapting\\n                             \\\"-n\\\", 2, # -n 2 required to remove FWD and REV from reads\\n                             \\\"-o\\\", fnFs.cut[i], \\\"-p\\\", fnRs.cut[i], # output files\\n                             fnFs[i], fnRs[i])) # input files\\n}\\n\\n# see here for a detailed explanation of the output:\\n# https://cutadapt.readthedocs.io/en/stable/guide.html#cutadapt-s-output\\n# Often, you will see this: \\\"WARNING: One or more of your adapter sequences may be incomplete. Please see the detailed output above.\\\"\\n# This usually refers to: \\\"WARNING: The adapter is preceded by \\\"T\\\" (or any other base) extremely often. The provided adapter sequence could be incomplete at its 3' end.\\\"\\n# The amplified regions and primer binding sites are usually highly conserved, so primer sequences are often preceded by the same base.\\n# Cutadapt just warns us that this is the case and tells us to check if the preceding base is indeed not part of the primer. Ignore the warning, this is not the case.\\n\\n# Count the presence of primers in the first cutadapt-ed sample as a check if cutadapt worked:\\n\\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), \\n      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), \\n      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), \\n      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))\\n\\n\\n# The primer-free sequence read files are now ready to be analyzed.\\n# Similar to the earlier steps of reading in FASTQ files, read in the names of the cutadapt-ed FASTQ files. \\n# Apply some string manipulation to get the matched lists of forward and reverse fastq files.\\n\\n# Forward and reverse fastq filenames have the format:\\ncutFs \\u003c- sort(list.files(path.cut, pattern = \\\"_1.fastq.gz\\\", full.names = TRUE))\\ncutRs \\u003c- sort(list.files(path.cut, pattern = \\\"_2.fastq.gz\\\", full.names = TRUE))\\n\\n# Check if forward and reverse files match:\\n\\nif(length(cutFs) == length(cutRs)) print(\\\"Forward and reverse files match. Go forth and explore\\\")\\nif (length(cutFs) != length(cutRs)) stop(\\\"Forward and reverse files do not match. Better go back and have a check\\\")\\n\\n# Extract sample names, assuming filenames have format:\\nget.sample.name \\u003c- function(fname) strsplit(basename(fname), \\\"_\\\")[[1]][1]\\nsample.names \\u003c- unname(sapply(cutFs, get.sample.name))\\nhead(sample.names)\\n\\n# Inspect read quality profiles. \\n# If there are more than 20 samples, grab 20 randomly\\n\\nset.seed(1)\\n\\nif(length(cutFs) \\u003c= 20) {\\n  fwd_qual_plots\\u003c-plotQualityProfile(cutFs) + \\n    scale_x_continuous(breaks=seq(0,300,20)) + \\n    scale_y_continuous(breaks=seq(0,40,5)) +\\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))+\\n    geom_hline(yintercept = 30)\\n  rev_qual_plots\\u003c-plotQualityProfile(cutRs) + \\n    scale_x_continuous(breaks=seq(0,300,20)) + \\n    scale_y_continuous(breaks=seq(0,40,5)) +\\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))+\\n    geom_hline(yintercept = 30)\\n} else {\\n  rand_samples \\u003c- sample(size = 20, 1:length(cutFs)) # grab 20 random samples to plot\\n  fwd_qual_plots \\u003c- plotQualityProfile(cutFs[rand_samples]) + \\n    scale_x_continuous(breaks=seq(0,300,20)) + \\n    scale_y_continuous(breaks=seq(0,40,5)) +\\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))+\\n    geom_hline(yintercept = 30)\\n  rev_qual_plots \\u003c- plotQualityProfile(cutRs[rand_samples]) + \\n    scale_x_continuous(breaks=seq(0,300,20)) + \\n    scale_y_continuous(breaks=seq(0,40,5)) +\\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))+\\n    geom_hline(yintercept = 30)\\n}\\nfwd_qual_plots\\nrev_qual_plots\\n\\n# Print out the forward quality plot\\n\\nsetwd(\\\"~/18S\\\")\\n\\njpeg(file=\\\"18S_Run1_quality_forward.jpg\\\",res=300, width=15, height=8, units=\\\"in\\\")\\nfwd_qual_plots\\ndev.off()\\n\\n# Print out the reverse quality plot\\n\\njpeg(file=\\\"18S_Run1_quality_reverse.jpg\\\",res=300, width=15, height=8, units=\\\"in\\\")\\nrev_qual_plots\\ndev.off()\\n\\n## Filter and trim ##\\n\\n# Assign filenames to the fastq.gz files of filtered and trimmed reads.\\n\\nfiltFs \\u003c- file.path(path.cut, \\\"filtered\\\", basename(cutFs))\\nfiltRs \\u003c- file.path(path.cut, \\\"filtered\\\", basename(cutRs))\\n\\n# Set filter and trim parameters.\\n\\nout \\u003c- filterAndTrim(cutFs, filtFs, cutRs, filtRs,maxN = 0, maxEE = c(2,4), \\n                     truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = T) \\n\\n# Save this output as RDS file for the read tracking table created downstream:\\nsaveRDS(out, \\\"filter_and_trim_out_Run1.rds\\\")\\n\\n# check how many reads remain after filtering\\n\\nout\\n\\n# Check if file names match\\n\\nsample.names \\u003c- sapply(strsplit(basename(filtFs), \\\"_\\\"), `[`, 1) # Assumes filename = samplename_XXX.fastq.gz\\nsample.namesR \\u003c- sapply(strsplit(basename(filtRs), \\\"_\\\"), `[`, 1) # Assumes filename = samplename_XXX.fastq.gz\\nif(identical(sample.names, sample.namesR)) {print(\\\"Files are still matching.....congratulations\\\")\\n} else {stop(\\\"Forward and reverse files do not match.\\\")}\\nnames(filtFs) \\u003c- sample.names\\nnames(filtRs) \\u003c- sample.namesR\\n\\n# Estimate error models of the amplicon dataset. \\n\\nset.seed(100) # set seed to ensure that randomized steps are replicatable\\nerrF \\u003c- learnErrors(filtFs, multithread=T)\\nerrR \\u003c- learnErrors(filtRs, multithread=T)\\n\\n# save error calculation as RDS files:\\n\\nsaveRDS(errF, \\\"errF_Run1.rds\\\")\\nsaveRDS(errR, \\\"errR_Run1.rds\\\")\\n\\n# As a sanity check, visualize the estimated error rates and write to file:\\n\\nplot_err_F\\u003c-plotErrors(errF, nominalQ = TRUE)\\nplot_err_R\\u003c-plotErrors(errR, nominalQ = TRUE)\\n\\njpeg(file=\\\"18S_Run1_error_forward.jpg\\\")\\nplot_err_F\\ndev.off()\\n\\njpeg(file=\\\"18S_Run1_error_reverse.jpg\\\")\\nplot_err_R\\ndev.off()\\n\\n### The dada2 tutorial implements a dereplication step at this point. \\n### This does not seem to be necessary any more with the newer dada2 versions, according to what the developers stated in the dada2 github forum.\\n\\n# Apply the dada2's core sequence-variant inference algorithm:\\n# Set pool = pseudo\\\", see https://benjjneb.github.io/dada2/pool.html\\n\\ndadaFs \\u003c- dada(filtFs, err=errF, multithread=T,pool=\\\"pseudo\\\")\\ndadaRs \\u003c- dada(filtRs, err=errR, multithread=T,pool=\\\"pseudo\\\")\\n\\n# Apply the sample names extracted earlier (see above) to remove the long fastq.gz file names\\nnames(dadaFs) \\u003c- sample.names\\nnames(dadaRs) \\u003c- sample.names\\n\\n# Save sequence-variant inference output as RDS files: \\n\\nsaveRDS(dadaFs, \\\"dadaFs_Run1.rds\\\")\\nsaveRDS(dadaRs, \\\"dadaRs_Run1.rds\\\")\\n\\n# Merge the forward and reverse reads.\\n# Adjust the minimum overlap (default = 12) and maximum mismatch allowed if necessary.\\n\\nmergers \\u003c- mergePairs(dadaFs, filtFs, dadaRs,filtRs,minOverlap = 10,maxMismatch = 1,verbose=TRUE)\\n\\nsaveRDS(mergers,\\\"mergers_Run1.rds\\\")\\n\\n# Construct an amplicon sequence variant table (ASV) table\\n# If maxMismatch \\u003e 0 has been allowed in the mergePairs step,\\n# \\\"Duplicate sequences detected and merged\\\" may appear as output during the sequence table creation\\n# This is not a problem, just ignore it.\\n\\nseqtab \\u003c- makeSequenceTable(mergers)\\n\\n# How many sequence variants were inferred?\\ndim(seqtab)\\n\\n# Save sequence table\\n\\nsaveRDS(seqtab, \\\"seqtab_Run1.rds\\\")\\n\\n## Track reads throughout the pipeline ##\\n\\n# Get number of reads in files prior to cutadapt application\\n\\ninput\\u003c-countFastq(path,pattern=\\\".gz\\\") # get statistics from input files\\ninput$Sample\\u003c-rownames(input)\\ninput$Sample\\u003c-gsub(\\\"_.*\\\",\\\"\\\",input$Sample) # Remove all characters after _ (incl. _) in file names\\ninput\\u003c-aggregate(.~Sample,input,FUN=\\\"mean\\\") # Aggregate forward and reverse read files\\n\\n# Get number of reads from each step of dada2 pipeline\\n\\ngetN \\u003c- function(x) sum(getUniques(x))\\ntrack \\u003c- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers,getN))\\ncolnames(track) \\u003c- c(\\\"cutadapt\\\", \\\"filtered\\\", \\\"denoisedF\\\", \\\"denoisedR\\\", \\\"merged\\\")\\nrownames(track) \\u003c- sample.names\\n\\n# Combine with read numbers from input files\\n\\ninput\\u003c-input[order(match(input[,1],rownames(track))),]\\ntrack\\u003c-cbind(input$records,track)\\ncolnames(track)[1]\\u003c-\\\"input\\\"\\n\\n# Save to file\\n\\nwrite.table(track,\\\"track_Run1.txt\\\",sep=\\\"\\\\t\\\",col.names = NA)\\n\",\"os_name\":null,\"os_version\":null}},\"7\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"COI: Run_3 example - length filtering and ASV inference with cutadapt and dada2 in R until read merging\",\"description\":\"\",\"guid\":\"CE627831428011EEB8630A58A9FEAC02\",\"name\":\"### dada2 COI workflow without cutadapt primer removal ###\\n\\n# load / install necessary packages\\n\\nif (!requireNamespace(\\\"BiocManager\\\", quietly = TRUE))\\n  install.packages(\\\"BiocManager\\\")\\n\\nBiocManager::install(\\\"dada2\\\") # if this does not work, try to install via devtools (requires prior installation of devtools)\\nBiocManager::install(\\\"ShortRead\\\")\\nBiocManager::install(\\\"Biostrings\\\")\\n\\nlibrary(dada2)\\nlibrary(ShortRead)\\nlibrary(Biostrings)\\nlibrary(ggplot2)\\n\\n# directory containing the fastq.gz files\\n\\npath \\u003c- \\\"~/COI/fastqs_cutadapt/Run_3\\\"\\n\\nlist.files(path)\\n\\n# generate matched lists of the forward and reverse read files, as well as parsing out the sample name\\n\\nfnFs \\u003c- sort(list.files(path, pattern = \\\"_1.fastq.gz\\\", full.names = TRUE))\\nfnRs \\u003c- sort(list.files(path, pattern = \\\"_2.fastq.gz\\\", full.names = TRUE))\\n\\n# Filter reads only for length\\n\\ncutadapt \\u003c- \\\"/sw/bioinfo/cutadapt/4.5/rackham/bin/cutadapt\\\" # CHANGE ME to the cutadapt path on your machine\\nsystem2(cutadapt, args = \\\"--version\\\") # see if R recognizes cutadapt and shows its version\\n\\n# Create output filenames for the cutadapt-ed files.\\n# Define the parameters for the cutadapt command.\\n# See here for a detailed explanation of paramter settings: https://cutadapt.readthedocs.io/en/stable/guide.html#\\n\\npath.cut \\u003c- file.path(path, \\\"cutadapt\\\")\\nif(!dir.exists(path.cut)) dir.create(path.cut)\\nfnFs.cut \\u003c- file.path(path.cut, basename(fnFs))\\nfnRs.cut \\u003c- file.path(path.cut, basename(fnRs))\\n\\n# Run Cutadapt just for length filtering\\nfor(i in seq_along(fnFs)) {\\n  system2(cutadapt, args = c(\\\"-m 1\\\", \\n                             \\\"-o\\\", fnFs.cut[i], \\\"-p\\\", fnRs.cut[i], # output files\\n                             fnFs[i], fnRs[i])) # input files\\n}\\n\\n# see here for a detailed explanation of the output:\\n# https://cutadapt.readthedocs.io/en/stable/guide.html#cutadapt-s-output\\n\\n# The length-filtered sequence read files are now ready to be analyzed.\\n# Similar to the earlier steps of reading in FASTQ files, read in the names of the cutadapt-ed FASTQ files. \\n# Apply some string manipulation to get the matched lists of forward and reverse fastq files.\\n\\n# Forward and reverse fastq filenames have the format:\\ncutFs \\u003c- sort(list.files(path.cut, pattern = \\\"_1.fastq.gz\\\", full.names = TRUE))\\ncutRs \\u003c- sort(list.files(path.cut, pattern = \\\"_2.fastq.gz\\\", full.names = TRUE))\\n\\n# Check if forward and reverse files match:\\n\\nif(length(cutFs) == length(cutRs)) print(\\\"Forward and reverse files match. Go forth and explore\\\")\\nif (length(cutFs) != length(cutRs)) stop(\\\"Forward and reverse files do not match. Better go back and have a check\\\")\\n\\n# Extract sample names, assuming filenames have format:\\nget.sample.name \\u003c- function(fname) strsplit(basename(fname), \\\"_\\\")[[1]][1]\\nsample.names \\u003c- unname(sapply(cutFs, get.sample.name))\\nhead(sample.names)\\n\\n# Inspect read quality profiles. \\n# If there are more than 20 samples, grab 20 randomly\\n\\nset.seed(1)\\n\\nif(length(cutFs) \\u003c= 20) {\\n  fwd_qual_plots\\u003c-plotQualityProfile(cutFs) + \\n    scale_x_continuous(breaks=seq(0,300,20)) + \\n    scale_y_continuous(breaks=seq(0,40,5)) +\\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))+\\n    geom_hline(yintercept = 30)\\n  rev_qual_plots\\u003c-plotQualityProfile(cutRs) + \\n    scale_x_continuous(breaks=seq(0,300,20)) + \\n    scale_y_continuous(breaks=seq(0,40,5)) +\\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))+\\n    geom_hline(yintercept = 30)\\n} else {\\n  rand_samples \\u003c- sample(size = 20, 1:length(cutFs)) # grab 20 random samples to plot\\n  fwd_qual_plots \\u003c- plotQualityProfile(cutFs[rand_samples]) + \\n    scale_x_continuous(breaks=seq(0,300,20)) + \\n    scale_y_continuous(breaks=seq(0,40,5)) +\\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))+\\n    geom_hline(yintercept = 30)\\n  rev_qual_plots \\u003c- plotQualityProfile(cutRs[rand_samples]) + \\n    scale_x_continuous(breaks=seq(0,300,20)) + \\n    scale_y_continuous(breaks=seq(0,40,5)) +\\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))+\\n    geom_hline(yintercept = 30)\\n}\\nfwd_qual_plots\\nrev_qual_plots\\n\\n# Print out the forward quality plot\\n\\nsetwd(\\\"~/COI\\\")\\n\\njpeg(file=\\\"COI_Run3_quality_forward.jpg\\\",res=300, width=15, height=8, units=\\\"in\\\")\\nfwd_qual_plots\\ndev.off()\\n\\n# Print out the reverse quality plot\\n\\njpeg(file=\\\"COI_Run3_quality_reverse.jpg\\\",res=300, width=15, height=8, units=\\\"in\\\")\\nrev_qual_plots\\ndev.off()\\n\\n## Filter and trim ##\\n\\n# Assign filenames to the fastq.gz files of filtered and trimmed reads.\\n\\nfiltFs \\u003c- file.path(path.cut, \\\"filtered\\\", basename(cutFs))\\nfiltRs \\u003c- file.path(path.cut, \\\"filtered\\\", basename(cutRs))\\n\\n# Set filter and trim parameters.\\n\\nout \\u003c- filterAndTrim(cutFs, filtFs, cutRs, filtRs, truncLen =c(200,130),maxN = 0, maxEE = c(2,4), \\n                     truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = T) \\n\\n# Save this output as RDS file for the read tracking table created downstream:\\nsaveRDS(out, \\\"filter_and_trim_out_Run3.rds\\\")\\n\\n# check how many reads remain after filtering\\n\\nout\\n\\n# Check if file names match\\n\\nsample.names \\u003c- sapply(strsplit(basename(filtFs), \\\"_\\\"), `[`, 1) # Assumes filename = samplename_XXX.fastq.gz\\nsample.namesR \\u003c- sapply(strsplit(basename(filtRs), \\\"_\\\"), `[`, 1) # Assumes filename = samplename_XXX.fastq.gz\\nif(identical(sample.names, sample.namesR)) {print(\\\"Files are still matching.....congratulations\\\")\\n} else {stop(\\\"Forward and reverse files do not match.\\\")}\\nnames(filtFs) \\u003c- sample.names\\nnames(filtRs) \\u003c- sample.namesR\\n\\n# Estimate error models of the amplicon dataset. \\n\\nset.seed(100) # set seed to ensure that randomized steps are replicatable\\nerrF \\u003c- learnErrors(filtFs, multithread=T)\\nerrR \\u003c- learnErrors(filtRs, multithread=T)\\n\\n# save error calculation as RDS files:\\n\\nsaveRDS(errF, \\\"errF_Run3.rds\\\")\\nsaveRDS(errR, \\\"errR_Run3.rds\\\")\\n\\n# As a sanity check, visualize the estimated error rates and write to file:\\n\\nplot_err_F\\u003c-plotErrors(errF, nominalQ = TRUE)\\nplot_err_R\\u003c-plotErrors(errR, nominalQ = TRUE)\\n\\njpeg(file=\\\"COI_Run3_error_forward.jpg\\\")\\nplot_err_F\\ndev.off()\\n\\njpeg(file=\\\"COI_Run3_error_reverse.jpg\\\")\\nplot_err_R\\ndev.off()\\n\\n### The dada2 tutorial implements a dereplication step at this point. \\n### This does not seem to be necessary any more with the newer dada2 versions, according to what the developers stated in the dada2 github forum.\\n\\n# Apply the dada2's core sequence-variant inference algorithm:\\n\\n# Set pool = pseudo\\\", see https://benjjneb.github.io/dada2/pool.html\\n\\ndadaFs \\u003c- dada(filtFs, err=errF, multithread=T,pool=\\\"pseudo\\\")\\ndadaRs \\u003c- dada(filtRs, err=errR, multithread=T,pool=\\\"pseudo\\\")\\n\\n# Apply the sample names extracted earlier (see above) to remove the long fastq.gz file names\\nnames(dadaFs) \\u003c- sample.names\\nnames(dadaRs) \\u003c- sample.names\\n\\n# Save sequence-variant inference output as RDS files: \\n\\nsaveRDS(dadaFs, \\\"dadaFs_Run3.rds\\\")\\nsaveRDS(dadaRs, \\\"dadaRs_Run3.rds\\\")\\n\\n# Merge the forward and reverse reads.\\n# Adjust the minimum overlap (default = 12) and maximum mismatch allowed if necessary.\\n\\nmergers \\u003c- mergePairs(dadaFs, filtFs, dadaRs,filtRs,minOverlap = 10,maxMismatch = 1,verbose=TRUE)\\n\\nsaveRDS(mergers,\\\"mergers_Run3.rds\\\")\\n\\n# Construct an amplicon sequence variant table (ASV) table\\n# If maxMismatch \\u003e 0 has been allowed in the mergePairs step,\\n# \\\"Duplicate sequences detected and merged\\\" may appear as output during the sequence table creation\\n# This is not a problem, just ignore it.\\n\\nseqtab \\u003c- makeSequenceTable(mergers)\\n\\n# How many sequence variants were inferred?\\ndim(seqtab)\\n\\n# Save sequence table\\n\\nsaveRDS(seqtab, \\\"seqtab_Run3.rds\\\")\\n\\n## Track reads throughout the pipeline ##\\n\\n# Get number of reads in files prior to cutadapt application\\n\\ninput\\u003c-countFastq(path,pattern=\\\".gz\\\") # get statistics from input files\\ninput$Sample\\u003c-rownames(input)\\ninput$Sample\\u003c-gsub(\\\"_.*\\\",\\\"\\\",input$Sample) # Remove all characters after _ (incl. _) in file names\\ninput\\u003c-aggregate(.~Sample,input,FUN=\\\"mean\\\") # Aggregate forward and reverse read files\\n\\n# Get number of reads from each step of dada2 pipeline\\n\\ngetN \\u003c- function(x) sum(getUniques(x))\\ntrack \\u003c- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers,getN))\\ncolnames(track) \\u003c- c(\\\"cutadapt\\\", \\\"filtered\\\", \\\"denoisedF\\\", \\\"denoisedR\\\", \\\"merged\\\")\\nrownames(track) \\u003c- sample.names\\n\\n# Combine with read numbers from input files\\n\\ninput\\u003c-input[order(match(input[,1],rownames(track))),]\\ntrack\\u003c-cbind(input$records,track)\\ncolnames(track)[1]\\u003c-\\\"input\\\"\\n\\n# Save to file\\n\\nwrite.table(track,\\\"track_Run3.txt\\\",sep=\\\"\\\\t\\\",col.names = NA)\\n\",\"os_name\":\"\",\"os_version\":\"\"}},\"8\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"18S: Run_4 example - length filtering and ASV inference with cutadapt and dada2 in R until read merging\",\"description\":\"\",\"guid\":\"0E97748E428311EEB8630A58A9FEAC02\",\"name\":\"### dada2 18S workflow without cutadapt primer removal ###\\n\\n# load / install necessary packages\\n\\nif (!requireNamespace(\\\"BiocManager\\\", quietly = TRUE))\\n  install.packages(\\\"BiocManager\\\")\\n\\nBiocManager::install(\\\"dada2\\\") # if this does not work, try to install via devtools (requires prior installation of devtools)\\nBiocManager::install(\\\"ShortRead\\\")\\nBiocManager::install(\\\"Biostrings\\\")\\n\\nlibrary(dada2)\\nlibrary(ShortRead)\\nlibrary(Biostrings)\\nlibrary(ggplot2)\\n\\n# directory containing the fastq.gz files\\n\\npath \\u003c- \\\"~/18S/fastqs_cutadapt/Run_4\\\"\\n\\nlist.files(path)\\n\\n# generate matched lists of the forward and reverse read files, as well as parsing out the sample name\\n\\nfnFs \\u003c- sort(list.files(path, pattern = \\\"_1.fastq.gz\\\", full.names = TRUE))\\nfnRs \\u003c- sort(list.files(path, pattern = \\\"_2.fastq.gz\\\", full.names = TRUE))\\n\\n# Filter reads only for length\\n\\ncutadapt \\u003c- \\\"/sw/bioinfo/cutadapt/4.5/rackham/bin/cutadapt\\\" # CHANGE ME to the cutadapt path on your machine\\nsystem2(cutadapt, args = \\\"--version\\\") # see if R recognizes cutadapt and shows its version\\n\\n# Create output filenames for the cutadapt-ed files.\\n# Define the parameters for the cutadapt command.\\n# See here for a detailed explanation of paramter settings: https://cutadapt.readthedocs.io/en/stable/guide.html#\\n\\npath.cut \\u003c- file.path(path, \\\"cutadapt\\\")\\nif(!dir.exists(path.cut)) dir.create(path.cut)\\nfnFs.cut \\u003c- file.path(path.cut, basename(fnFs))\\nfnRs.cut \\u003c- file.path(path.cut, basename(fnRs))\\n\\n# Run Cutadapt just for length filtering\\nfor(i in seq_along(fnFs)) {\\n  system2(cutadapt, args = c(\\\"-m 1\\\", \\n                             \\\"-o\\\", fnFs.cut[i], \\\"-p\\\", fnRs.cut[i], # output files\\n                             fnFs[i], fnRs[i])) # input files\\n}\\n\\n# see here for a detailed explanation of the output:\\n# https://cutadapt.readthedocs.io/en/stable/guide.html#cutadapt-s-output\\n\\n# The primer-free sequence read files are now ready to be analyzed.\\n# Similar to the earlier steps of reading in FASTQ files, read in the names of the cutadapt-ed FASTQ files. \\n# Apply some string manipulation to get the matched lists of forward and reverse fastq files.\\n\\n# Forward and reverse fastq filenames have the format:\\ncutFs \\u003c- sort(list.files(path.cut, pattern = \\\"_1.fastq.gz\\\", full.names = TRUE))\\ncutRs \\u003c- sort(list.files(path.cut, pattern = \\\"_2.fastq.gz\\\", full.names = TRUE))\\n\\n# Check if forward and reverse files match:\\n\\nif(length(cutFs) == length(cutRs)) print(\\\"Forward and reverse files match. Go forth and explore\\\")\\nif (length(cutFs) != length(cutRs)) stop(\\\"Forward and reverse files do not match. Better go back and have a check\\\")\\n\\n# Extract sample names, assuming filenames have format:\\nget.sample.name \\u003c- function(fname) strsplit(basename(fname), \\\"_\\\")[[1]][1]\\nsample.names \\u003c- unname(sapply(cutFs, get.sample.name))\\nhead(sample.names)\\n\\n# Inspect read quality profiles. \\n# If there are more than 20 samples, grab 20 randomly\\n\\nset.seed(1)\\n\\nif(length(cutFs) \\u003c= 20) {\\n  fwd_qual_plots\\u003c-plotQualityProfile(cutFs) + \\n    scale_x_continuous(breaks=seq(0,300,20)) + \\n    scale_y_continuous(breaks=seq(0,40,5)) +\\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))+\\n    geom_hline(yintercept = 30)\\n  rev_qual_plots\\u003c-plotQualityProfile(cutRs) + \\n    scale_x_continuous(breaks=seq(0,300,20)) + \\n    scale_y_continuous(breaks=seq(0,40,5)) +\\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))+\\n    geom_hline(yintercept = 30)\\n} else {\\n  rand_samples \\u003c- sample(size = 20, 1:length(cutFs)) # grab 20 random samples to plot\\n  fwd_qual_plots \\u003c- plotQualityProfile(cutFs[rand_samples]) + \\n    scale_x_continuous(breaks=seq(0,300,20)) + \\n    scale_y_continuous(breaks=seq(0,40,5)) +\\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))+\\n    geom_hline(yintercept = 30)\\n  rev_qual_plots \\u003c- plotQualityProfile(cutRs[rand_samples]) + \\n    scale_x_continuous(breaks=seq(0,300,20)) + \\n    scale_y_continuous(breaks=seq(0,40,5)) +\\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))+\\n    geom_hline(yintercept = 30)\\n}\\nfwd_qual_plots\\nrev_qual_plots\\n\\n# Print out the forward quality plot\\n\\nsetwd(\\\"~/18S\\\")\\n\\njpeg(file=\\\"18S_Run4_quality_forward.jpg\\\",res=300, width=15, height=8, units=\\\"in\\\")\\nfwd_qual_plots\\ndev.off()\\n\\n# Print out the reverse quality plot\\n\\njpeg(file=\\\"18S_Run4_quality_reverse.jpg\\\",res=300, width=15, height=8, units=\\\"in\\\")\\nrev_qual_plots\\ndev.off()\\n\\n## Filter and trim ##\\n\\n# Assign filenames to the fastq.gz files of filtered and trimmed reads.\\n\\nfiltFs \\u003c- file.path(path.cut, \\\"filtered\\\", basename(cutFs))\\nfiltRs \\u003c- file.path(path.cut, \\\"filtered\\\", basename(cutRs))\\n\\n# Set filter and trim parameters.\\n\\nout \\u003c- filterAndTrim(cutFs, filtFs, cutRs, filtRs,maxN = 0, maxEE = c(2,4), \\n                     truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = T) \\n\\n# Save this output as RDS file for the read tracking table created downstream:\\nsaveRDS(out, \\\"filter_and_trim_out_Run4.rds\\\")\\n\\n# check how many reads remain after filtering\\n\\nout\\n\\n# Check if file names match\\n\\nsample.names \\u003c- sapply(strsplit(basename(filtFs), \\\"_\\\"), `[`, 1) # Assumes filename = samplename_XXX.fastq.gz\\nsample.namesR \\u003c- sapply(strsplit(basename(filtRs), \\\"_\\\"), `[`, 1) # Assumes filename = samplename_XXX.fastq.gz\\nif(identical(sample.names, sample.namesR)) {print(\\\"Files are still matching.....congratulations\\\")\\n} else {stop(\\\"Forward and reverse files do not match.\\\")}\\nnames(filtFs) \\u003c- sample.names\\nnames(filtRs) \\u003c- sample.namesR\\n\\n# Estimate error models of the amplicon dataset. \\n\\nset.seed(100) # set seed to ensure that randomized steps are replicatable\\nerrF \\u003c- learnErrors(filtFs, multithread=T)\\nerrR \\u003c- learnErrors(filtRs, multithread=T)\\n\\n# save error calculation as RDS files:\\n\\nsaveRDS(errF, \\\"errF_Run4.rds\\\")\\nsaveRDS(errR, \\\"errR_Run4.rds\\\")\\n\\n# As a sanity check, visualize the estimated error rates and write to file:\\n\\nplot_err_F\\u003c-plotErrors(errF, nominalQ = TRUE)\\nplot_err_R\\u003c-plotErrors(errR, nominalQ = TRUE)\\n\\njpeg(file=\\\"18S_Run4_error_forward.jpg\\\")\\nplot_err_F\\ndev.off()\\n\\njpeg(file=\\\"18S_Run4_error_reverse.jpg\\\")\\nplot_err_R\\ndev.off()\\n\\n### The dada2 tutorial implements a dereplication step at this point. \\n### This does not seem to be necessary any more with the newer dada2 versions, according to what the developers stated in the dada2 github forum.\\n\\n# Apply the dada2's core sequence-variant inference algorithm:\\n\\n# Set pool = pseudo\\\", see https://benjjneb.github.io/dada2/pool.html\\n\\ndadaFs \\u003c- dada(filtFs, err=errF, multithread=T,pool=\\\"pseudo\\\")\\ndadaRs \\u003c- dada(filtRs, err=errR, multithread=T,pool=\\\"pseudo\\\")\\n\\n# Apply the sample names extracted earlier (see above) to remove the long fastq.gz file names\\nnames(dadaFs) \\u003c- sample.names\\nnames(dadaRs) \\u003c- sample.names\\n\\n# Save sequence-variant inference output as RDS files: \\n\\nsaveRDS(dadaFs, \\\"dadaFs_Run4.rds\\\")\\nsaveRDS(dadaRs, \\\"dadaRs_Run4.rds\\\")\\n\\n# Merge the forward and reverse reads.\\n# Adjust the minimum overlap (default = 12) and maximum mismatch allowed if necessary.\\n\\nmergers \\u003c- mergePairs(dadaFs, filtFs, dadaRs,filtRs,minOverlap = 10,maxMismatch = 1,verbose=TRUE)\\n\\nsaveRDS(mergers,\\\"mergers_Run4.rds\\\")\\n\\n# Construct an amplicon sequence variant table (ASV) table\\n# If maxMismatch \\u003e 0 has been allowed in the mergePairs step,\\n# \\\"Duplicate sequences detected and merged\\\" may appear as output during the sequence table creation\\n# This is not a problem, just ignore it.\\n\\nseqtab \\u003c- makeSequenceTable(mergers)\\n\\n# How many sequence variants were inferred?\\ndim(seqtab)\\n\\n# Save sequence table\\n\\nsaveRDS(seqtab, \\\"seqtab_Run4.rds\\\")\\n\\n## Track reads throughout the pipeline ##\\n\\n# Get number of reads in files prior to cutadapt application\\n\\ninput\\u003c-countFastq(path,pattern=\\\".gz\\\") # get statistics from input files\\ninput$Sample\\u003c-rownames(input)\\ninput$Sample\\u003c-gsub(\\\"_.*\\\",\\\"\\\",input$Sample) # Remove all characters after _ (incl. _) in file names\\ninput\\u003c-aggregate(.~Sample,input,FUN=\\\"mean\\\") # Aggregate forward and reverse read files\\n\\n# Get number of reads from each step of dada2 pipeline\\n\\ngetN \\u003c- function(x) sum(getUniques(x))\\ntrack \\u003c- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers,getN))\\ncolnames(track) \\u003c- c(\\\"cutadapt\\\", \\\"filtered\\\", \\\"denoisedF\\\", \\\"denoisedR\\\", \\\"merged\\\")\\nrownames(track) \\u003c- sample.names\\n\\n# Combine with read numbers from input files\\n\\ninput\\u003c-input[order(match(input[,1],rownames(track))),]\\ntrack\\u003c-cbind(input$records,track)\\ncolnames(track)[1]\\u003c-\\\"input\\\"\\n\\n# Save to file\\n\\nwrite.table(track,\\\"track_Run4.txt\\\",sep=\\\"\\\\t\\\",col.names = NA)\\n\",\"os_name\":\"\",\"os_version\":\"\"}},\"9\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"4568121B428911EEB8630A58A9FEAC02\",\"url\":\"https://zenodo.org/record/1172783\"}}}}","data":null,"protocol_id":86559,"case_id":0,"critical_ids":"","duration":0,"original_id":1151036,"number":"2","cases":[],"critical":null},{"id":1766860,"guid":"E040D7FA3C5411EE882A0A58A9FEAC02","previous_id":1766863,"previous_guid":"E04308223C5411EE882A0A58A9FEAC02","section":"\u003cp\u003eClustering ASVs into Molecular Operational Taxonomic Units (MOTUs) using \u003cem\u003eswarm\u003c/em\u003e\u003c/p\u003e","section_color":"#EA94FF","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"7vv2t\",\"text\":\"Install the most recent version of swarm on your system. See here: https://github.com/torognes/swarm. We used swarm v3.0.0. A note on this part: downloading the installation folder of a corresponding release will provide you with a bin folder containing a swarm.exe file (at least this was the case for the Windows version). This made a more complicated installation via compilation unnecessary. According to the developers, this file is provided for convenience and can just be executed as is. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":35,\"length\":5},{\"style\":\"italic\",\"offset\":110,\"length\":5}],\"entityRanges\":[{\"key\":0,\"offset\":67,\"length\":33}],\"data\":{}},{\"key\":\"9fbcp\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"321i5\",\"text\":\"The previously generated fasta files containing the blank-corrected ASVs needs to be dereplicated (total read abundances neet to be added to the ASV headers as _XXX) to be used for clustering with swarm. Run the following script in R / RStudio to generate a file with the appropriate ASV header format:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":197,\"length\":5},{\"style\":\"italic\",\"offset\":232,\"length\":1},{\"style\":\"italic\",\"offset\":236,\"length\":8}],\"entityRanges\":[],\"data\":{}},{\"key\":\"ae7ln\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":1,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"ctq3\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7cmuj\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"6mvbg\",\"text\":\"Replace the headers in the fasta files containing the blank-corrected ASVs with the dereplicated headers. Run the following in command line:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4r5lj\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":2,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"659li\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"96t1n\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"fmh0b\",\"text\":\"Then run the swarm.exe file. Something like the following will pop up, swarm is now waiting for data input.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":71,\"length\":5}],\"entityRanges\":[],\"data\":{}},{\"key\":\"8r6sh\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":3,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"796sn\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"f5373\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3d1hj\",\"text\":\"This window should not be closed, swarm is now waiting for data input. We then ran the actual swarm algorithm from comand line. -d sets the most important parameter, the number of differences allowed between ASVs to be grouped into the same molecular operational taxonomic unit (MOTU). -i, -o, -s and -u allow us to write certain information and statistics to files specified by the corresponding file names. -w and the following specified fasta file name determine that a fasta file with the representative sequences of each MOTU is written (headers show which ASV was determined as representative and the total number of reads of the corresponding MOTU), the ultimate file name specifies the input fasta file for the clustering procedure. For COI, d = 13 was applied, while for 18S, d was set to 1 and the fastidious option was enabled (-f, only available for d = 1):\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":34,\"length\":5},{\"style\":\"italic\",\"offset\":94,\"length\":5}],\"entityRanges\":[],\"data\":{}},{\"key\":\"32eh8\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":4,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"2tsmb\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"578te\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{\"0\":{\"type\":\"link\",\"mutability\":\"\",\"data\":{\"guid\":\"58AEBFD98E5511EBBFEB0A58A9FEAC02\",\"url\":\"https://github.com/torognes/swarm\"}},\"1\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Dereplication of ASV headers in R\",\"description\":null,\"guid\":\"47C448C5341F11EE9D4302C0B41BC903\",\"name\":\"# For clustering ASVs into MOTUs using swarm, an ASV fasta with headers containing the total abundance of each is required.\\n# Here, we generate these new headers for the fasta files which will be used as input for swarm.\\n\\n### COI ###\\n\\nsetwd(\\\"~/COI\\\")\\n\\n# Read the non-contaminant COI ASV count table \\n\\nASV_counts\\u003c- read.table(file=\\\"asv_no_contaminants_COI.txt\\\",sep=\\\"\\\\t\\\",row.names=1,header=T)\\n\\n# Count total read abundances per ASV \\n\\nASV_sums\\u003c-rowSums(ASV_counts)\\n\\n# Create headers containing read counts\\n\\nseqnames\\u003c-paste0(\\\"\\u003e\\\",paste(rownames(ASV_counts),ASV_sums,sep=\\\"_\\\"))\\n\\nwrite.table(seqnames, \\\"ASV_dereplicated.txt\\\",sep=\\\"\\\\t\\\",col.names=F,row.names = F,quote=F)\\n\\n### 18S ###\\n\\nsetwd(\\\"~/18S\\\")\\n\\n# Read the non-contaminant 18S ASV count table \\n\\nASV_counts\\u003c- read.table(file=\\\"asv_no_contaminants_18S.txt\\\",sep=\\\"\\\\t\\\",row.names=1,header=T)\\n\\n# Count total read abundances per ASV \\n\\nASV_sums\\u003c-rowSums(ASV_counts)\\n\\n# Create headers containing read counts\\n\\nseqnames\\u003c-paste0(\\\"\\u003e\\\",paste(rownames(ASV_counts),ASV_sums,sep=\\\"_\\\"))\\n\\nwrite.table(seqnames, \\\"ASV_dereplicated.txt\\\",sep=\\\"\\\\t\\\",col.names=F,row.names = F,quote=F)\",\"os_name\":null,\"os_version\":null}},\"2\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Replace headers with dereplicated header names\",\"description\":null,\"guid\":\"47C4490B341F11EE9D4302C0B41BC903\",\"name\":\"# COI\\ncd ~/COI\\nawk 'NR%2==0' COI_nochim_nosingle_nopseudo_nocontam.fa | paste -d'\\\\n' ASV_dereplicated.txt - \\u003e COI_dereplicated_ASVs.fa\\n\\n# 18S\\ncd ~/18S\\nawk 'NR%2==0' 18S_nochim_nosingle_nocontam.fa | paste -d'\\\\n' ASV_dereplicated.txt - \\u003e 18S_dereplicated_ASVs.fa\",\"os_name\":null,\"os_version\":null}},\"3\":{\"type\":\"command\",\"mutability\":\"\",\"data\":{\"can_edit\":1,\"command_name\":\"Swarm\",\"description\":null,\"guid\":\"47BFFE53341F11EE9D4302C0B41BC903\",\"name\":\"Swarm 3.0.0\\nCopyright (C) 2012-2019 Torbjorn Rognes and Frederic Mahe\\nhttps://github.com/torognes/swarm\\n\\nMahe F, Rognes T, Quince C, de Vargas C, Dunthorn M (2014)\\nSwarm: robust and fast clustering method for amplicon-based studies\\nPeerJ 2:e593 https://doi.org/10.7717/peerj.593\\n\\nMahe F, Rognes T, Quince C, de Vargas C, Dunthorn M (2015)\\nSwarm v2: highly-scalable and high-resolution amplicon clustering\\nPeerJ 3:e1420 https://doi.org/10.7717/peerj.1420\\n\\nCPU features:      mmx sse sse2 sse3 ssse3 sse4.1 sse4.2 popcnt avx avx2\\nDatabase file:     -\\nOutput file:       -\\nResolution (d):    1\\nThreads:           1\\nBreak OTUs:        Yes\\nFastidious:        No\\n\\nWaiting for data... (Hit Ctrl-C and run swarm -h if you meant to read data from a file.)\",\"os_name\":null,\"os_version\":null}},\"4\":{\"type\":\"command\",\"mutability\":\"\",\"data\":{\"can_edit\":true,\"command_name\":\"swarm clustering execution\",\"description\":null,\"guid\":\"47BFFED1341F11EE9D4302C0B41BC903\",\"name\":\"# COI\\ncd ~/COI\\nswarm -d 13 -i internal.txt -o output.txt -s statistics.txt -u uclust.txt -w COI_cluster_reps.fa COI_dereplicated_ASVs.fa\\n\\n# 18S\\ncd ~/18S\\nswarm -d 1 -f -i internal.txt -o output.txt -s statistics.txt -u uclust.txt -w 18S_cluster_reps.fa 18S_dereplicated_ASVs.fa\",\"os_name\":null,\"os_version\":null}}}}","data":null,"protocol_id":86559,"case_id":0,"critical_ids":"","duration":0,"original_id":1151088,"number":"7","cases":[],"critical":null},{"id":1766861,"guid":"E0422DAF3C5411EE882A0A58A9FEAC02","previous_id":1778491,"previous_guid":"E2516226E92F42F58F15C4EC9C3F3A37","section":"\u003cp\u003eTracking ASVs and MOTUs through the pipeline and manually curate identified NIS\u003c/p\u003e","section_color":"#EA94FF","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"drji2\",\"text\":\"All occurrences of NIS were carefully curated manually based on this two-step approach:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"dcr6c\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"77o7n\",\"text\":\"a) Taxonomic classification of short amplicons from DNA metabarcoding data sets is challenging and species level assignments may lack a certain confidence for closely related taxa. All ASVs ending up in MOTUs identified as NIS were manually classified again using BOLD (COI) or NCBI's GenBank (18S, and in some rare cases COI) to assess confidence of taxonomic assignments. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":3},{\"style\":\"italic\",\"offset\":264,\"length\":4},{\"style\":\"italic\",\"offset\":278,\"length\":4},{\"style\":\"italic\",\"offset\":285,\"length\":7}],\"entityRanges\":[],\"data\":{}},{\"key\":\"blepv\",\"text\":\"b) Based on a literature search (i.e., checking sources mentioned in WRiMS for each respective species and performing additional web-based literature search) it was assessed if occurrences of MOTUs identified as NIS could actually be considered as being outside of their native range.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":2},{\"style\":\"italic\",\"offset\":69,\"length\":5}],\"entityRanges\":[],\"data\":{}},{\"key\":\"eba7d\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"6mphg\",\"text\":\"As a first step, the fate of ASVs (i.e., which MOTU they ultimately ended up in) was tracked throughout the entire pipeline using the previously generated files resulting from swarm clustering, LULU curation, merging of same-species MOTUs and mapping of MOTUs to their representative ASVs. Subsequently, this information was subset to ASVs belonging to NIS MOTUs to obtain their respective sequences and read counts per ARMS sampling event. See below for the script executed in R / RStudio:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":175,\"length\":6},{\"style\":\"italic\",\"offset\":194,\"length\":4},{\"style\":\"italic\",\"offset\":478,\"length\":11}],\"entityRanges\":[],\"data\":{}},{\"key\":\"fv2lt\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":0,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"3n2es\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"f72bk\",\"text\":\"The output of this step are tables (NIS_MOTU_ASV_counts.txt; one each for COI and 18S) containing info on MOTUs identified as NIS, their species assignment, the ASVs they contain plus the respective ASV read counts for each ARMS sampling event. We only considered ASV occurrences with at least 5 reads per sample (other occurrences were set to zero and ASVs now left with zero occurrences only were discarded). In addition, the sequence headers of those ASVs were written to file, as well as a mapping file to replace the ASV headers with \\\"MOTUyz_ASVxy\\\". Using the commands below in Git BASH, the initial fasta files (dada2 output) were subset to the ASVs found in NIS MOTUs and the sequence headers were replaced with the aforementioned \\\"MOTUyz_ASVxy\\\" pattern with SeqKit:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":36,\"length\":23},{\"style\":\"italic\",\"offset\":583,\"length\":8},{\"style\":\"italic\",\"offset\":618,\"length\":5},{\"style\":\"italic\",\"offset\":766,\"length\":6}],\"entityRanges\":[],\"data\":{}},{\"key\":\"5qmln\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":1,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"2ctvu\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"6gutm\",\"text\":\"We re-did the previous steps briefly to check if more NIS would be detected without the minimum threshold of 5 reads per ASV and sample. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"av91f\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":2,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"b4j7u\",\"text\":\"We then also generated fasta files again for these two data sets (COI and 18S) as described above to check taxonomy of these sequences (see below). However, these two data sets without any minimum read threshold for NIS ASVs were not further consiered in the analysis. The steps detailed below were performed for the data sets excluding ASV occurrences of less than 5 reads.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"92abt\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"bm1k2\",\"text\":\"All COI sequences were then classified again manually using BOLD's Identification Engine with the Public Record Barcode Database (https://v3.boldsystems.org/index.php/IDS_OpenIdEngine). This was done in batches, as classification of a maximum of 50 sequences can be performed at a time. Where there was no match in BOLD for COI, sequences were manually classified using the NCBI blastn suite (https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastn\\u0026PAGE_TYPE=BlastSearch\\u0026LINK_LOC=blasthome) with default settings. The sequences of the 18S NIS data set were also manually classified again using the NCBI blastn suite.  If there was no hit for 18S sequences on NCBI with a query cover of 100% and similarity \\u003e98%, ASVs were manually queried against PR2 using its web service (https://app.pr2-database.org/pr2-database/, go to Query on the top of the page). BOLD gives information on the confidence of a species level assignment for each sequence and notes if an assignment has a high confidence, a sequence may belong to a particular set of taxa or what the nearest match is. NCBI's web service states similarity, query cover and E value for assignments. PR2 states percentage identity, mismatches and gaps of query alignments.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":60,\"length\":4},{\"style\":\"italic\",\"offset\":98,\"length\":30},{\"style\":\"italic\",\"offset\":315,\"length\":5},{\"style\":\"italic\",\"offset\":374,\"length\":11},{\"style\":\"italic\",\"offset\":598,\"length\":11},{\"style\":\"italic\",\"offset\":659,\"length\":4},{\"style\":\"italic\",\"offset\":747,\"length\":3},{\"style\":\"italic\",\"offset\":824,\"length\":5},{\"style\":\"italic\",\"offset\":855,\"length\":4},{\"style\":\"italic\",\"offset\":1074,\"length\":4},{\"style\":\"italic\",\"offset\":1153,\"length\":3},{\"style\":\"bold\",\"offset\":98,\"length\":30}],\"entityRanges\":[{\"key\":3,\"offset\":130,\"length\":53},{\"key\":4,\"offset\":393,\"length\":96},{\"key\":5,\"offset\":774,\"length\":42}],\"data\":{}},{\"key\":\"3uv3u\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"fgaqm\",\"text\":\"A literature search based on the sources stated in WRiMS / WoRMS for each respective taxon and further scientific and public literature was performed to assess if the occurrences at a given location in the data set may be considered outside of the respective native range. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":51,\"length\":14},{\"style\":\"italic\",\"offset\":102,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"kib8\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3nh0c\",\"text\":\"In Excel, the NIS_MOTU_ASV_counts_COI.txt and NIS_MOTU_ASV_counts_18S.txt files were then curated manually. This curation was done as follows:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":3,\"length\":5},{\"style\":\"italic\",\"offset\":14,\"length\":27},{\"style\":\"italic\",\"offset\":46,\"length\":27}],\"entityRanges\":[],\"data\":{}},{\"key\":\"7n6s2\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"710vf\",\"text\":\"COI ASVs or their occurrences at certain locations were removed if...\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"9modo\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"fg5q7\",\"text\":\"...there was no match in BOLD and where for these ASVs the classification with NCBI's blastn had no hits with a query cover of 100% and similarity \\u003e98% for the MOTU's species assignment\",\"type\":\"unordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":25,\"length\":4},{\"style\":\"italic\",\"offset\":79,\"length\":13}],\"entityRanges\":[],\"data\":{}},{\"key\":\"4i3vr\",\"text\":\"...a species level match could not be made according to BOLD and one of the likely species assignments given by BOLD was potentially a native species\",\"type\":\"unordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":56,\"length\":4},{\"style\":\"italic\",\"offset\":112,\"length\":4}],\"entityRanges\":[],\"data\":{}},{\"key\":\"9lg6r\",\"text\":\"...the presence of the respective taxon was unlikely at the given location (e.g. occurrences in the Baltic Sea of species requiring a higher salinity than present there, etc.)\",\"type\":\"unordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"2da2b\",\"text\":\"...this ASV and all other ASVs of this MOTU only occurred at locations where the respective species is native. This means the curated data set still contained occurrences of MOTUs where they are native, as long as they occurred at least at one location where they are not native\",\"type\":\"unordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"bqfp4\",\"text\":\"...there was no clear indication based on literature search that a taxon is actually considered as a putative NIS at any of the locations of occurrence\",\"type\":\"unordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"581cc\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"c7n07\",\"text\":\"18S ASVs or their occurrences at certain locations were removed if...\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"b6cg2\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"6vs3u\",\"text\":\"...the highest valued match (based on E value) in NCBI's blastn had no hits with a query cover of 100% and similarity \\u003e98% for the MOTU's species assignment and there was no top hit for this assignment in PR2 with similarity \\u003e98%\",\"type\":\"unordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":50,\"length\":13},{\"style\":\"italic\",\"offset\":205,\"length\":4},{\"style\":\"italic\",\"offset\":213,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"6spp9\",\"text\":\"...the top hits in NCBI's blastn with a query cover of 100% and similarity \\u003e98% with the highest E value represented several taxa and based on its geographic occurrence and the scientific literature it could not be determined if the sequence likely belonged to the respective NIS\",\"type\":\"unordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":19,\"length\":13}],\"entityRanges\":[],\"data\":{}},{\"key\":\"djq08\",\"text\":\"...the presence of the respective taxon was unlikely at the given location (e.g. occurrences in the Baltic Sea of species requiring a higher salinity than present there, etc.)\",\"type\":\"unordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3pifm\",\"text\":\"...this ASV and all other ASVs of this MOTU only occurred at locations where the respective species is native. This means the curated data set still contained occurrences of MOTUs where they are native, as long as they occurred at least at one location where they are not native\",\"type\":\"unordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"9cre4\",\"text\":\"...there was no clear indication based on literature search that a taxon is actually considered as a putative NIS at any of the locations of occurrence\",\"type\":\"unordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"b2601\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3e55l\",\"text\":\"In some cases, the appropriate taxonomic assignment for certain ASVs and/or MOTUs was manually set as follows:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"6p0ou\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"615pp\",\"text\":\"Some ASVs had multiple likely assignments, but based on literature and known distribution, the correct assignment could be inferred and was decided on. \",\"type\":\"unordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8v6d6\",\"text\":\"Where all ASVs of the same MOTU had multiple likely species assignments, and all of these where putative NIS and belonged to the same genus, we adjusted the taxonomy of this MOTU to the genus level (i.e., genus X sp.).\",\"type\":\"unordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"624qq\",\"text\":\"Where some ASVs within the same MOTU had a clear species classification belonging to a NIS, while some ASVs in this MOTU had several likely species assignments and it could not be established based on literature what the likely assignment was, the latter were removed to not have a mix of species and genus level assignments within the same MOTU.\",\"type\":\"unordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"5ls8c\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"747fv\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"ap38v\",\"text\":\"The curated data sets were saved as NIS_MOTU_ASV_counts_COI_curated.txt and NIS_MOTU_ASV_counts_18S_curated.txt, respectively. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":36,\"length\":36},{\"style\":\"italic\",\"offset\":75,\"length\":36},{\"style\":\"italic\",\"offset\":125,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"4ce23\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"bjsfs\",\"text\":\"These files are processed in R / RStudio to aggregate all remaining MOTU/ASVs and occurrences for each MOTU per ARMS samplig event to create a final NIS presence-absence table for each marker gene. These tables of each marker gene are ultimately merged to obtain a final data set of NIS presence-absence per ARMS sampling event. A second table is created linking ARMS sampling events to the respective coordinates of ARMS. The coordinates for each ARMS unit were obtained from the respective metadata file provided on the ARMS-MBON GitHub page (see above) are can be found in the file below (this file contains coordinates for all ARMS units deployed by ARMS-MBON members to date, and therefore also for ARMS units not part of this present data set): \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":29,\"length\":11}],\"entityRanges\":[],\"data\":{}},{\"key\":\"fm707\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"6vt78\",\"text\":\"  \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":6,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"fgm30\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3ci0u\",\"text\":\"The final steps were run in R / RStudio, see below:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":28,\"length\":11}],\"entityRanges\":[],\"data\":{}},{\"key\":\"64tqd\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":7,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"2hqkq\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"9eg2s\",\"text\":\"For further downstream processing, the NIS_MOTU_ASV_counts_COI_curated.txt and NIS_MOTU_ASV_counts_18S_curated.txt files were additionally manually filtered in Excel. These files still contain occurrences of species where they are native, as long as they occurred at least at one location where they are not native. Two new files were generated containing ONLY NIS occurrences at locations where they could be considered as NIS. All other occurrences were set to zero. These new files are called NIS_MOTU_ASV_counts_COI_curated_filtered.txt and NIS_MOTU_ASV_counts_18S_curated_filtered.txt, respectively. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":39,\"length\":36},{\"style\":\"italic\",\"offset\":78,\"length\":37},{\"style\":\"italic\",\"offset\":160,\"length\":5},{\"style\":\"italic\",\"offset\":356,\"length\":4},{\"style\":\"italic\",\"offset\":496,\"length\":45},{\"style\":\"italic\",\"offset\":544,\"length\":45},{\"style\":\"italic\",\"offset\":603,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"2u9la\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{\"0\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Track fate of ASVs throughout the pipeline in R\",\"description\":\"\",\"guid\":\"6D48AE99677B11EE836F0A58A9FEAC02\",\"name\":\"library(phyloseq)\\nlibrary(dplyr)\\nlibrary(tidyr)\\nlibrary(data.table)\\n\\n### COI ###\\n\\nsetwd(\\\"~/COI\\\")\\n\\n## Create final mapping file with all ASVs that have been placed in one MOTU throughout the pipeline ##\\n\\n# read LULU and swarm output files and the mapping file of merging MOTUs with same species assignment \\n\\nlulu\\u003c-read.table(\\\"motu_map_lulu_COI.txt\\\",sep=\\\"\\\\t\\\",header=T)\\nswarm\\u003c-read.table(\\\"output.txt\\\",sep=\\\"\\\\t\\\",stringsAsFactors = F) \\nspec_merge\\u003c-read.table(\\\"motu_map_identical_species.txt\\\",sep=\\\"\\\\t\\\",header=T)\\n\\n# Separate swarm table into columns by space\\n# Remove read count strings from ASV names (lapply and function necessary to do this for every entry in the table, not just specific columns)\\n\\nswarm\\u003c-separate_wider_delim(swarm,V1, delim = \\\" \\\", names_sep=\\\"\\\",too_few = \\\"align_start\\\")\\nswarm[] \\u003c- lapply(swarm, function(y) gsub(\\\"_.*\\\",\\\"\\\", y))\\n\\n# Order entries of first column in swarm table based on first column in lulu table\\n\\nswarm\\u003c-swarm[order(match(swarm$V11,lulu$X)),]\\n\\n# Merge swarm and lulu tables\\n\\nswarm_lulu_map\\u003c-cbind(lulu[,4],swarm)\\ncolnames(swarm_lulu_map)[1]\\u003c-\\\"MOTU\\\"\\n\\n# Separate MOTU strings in spec_merge into separate columns\\n\\nspec_merge\\u003c-separate_wider_delim(spec_merge,MOTU, delim = \\\",\\\", names_sep=\\\"\\\",too_few = \\\"align_start\\\")\\n\\n# Create ID column in spec_merge (= MOTU other MOTUs have been merged onto) and transform to long format\\n\\nspec_merge\\u003c-cbind(spec_merge[,c(2,2:ncol(spec_merge))])\\ncolnames(spec_merge)[1]\\u003c-\\\"ID\\\"\\n\\nspec_merge_long \\u003c- melt(setDT(spec_merge), id.vars = \\\"ID\\\", variable.name = \\\"string\\\")\\nspec_merge_long\\u003c-spec_merge_long[,-2]\\ncolnames(spec_merge_long)[2]\\u003c-\\\"MOTU\\\"\\nspec_merge_long\\u003c-spec_merge_long[!is.na(spec_merge_long$MOTU),]\\n\\n## Merge swarm_lulu_map and spec_merge_long\\n\\nswarm_lulu_spec_merge_map\\u003c-as.data.frame(merge(swarm_lulu_map, spec_merge_long, by = \\\"MOTU\\\", all = TRUE))\\nswarm_lulu_spec_merge_map\\u003c-swarm_lulu_spec_merge_map %\\u003e% relocate(ID)\\n\\n# Where ID is NA, fill in with entry of MOTU column. Then, remove MOTU column.\\n\\nswarm_lulu_spec_merge_map$ID\\u003c-ifelse(is.na(swarm_lulu_spec_merge_map$ID),swarm_lulu_spec_merge_map$MOTU,swarm_lulu_spec_merge_map$ID)\\nswarm_lulu_spec_merge_map\\u003c-swarm_lulu_spec_merge_map[,-2]\\n\\n# Write all columns except ID column into one string, MOTU names separated by comma\\n\\nswarm_lulu_spec_merge_map$MOTU_string\\u003c-apply(swarm_lulu_spec_merge_map[,2:ncol(swarm_lulu_spec_merge_map)], 1,paste, collapse=\\\",\\\") \\n\\n# Remove ,NA strings (occurred during the previous step when empty columns were pasted together)\\n# Keep only ID column and the MOTU_string column\\n\\nswarm_lulu_spec_merge_map$MOTU_string\\u003c-gsub(\\\",NA.*\\\",\\\"\\\", swarm_lulu_spec_merge_map$MOTU_string)\\nswarm_lulu_spec_merge_map\\u003c-swarm_lulu_spec_merge_map[,c(1,ncol(swarm_lulu_spec_merge_map))]\\n\\n# Aggregate rows based on ID column\\n\\nswarm_lulu_spec_merge_map\\u003c-aggregate(.~ ID, data = swarm_lulu_spec_merge_map, paste, collapse = \\\",\\\")\\n\\n# Map ASVs to MOTUs \\n\\n# motu_asv_mapping.txt stems from initial phyloseq processing performed previously \\nmapping\\u003c-read.table(\\\"motu_asv_mapping.txt\\\",sep=\\\"\\\\t\\\",header=T)\\nmapping\\u003c-mapping[order(match(mapping[,1],swarm_lulu_spec_merge_map[,1])),]\\n\\nswarm_lulu_spec_merge_map\\u003c-cbind(mapping[,2],swarm_lulu_spec_merge_map)\\ncolnames(swarm_lulu_spec_merge_map)[1:2]\\u003c-c(\\\"MOTU\\\",\\\"ASV_representative\\\")\\nswarm_lulu_spec_merge_map\\u003c-separate_wider_delim(swarm_lulu_spec_merge_map,MOTU_string, delim = \\\",\\\", names_sep=\\\"\\\",too_few = \\\"align_start\\\")\\n\\n# Subset to MOTUs previously identified as NIS\\n\\nunfiltered_nis_arms\\u003c-readRDS(\\\"unfiltered_nis_arms.rds\\\")# Saved in previous script when filtering MOTUs for NIS\\nnis_asv_motu_map\\u003c-swarm_lulu_spec_merge_map %\\u003e% filter(MOTU %in% taxa_names(unfiltered_nis_arms))\\n\\n# Create table mapping ASVs to those MOTUs and get respective ASV read counts\\n\\nnis_asv_motu_map \\u003c- melt(setDT(nis_asv_motu_map[,-2]), id.vars = \\\"MOTU\\\", variable.name = \\\"string\\\")\\nnis_asv_motu_map\\u003c-nis_asv_motu_map[,-2]\\ncolnames(nis_asv_motu_map)[2]\\u003c-\\\"ASV\\\"\\nnis_asv_motu_map\\u003c-nis_asv_motu_map[!is.na(nis_asv_motu_map$ASV),]\\nasv_counts\\u003c-read.table(\\\"asv_no_contaminants_COI.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1) # read blank-corrected ASV count table\\nasv_counts\\u003c-asv_counts[rownames(asv_counts) %in% nis_asv_motu_map$ASV,]\\n\\n# Subset ASV counts to the samples present in NIS data set, set ASV occurrences below 5 to zero and merge samples per ARMS sampling event\\n\\nunfiltered_nis\\u003c-readRDS(\\\"unfiltered_nis.rds\\\") # Saved in previous script when filtering MOTUs for NIS\\nnis_motu_physeq\\u003c-subset_samples(unfiltered_nis,sample_event %in% sample_names(unfiltered_nis_arms)) # get sample names of NIS data set with MOTUs showing at least 10 reads per ARMS sampling event\\nasv_counts\\u003c-asv_counts[colnames(asv_counts)%in%sample_names(nis_motu_physeq)] # Subset respective ASV count table to these samples\\nasv_counts[asv_counts \\u003c 5] \\u003c- 0\\nasv_counts\\u003c-asv_counts[rowSums(asv_counts[])\\u003e0,]\\nasv_counts\\u003c-asv_counts[, colSums(asv_counts != 0) \\u003e 0]\\nnis_asv_physeq\\u003c-phyloseq(otu_table(asv_counts,taxa_are_rows = TRUE), sample_data(sample_data(nis_motu_physeq))) # make new phyloseq object with ASV data set to merge samples per ARMS sampling event\\nnis_asv_physeq\\u003c-merge_samples(nis_asv_physeq,\\\"sample_event\\\") # ATTENTION: this transposes otu_table\\nasv_nis_counts\\u003c-as.data.frame(t(otu_table(nis_asv_physeq)))\\n\\n# Generate final table of NIS MOTUs, species names, the ASVs they contain and the ASV read counts per ARMS sampling event\\n\\nnis_asv_motu_map\\u003c-nis_asv_motu_map %\\u003e% filter(ASV %in% rownames(asv_nis_counts))\\nnis_asv_motu_map\\u003c-nis_asv_motu_map[order(match(nis_asv_motu_map$ASV,rownames(asv_nis_counts))),]\\nasv_motu_counts\\u003c-cbind(nis_asv_motu_map,asv_nis_counts)\\nasv_motu_counts\\u003c-asv_motu_counts[order(asv_motu_counts$MOTU),]\\nnis_taxa_counts\\u003c-read.table(\\\"nis_taxa_counts_COI.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1) # Read the nis_taxa_count table created in the previous NIS script to get taxonomy information\\n# add species names for MOTUs\\nfor (i in 1:nrow(nis_taxa_counts)) { \\n  asv_motu_counts$Species[asv_motu_counts$MOTU == rownames(nis_taxa_counts)[i]] \\u003c- nis_taxa_counts$Species[i]\\n} \\nasv_motu_counts\\u003c-asv_motu_counts %\\u003e% relocate(Species, .after=MOTU)\\n\\nwrite.table(asv_motu_counts,\\\"NIS_MOTU_ASV_counts_COI.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\n## Write sequence headers to file to subset the fasta file to the NIS sequences outside of R\\n\\n# Write headers with ASV names, as fasta file still has sequences named by ASV names of representative sequences\\nnis_headers_asv\\u003c-paste0(\\\"\\u003e\\\",asv_motu_counts$ASV)\\nwrite.table(nis_headers_asv,\\\"nis_headers_asv.txt\\\",sep=\\\"\\\\t\\\",row.names = F,quote=F,col.names = F)\\n\\n# Write mapping table of ASV to MOTU names to replace the ASV names in the fasta file with seqkit outside of R\\n\\nnew_headers\\u003c-cbind(asv_motu_counts$ASV,paste(asv_motu_counts$MOTU,asv_motu_counts$ASV,sep=\\\"_\\\"))\\nwrite.table(new_headers,\\\"nis_headers_motu.txt\\\",sep=\\\"\\\\t\\\",row.names = F,quote=F,col.names = F)\\n\\n## Outisde of R, manually classify sequences again to check for confidence of assignments. In addition, check if occurences at respective location can actually be considered outside of native range.\\n\\n### 18S ###\\n\\nsetwd(\\\"~/18S\\\")\\n\\n## Create final mapping file with all ASVs that have been placed in one MOTU throughout the pipeline ##\\n\\n# read LULU and swarm output files and the mapping file of merging MOTUs with same species assignment \\n\\nlulu\\u003c-read.table(\\\"motu_map_lulu_18S.txt\\\",sep=\\\"\\\\t\\\",header=T)\\nswarm\\u003c-read.table(\\\"output.txt\\\",sep=\\\"\\\\t\\\",stringsAsFactors = F) \\nspec_merge\\u003c-read.table(\\\"motu_map_identical_species.txt\\\",sep=\\\"\\\\t\\\",header=T)\\n\\n# Separate swarm table into columns by space\\n# Remove read count strings from ASV names (lapply and function necessary to do this for every entry in the table, not just specific columns)\\n\\nswarm\\u003c-separate_wider_delim(swarm,V1, delim = \\\" \\\", names_sep=\\\"\\\",too_few = \\\"align_start\\\")\\nswarm[] \\u003c- lapply(swarm, function(y) gsub(\\\"_.*\\\",\\\"\\\", y))\\n\\n# Order entries of first column in swarm table based on first column in lulu table\\n\\nswarm\\u003c-swarm[order(match(swarm$V11,lulu$X)),]\\n\\n# Merge swarm and lulu tables\\n\\nswarm_lulu_map\\u003c-cbind(lulu[,4],swarm)\\ncolnames(swarm_lulu_map)[1]\\u003c-\\\"MOTU\\\"\\n\\n# Separate MOTU strings in spec_merge into separate columns\\n\\nspec_merge\\u003c-separate_wider_delim(spec_merge,MOTU, delim = \\\",\\\", names_sep=\\\"\\\",too_few = \\\"align_start\\\")\\n\\n# Create ID column in spec_merge (= MOTU other MOTUs have been merged onto) and transform to long format\\n\\nspec_merge\\u003c-cbind(spec_merge[,c(2,2:ncol(spec_merge))])\\ncolnames(spec_merge)[1]\\u003c-\\\"ID\\\"\\n\\nspec_merge_long \\u003c- melt(setDT(spec_merge), id.vars = \\\"ID\\\", variable.name = \\\"string\\\")\\nspec_merge_long\\u003c-spec_merge_long[,-2]\\ncolnames(spec_merge_long)[2]\\u003c-\\\"MOTU\\\"\\nspec_merge_long\\u003c-spec_merge_long[!is.na(spec_merge_long$MOTU),]\\n\\n## Merge swarm_lulu_map and spec_merge_long\\n\\nswarm_lulu_spec_merge_map\\u003c-as.data.frame(merge(swarm_lulu_map, spec_merge_long, by = \\\"MOTU\\\", all = TRUE))\\nswarm_lulu_spec_merge_map\\u003c-swarm_lulu_spec_merge_map %\\u003e% relocate(ID)\\n\\n# Where ID is NA, fill in with entry of MOTU column. Then, remove MOTU column.\\n\\nswarm_lulu_spec_merge_map$ID\\u003c-ifelse(is.na(swarm_lulu_spec_merge_map$ID),swarm_lulu_spec_merge_map$MOTU,swarm_lulu_spec_merge_map$ID)\\nswarm_lulu_spec_merge_map\\u003c-swarm_lulu_spec_merge_map[,-2]\\n\\n# Write all columns except ID column into one string, MOTU names separated by comma\\n\\nswarm_lulu_spec_merge_map$MOTU_string\\u003c-apply(swarm_lulu_spec_merge_map[,2:ncol(swarm_lulu_spec_merge_map)], 1,paste, collapse=\\\",\\\") \\n\\n# Remove ,NA strings (occurred during the previous step when empty columns were pasted together)\\n# Keep only ID column and the MOTU_string column\\n\\nswarm_lulu_spec_merge_map$MOTU_string\\u003c-gsub(\\\",NA.*\\\",\\\"\\\", swarm_lulu_spec_merge_map$MOTU_string)\\nswarm_lulu_spec_merge_map\\u003c-swarm_lulu_spec_merge_map[,c(1,ncol(swarm_lulu_spec_merge_map))]\\n\\n# Aggregate rows based on ID column\\n\\nswarm_lulu_spec_merge_map\\u003c-aggregate(.~ ID, data = swarm_lulu_spec_merge_map, paste, collapse = \\\",\\\")\\n\\n# Map ASVs to MOTUs \\n\\n# motu_asv_mapping.txt stems from initial phyloseq processing performed previously \\nmapping\\u003c-read.table(\\\"motu_asv_mapping.txt\\\",sep=\\\"\\\\t\\\",header=T)\\nmapping\\u003c-mapping[order(match(mapping[,1],swarm_lulu_spec_merge_map[,1])),]\\n\\nswarm_lulu_spec_merge_map\\u003c-cbind(mapping[,2],swarm_lulu_spec_merge_map)\\ncolnames(swarm_lulu_spec_merge_map)[1:2]\\u003c-c(\\\"MOTU\\\",\\\"ASV_representative\\\")\\nswarm_lulu_spec_merge_map\\u003c-separate_wider_delim(swarm_lulu_spec_merge_map,MOTU_string, delim = \\\",\\\", names_sep=\\\"\\\",too_few = \\\"align_start\\\")\\n\\n# Subset to MOTUs previously identified as NIS\\n\\nunfiltered_nis_arms\\u003c-readRDS(\\\"unfiltered_nis_arms.rds\\\")# Saved in previous script when filtering MOTUs for NIS\\nnis_asv_motu_map\\u003c-swarm_lulu_spec_merge_map %\\u003e% filter(MOTU %in% taxa_names(unfiltered_nis_arms))\\n\\n# Create table mapping ASVs to those MOTUs and get respective ASV read counts\\n\\nnis_asv_motu_map \\u003c- melt(setDT(nis_asv_motu_map[,-2]), id.vars = \\\"MOTU\\\", variable.name = \\\"string\\\")\\nnis_asv_motu_map\\u003c-nis_asv_motu_map[,-2]\\ncolnames(nis_asv_motu_map)[2]\\u003c-\\\"ASV\\\"\\nnis_asv_motu_map\\u003c-nis_asv_motu_map[!is.na(nis_asv_motu_map$ASV),]\\nasv_counts\\u003c-read.table(\\\"asv_no_contaminants_18S.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1) # read blank-corrected ASV count table\\nasv_counts\\u003c-asv_counts[rownames(asv_counts) %in% nis_asv_motu_map$ASV,]\\n\\n# Subset ASV counts to the samples present in NIS data set, set ASV occurrences below 5 to zero and merge samples per ARMS sampling event\\n\\nunfiltered_nis\\u003c-readRDS(\\\"unfiltered_nis.rds\\\") # Saved in previous script when filtering MOTUs for NIS\\nnis_motu_physeq\\u003c-subset_samples(unfiltered_nis,sample_event %in% sample_names(unfiltered_nis_arms)) # get sample names of NIS data set with MOTUs showing at least 10 reads per ARMS sampling event\\nasv_counts\\u003c-asv_counts[colnames(asv_counts)%in%sample_names(nis_motu_physeq)] # Subset respective ASV count table to these samples\\nasv_counts[asv_counts \\u003c 5] \\u003c- 0\\nasv_counts\\u003c-asv_counts[rowSums(asv_counts[])\\u003e0,]\\nasv_counts\\u003c-asv_counts[, colSums(asv_counts != 0) \\u003e 0]\\nnis_asv_physeq\\u003c-phyloseq(otu_table(asv_counts,taxa_are_rows = TRUE), sample_data(sample_data(nis_motu_physeq))) # make new phyloseq object with ASV data set to merge samples per ARMS sampling event\\nnis_asv_physeq\\u003c-merge_samples(nis_asv_physeq,\\\"sample_event\\\") # ATTENTION: this transposes otu_table\\nasv_nis_counts\\u003c-as.data.frame(t(otu_table(nis_asv_physeq)))\\n\\n# Generate final table of NIS MOTUs, species names, the ASVs they contain and the ASV read counts per ARMS sampling event\\n\\nnis_asv_motu_map\\u003c-nis_asv_motu_map %\\u003e% filter(ASV %in% rownames(asv_nis_counts))\\nnis_asv_motu_map\\u003c-nis_asv_motu_map[order(match(nis_asv_motu_map$ASV,rownames(asv_nis_counts))),]\\nasv_motu_counts\\u003c-cbind(nis_asv_motu_map,asv_nis_counts)\\nasv_motu_counts\\u003c-asv_motu_counts[order(asv_motu_counts$MOTU),]\\nnis_taxa_counts\\u003c-read.table(\\\"nis_taxa_counts_18S.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1) # Read the nis_taxa_count table created in the previous NIS script to get taxonomy information\\n# add species names for MOTUs\\nfor (i in 1:nrow(nis_taxa_counts)) { \\n  asv_motu_counts$Species[asv_motu_counts$MOTU == rownames(nis_taxa_counts)[i]] \\u003c- nis_taxa_counts$Species[i]\\n} \\nasv_motu_counts\\u003c-asv_motu_counts %\\u003e% relocate(Species, .after=MOTU)\\n\\nwrite.table(asv_motu_counts,\\\"NIS_MOTU_ASV_counts_18S.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\n## Write sequence headers to file to subset the fasta file to the NIS sequences outside of R\\n\\n# Write headers with ASV names, as fasta file still has sequences named by ASV names of representative sequences\\nnis_headers_asv\\u003c-paste0(\\\"\\u003e\\\",asv_motu_counts$ASV)\\nwrite.table(nis_headers_asv,\\\"nis_headers_asv.txt\\\",sep=\\\"\\\\t\\\",row.names = F,quote=F,col.names = F)\\n\\n# Write mapping table of ASV to MOTU names to replace the ASV names in the fasta file with seqkit outside of R\\n\\nnew_headers\\u003c-cbind(asv_motu_counts$ASV,paste(asv_motu_counts$MOTU,asv_motu_counts$ASV,sep=\\\"_\\\"))\\nwrite.table(new_headers,\\\"nis_headers_motu.txt\\\",sep=\\\"\\\\t\\\",row.names = F,quote=F,col.names = F)\\n\\n## Outisde of R, manually classify sequences again to check for confidence of assignments. In addition, check if occurences at respective location can actually be considered outside of native range.\\n\",\"os_name\":\"\",\"os_version\":\"\"}},\"1\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Make fasta of ASVs found in NIS MOTUs\",\"description\":null,\"guid\":\"E96046BA4CA311EE83BB0A58A9FEAC02\",\"name\":\"## COI ##\\n\\ncd ~/COI\\n\\n# Subset ASV sequences found in NIS MOTUs\\ngrep -w -A 1 -f nis_headers_asv.txt COI_nochim_nosingle_ASVs.fa \\u003e COI_ASV_NIS.fa --no-group-separator\\n\\n# Replace ASV names with MOTU_ASV names\\nseqkit replace -p \\\"^(\\\\S+)\\\" --replacement \\\"{kv}\\\" --kv-file nis_headers_motu.txt COI_ASV_NIS.fa --keep-key \\u003e COI_NIS_MOTU.fa\\n\\n# Sort fasta file to have all ASVs found in the same MOTU as consecutive sequences\\n\\nseqkit sort COI_NIS_MOTU.fa -o COI_NIS_MOTU_ASV.fa\\n\\n# Seqkit creates multiple-line sequences. Change back to single line\\nawk '/^\\u003e/ { print (NR==1 ? \\\"\\\" : RS) $0; next } { printf \\\"%s\\\", $0 } END { printf RS }' COI_NIS_MOTU_ASV.fa \\u003e tmp \\u0026\\u0026 mv tmp COI_NIS_MOTU_ASV_final.fa\\n\\n## 18S ##\\n\\ncd ~/18S\\n\\n# Subset ASV sequences found in NIS MOTUs\\ngrep -w -A 1 -f nis_headers_asv.txt 18S_nochim_nosingle_ASVs.fa \\u003e 18S_ASV_NIS.fa --no-group-separator\\n\\n# Replace ASV names with MOTU_ASV names\\nseqkit replace -p \\\"^(\\\\S+)\\\" --replacement \\\"{kv}\\\" --kv-file nis_headers_motu.txt 18S_ASV_NIS.fa --keep-key \\u003e 18S_NIS_MOTU.fa\\n\\n# Sort fasta file to have all ASVs found in the same MOTU as consecutive sequences\\n\\nseqkit sort 18S_NIS_MOTU.fa -o 18S_NIS_MOTU_ASV.fa\\n\\n# Seqkit creates multiple-line sequences. Change back to single line\\nawk '/^\\u003e/ { print (NR==1 ? \\\"\\\" : RS) $0; next } { printf \\\"%s\\\", $0 } END { printf RS }' 18S_NIS_MOTU_ASV.fa \\u003e tmp \\u0026\\u0026 mv tmp 18S_NIS_MOTU_ASV_final.fa\",\"os_name\":\"\",\"os_version\":\"\"}},\"2\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Track fate of ASVs throughout the pipeline without ASV minimum read threshold in R\",\"description\":null,\"guid\":\"6D48AE99677B11EE836F0A58A9FEAC02\",\"name\":\"library(phyloseq)\\nlibrary(dplyr)\\nlibrary(tidyr)\\nlibrary(data.table)\\n\\n### COI ###\\n\\nsetwd(\\\"~/COI\\\")\\n\\n## Create final mapping file with all ASVs that have been placed in one MOTU throughout the pipeline ##\\n\\n# read LULU and swarm output files and the mapping file of merging MOTUs with same species assignment \\n\\nlulu\\u003c-read.table(\\\"motu_map_lulu_COI.txt\\\",sep=\\\"\\\\t\\\",header=T)\\nswarm\\u003c-read.table(\\\"output.txt\\\",sep=\\\"\\\\t\\\",stringsAsFactors = F) \\nspec_merge\\u003c-read.table(\\\"motu_map_identical_species.txt\\\",sep=\\\"\\\\t\\\",header=T)\\n\\n# Separate swarm table into columns by space\\n# Remove read count strings from ASV names (lapply and function necessary to do this for every entry in the table, not just specific columns)\\n\\nswarm\\u003c-separate_wider_delim(swarm,V1, delim = \\\" \\\", names_sep=\\\"\\\",too_few = \\\"align_start\\\")\\nswarm[] \\u003c- lapply(swarm, function(y) gsub(\\\"_.*\\\",\\\"\\\", y))\\n\\n# Order entries of first column in swarm table based on first column in lulu table\\n\\nswarm\\u003c-swarm[order(match(swarm$V11,lulu$X)),]\\n\\n# Merge swarm and lulu tables\\n\\nswarm_lulu_map\\u003c-cbind(lulu[,4],swarm)\\ncolnames(swarm_lulu_map)[1]\\u003c-\\\"MOTU\\\"\\n\\n# Separate MOTU strings in spec_merge into separate columns\\n\\nspec_merge\\u003c-separate_wider_delim(spec_merge,MOTU, delim = \\\",\\\", names_sep=\\\"\\\",too_few = \\\"align_start\\\")\\n\\n# Create ID column in spec_merge (= MOTU other MOTUs have been merged onto) and transform to long format\\n\\nspec_merge\\u003c-cbind(spec_merge[,c(2,2:ncol(spec_merge))])\\ncolnames(spec_merge)[1]\\u003c-\\\"ID\\\"\\n\\nspec_merge_long \\u003c- melt(setDT(spec_merge), id.vars = \\\"ID\\\", variable.name = \\\"string\\\")\\nspec_merge_long\\u003c-spec_merge_long[,-2]\\ncolnames(spec_merge_long)[2]\\u003c-\\\"MOTU\\\"\\nspec_merge_long\\u003c-spec_merge_long[!is.na(spec_merge_long$MOTU),]\\n\\n## Merge swarm_lulu_map and spec_merge_long\\n\\nswarm_lulu_spec_merge_map\\u003c-as.data.frame(merge(swarm_lulu_map, spec_merge_long, by = \\\"MOTU\\\", all = TRUE))\\nswarm_lulu_spec_merge_map\\u003c-swarm_lulu_spec_merge_map %\\u003e% relocate(ID)\\n\\n# Where ID is NA, fill in with entry of MOTU column. Then, remove MOTU column.\\n\\nswarm_lulu_spec_merge_map$ID\\u003c-ifelse(is.na(swarm_lulu_spec_merge_map$ID),swarm_lulu_spec_merge_map$MOTU,swarm_lulu_spec_merge_map$ID)\\nswarm_lulu_spec_merge_map\\u003c-swarm_lulu_spec_merge_map[,-2]\\n\\n# Write all columns except ID column into one string, MOTU names separated by comma\\n\\nswarm_lulu_spec_merge_map$MOTU_string\\u003c-apply(swarm_lulu_spec_merge_map[,2:ncol(swarm_lulu_spec_merge_map)], 1,paste, collapse=\\\",\\\") \\n\\n# Remove ,NA strings (occurred during the previous step when empty columns were pasted together)\\n# Keep only ID column and the MOTU_string column\\n\\nswarm_lulu_spec_merge_map$MOTU_string\\u003c-gsub(\\\",NA.*\\\",\\\"\\\", swarm_lulu_spec_merge_map$MOTU_string)\\nswarm_lulu_spec_merge_map\\u003c-swarm_lulu_spec_merge_map[,c(1,ncol(swarm_lulu_spec_merge_map))]\\n\\n# Aggregate rows based on ID column\\n\\nswarm_lulu_spec_merge_map\\u003c-aggregate(.~ ID, data = swarm_lulu_spec_merge_map, paste, collapse = \\\",\\\")\\n\\n# Map ASVs to MOTUs \\n\\n# motu_asv_mapping.txt stems from initial phyloseq processing performed previously \\nmapping\\u003c-read.table(\\\"motu_asv_mapping.txt\\\",sep=\\\"\\\\t\\\",header=T)\\nmapping\\u003c-mapping[order(match(mapping[,1],swarm_lulu_spec_merge_map[,1])),]\\n\\nswarm_lulu_spec_merge_map\\u003c-cbind(mapping[,2],swarm_lulu_spec_merge_map)\\ncolnames(swarm_lulu_spec_merge_map)[1:2]\\u003c-c(\\\"MOTU\\\",\\\"ASV_representative\\\")\\nswarm_lulu_spec_merge_map\\u003c-separate_wider_delim(swarm_lulu_spec_merge_map,MOTU_string, delim = \\\",\\\", names_sep=\\\"\\\",too_few = \\\"align_start\\\")\\n\\n# Subset to MOTUs previously identified as NIS\\n\\nunfiltered_nis_arms\\u003c-readRDS(\\\"unfiltered_nis_arms.rds\\\")# Saved in previous script when filtering MOTUs for NIS\\nnis_asv_motu_map\\u003c-swarm_lulu_spec_merge_map %\\u003e% filter(MOTU %in% taxa_names(unfiltered_nis_arms))\\n\\n# Create table mapping ASVs to those MOTUs and get respective ASV read counts\\n\\nnis_asv_motu_map \\u003c- melt(setDT(nis_asv_motu_map[,-2]), id.vars = \\\"MOTU\\\", variable.name = \\\"string\\\")\\nnis_asv_motu_map\\u003c-nis_asv_motu_map[,-2]\\ncolnames(nis_asv_motu_map)[2]\\u003c-\\\"ASV\\\"\\nnis_asv_motu_map\\u003c-nis_asv_motu_map[!is.na(nis_asv_motu_map$ASV),]\\nasv_counts\\u003c-read.table(\\\"asv_no_contaminants_COI.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1) # read blank-corrected ASV count table\\nasv_counts\\u003c-asv_counts[rownames(asv_counts) %in% nis_asv_motu_map$ASV,]\\n\\n# Subset ASV counts to the samples present in NIS data set and merge samples per ARMS sampling event\\n\\nunfiltered_nis\\u003c-readRDS(\\\"unfiltered_nis.rds\\\") # Saved in previous script when filtering MOTUs for NIS\\nnis_motu_physeq\\u003c-subset_samples(unfiltered_nis,sample_event %in% sample_names(unfiltered_nis_arms)) # get sample names of NIS data set with MOTUs showing at least 10 reads per ARMS sampling event\\nasv_counts\\u003c-asv_counts[colnames(asv_counts)%in%sample_names(nis_motu_physeq)] # Subset respective ASV count table to these samples\\nnis_asv_physeq\\u003c-phyloseq(otu_table(asv_counts,taxa_are_rows = TRUE), sample_data(sample_data(nis_motu_physeq))) # make new phyloseq object with ASV data set to merge samples per ARMS sampling event\\nnis_asv_physeq\\u003c-merge_samples(nis_asv_physeq,\\\"sample_event\\\") # ATTENTION: this transposes otu_table\\nasv_nis_counts\\u003c-as.data.frame(t(otu_table(nis_asv_physeq)))\\n\\n# Generate final table of NIS MOTUs, species names, the ASVs they contain and the ASV read counts per ARMS sampling event\\n\\nnis_asv_motu_map\\u003c-nis_asv_motu_map %\\u003e% filter(ASV %in% rownames(asv_nis_counts))\\nnis_asv_motu_map\\u003c-nis_asv_motu_map[order(match(nis_asv_motu_map$ASV,rownames(asv_nis_counts))),]\\nasv_motu_counts\\u003c-cbind(nis_asv_motu_map,asv_nis_counts)\\nasv_motu_counts\\u003c-asv_motu_counts[order(asv_motu_counts$MOTU),]\\nnis_taxa_counts\\u003c-read.table(\\\"nis_taxa_counts_COI.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1) # Read the nis_taxa_count table created in the previous NIS script to get taxonomy information\\n# add species names for MOTUs\\nfor (i in 1:nrow(nis_taxa_counts)) { \\n  asv_motu_counts$Species[asv_motu_counts$MOTU == rownames(nis_taxa_counts)[i]] \\u003c- nis_taxa_counts$Species[i]\\n} \\nasv_motu_counts\\u003c-asv_motu_counts %\\u003e% relocate(Species, .after=MOTU)\\n\\nwrite.table(asv_motu_counts,\\\"NIS_MOTU_ASV_counts_COI_nomin5.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\n## Write sequence headers to file to subset the fasta file to the NIS sequences outside of R\\n\\n# Write headers with ASV names, as fasta file still has sequences named by ASV names of representative sequences\\nnis_headers_asv\\u003c-paste0(\\\"\\u003e\\\",asv_motu_counts$ASV)\\nwrite.table(nis_headers_asv,\\\"nis_headers_asv_nomin5.txt\\\",sep=\\\"\\\\t\\\",row.names = F,quote=F,col.names = F)\\n\\n# Write mapping table of ASV to MOTU names to replace the ASV names in the fasta file with seqkit outside of R\\n\\nnew_headers\\u003c-cbind(asv_motu_counts$ASV,paste(asv_motu_counts$MOTU,asv_motu_counts$ASV,sep=\\\"_\\\"))\\nwrite.table(new_headers,\\\"nis_headers_motu_nomin5.txt\\\",sep=\\\"\\\\t\\\",row.names = F,quote=F,col.names = F)\\n\\n## Outisde of R, manually classify sequences again to check for confidence of assignments. In addition, check if occurences at respective location can actually be considered outside of native range.\\n\\n### 18S ###\\n\\nsetwd(\\\"~/18S\\\")\\n\\n## Create final mapping file with all ASVs that have been placed in one MOTU throughout the pipeline ##\\n\\n# read LULU and swarm output files and the mapping file of merging MOTUs with same species assignment \\n\\nlulu\\u003c-read.table(\\\"motu_map_lulu_18S.txt\\\",sep=\\\"\\\\t\\\",header=T)\\nswarm\\u003c-read.table(\\\"output.txt\\\",sep=\\\"\\\\t\\\",stringsAsFactors = F) \\nspec_merge\\u003c-read.table(\\\"motu_map_identical_species.txt\\\",sep=\\\"\\\\t\\\",header=T)\\n\\n# Separate swarm table into columns by space\\n# Remove read count strings from ASV names (lapply and function necessary to do this for every entry in the table, not just specific columns)\\n\\nswarm\\u003c-separate_wider_delim(swarm,V1, delim = \\\" \\\", names_sep=\\\"\\\",too_few = \\\"align_start\\\")\\nswarm[] \\u003c- lapply(swarm, function(y) gsub(\\\"_.*\\\",\\\"\\\", y))\\n\\n# Order entries of first column in swarm table based on first column in lulu table\\n\\nswarm\\u003c-swarm[order(match(swarm$V11,lulu$X)),]\\n\\n# Merge swarm and lulu tables\\n\\nswarm_lulu_map\\u003c-cbind(lulu[,4],swarm)\\ncolnames(swarm_lulu_map)[1]\\u003c-\\\"MOTU\\\"\\n\\n# Separate MOTU strings in spec_merge into separate columns\\n\\nspec_merge\\u003c-separate_wider_delim(spec_merge,MOTU, delim = \\\",\\\", names_sep=\\\"\\\",too_few = \\\"align_start\\\")\\n\\n# Create ID column in spec_merge (= MOTU other MOTUs have been merged onto) and transform to long format\\n\\nspec_merge\\u003c-cbind(spec_merge[,c(2,2:ncol(spec_merge))])\\ncolnames(spec_merge)[1]\\u003c-\\\"ID\\\"\\n\\nspec_merge_long \\u003c- melt(setDT(spec_merge), id.vars = \\\"ID\\\", variable.name = \\\"string\\\")\\nspec_merge_long\\u003c-spec_merge_long[,-2]\\ncolnames(spec_merge_long)[2]\\u003c-\\\"MOTU\\\"\\nspec_merge_long\\u003c-spec_merge_long[!is.na(spec_merge_long$MOTU),]\\n\\n## Merge swarm_lulu_map and spec_merge_long\\n\\nswarm_lulu_spec_merge_map\\u003c-as.data.frame(merge(swarm_lulu_map, spec_merge_long, by = \\\"MOTU\\\", all = TRUE))\\nswarm_lulu_spec_merge_map\\u003c-swarm_lulu_spec_merge_map %\\u003e% relocate(ID)\\n\\n# Where ID is NA, fill in with entry of MOTU column. Then, remove MOTU column.\\n\\nswarm_lulu_spec_merge_map$ID\\u003c-ifelse(is.na(swarm_lulu_spec_merge_map$ID),swarm_lulu_spec_merge_map$MOTU,swarm_lulu_spec_merge_map$ID)\\nswarm_lulu_spec_merge_map\\u003c-swarm_lulu_spec_merge_map[,-2]\\n\\n# Write all columns except ID column into one string, MOTU names separated by comma\\n\\nswarm_lulu_spec_merge_map$MOTU_string\\u003c-apply(swarm_lulu_spec_merge_map[,2:ncol(swarm_lulu_spec_merge_map)], 1,paste, collapse=\\\",\\\") \\n\\n# Remove ,NA strings (occurred during the previous step when empty columns were pasted together)\\n# Keep only ID column and the MOTU_string column\\n\\nswarm_lulu_spec_merge_map$MOTU_string\\u003c-gsub(\\\",NA.*\\\",\\\"\\\", swarm_lulu_spec_merge_map$MOTU_string)\\nswarm_lulu_spec_merge_map\\u003c-swarm_lulu_spec_merge_map[,c(1,ncol(swarm_lulu_spec_merge_map))]\\n\\n# Aggregate rows based on ID column\\n\\nswarm_lulu_spec_merge_map\\u003c-aggregate(.~ ID, data = swarm_lulu_spec_merge_map, paste, collapse = \\\",\\\")\\n\\n# Map ASVs to MOTUs \\n\\n# motu_asv_mapping.txt stems from initial phyloseq processing performed previously \\nmapping\\u003c-read.table(\\\"motu_asv_mapping.txt\\\",sep=\\\"\\\\t\\\",header=T)\\nmapping\\u003c-mapping[order(match(mapping[,1],swarm_lulu_spec_merge_map[,1])),]\\n\\nswarm_lulu_spec_merge_map\\u003c-cbind(mapping[,2],swarm_lulu_spec_merge_map)\\ncolnames(swarm_lulu_spec_merge_map)[1:2]\\u003c-c(\\\"MOTU\\\",\\\"ASV_representative\\\")\\nswarm_lulu_spec_merge_map\\u003c-separate_wider_delim(swarm_lulu_spec_merge_map,MOTU_string, delim = \\\",\\\", names_sep=\\\"\\\",too_few = \\\"align_start\\\")\\n\\n# Subset to MOTUs previously identified as NIS\\n\\nunfiltered_nis_arms\\u003c-readRDS(\\\"unfiltered_nis_arms.rds\\\")# Saved in previous script when filtering MOTUs for NIS\\nnis_asv_motu_map\\u003c-swarm_lulu_spec_merge_map %\\u003e% filter(MOTU %in% taxa_names(unfiltered_nis_arms))\\n\\n# Create table mapping ASVs to those MOTUs and get respective ASV read counts\\n\\nnis_asv_motu_map \\u003c- melt(setDT(nis_asv_motu_map[,-2]), id.vars = \\\"MOTU\\\", variable.name = \\\"string\\\")\\nnis_asv_motu_map\\u003c-nis_asv_motu_map[,-2]\\ncolnames(nis_asv_motu_map)[2]\\u003c-\\\"ASV\\\"\\nnis_asv_motu_map\\u003c-nis_asv_motu_map[!is.na(nis_asv_motu_map$ASV),]\\nasv_counts\\u003c-read.table(\\\"asv_no_contaminants_18S.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1) # read blank-corrected ASV count table\\nasv_counts\\u003c-asv_counts[rownames(asv_counts) %in% nis_asv_motu_map$ASV,]\\n\\n# Subset ASV counts to the samples present in NIS data set and merge samples per ARMS sampling event\\n\\nunfiltered_nis\\u003c-readRDS(\\\"unfiltered_nis.rds\\\") # Saved in previous script when filtering MOTUs for NIS\\nnis_motu_physeq\\u003c-subset_samples(unfiltered_nis,sample_event %in% sample_names(unfiltered_nis_arms)) # get sample names of NIS data set with MOTUs showing at least 10 reads per ARMS sampling event\\nasv_counts\\u003c-asv_counts[colnames(asv_counts)%in%sample_names(nis_motu_physeq)] # Subset respective ASV count table to these samples\\nnis_asv_physeq\\u003c-phyloseq(otu_table(asv_counts,taxa_are_rows = TRUE), sample_data(sample_data(nis_motu_physeq))) # make new phyloseq object with ASV data set to merge samples per ARMS sampling event\\nnis_asv_physeq\\u003c-merge_samples(nis_asv_physeq,\\\"sample_event\\\") # ATTENTION: this transposes otu_table\\nasv_nis_counts\\u003c-as.data.frame(t(otu_table(nis_asv_physeq)))\\n\\n# Generate final table of NIS MOTUs, species names, the ASVs they contain and the ASV read counts per ARMS sampling event\\n\\nnis_asv_motu_map\\u003c-nis_asv_motu_map %\\u003e% filter(ASV %in% rownames(asv_nis_counts))\\nnis_asv_motu_map\\u003c-nis_asv_motu_map[order(match(nis_asv_motu_map$ASV,rownames(asv_nis_counts))),]\\nasv_motu_counts\\u003c-cbind(nis_asv_motu_map,asv_nis_counts)\\nasv_motu_counts\\u003c-asv_motu_counts[order(asv_motu_counts$MOTU),]\\nnis_taxa_counts\\u003c-read.table(\\\"nis_taxa_counts_18S.txt\\\",sep=\\\"\\\\t\\\",header=T,row.names = 1) # Read the nis_taxa_count table created in the previous NIS script to get taxonomy information\\n# add species names for MOTUs\\nfor (i in 1:nrow(nis_taxa_counts)) { \\n  asv_motu_counts$Species[asv_motu_counts$MOTU == rownames(nis_taxa_counts)[i]] \\u003c- nis_taxa_counts$Species[i]\\n} \\nasv_motu_counts\\u003c-asv_motu_counts %\\u003e% relocate(Species, .after=MOTU)\\n\\nwrite.table(asv_motu_counts,\\\"NIS_MOTU_ASV_counts_18S_nomin5.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\n## Write sequence headers to file to subset the fasta file to the NIS sequences outside of R\\n\\n# Write headers with ASV names, as fasta file still has sequences named by ASV names of representative sequences\\nnis_headers_asv\\u003c-paste0(\\\"\\u003e\\\",asv_motu_counts$ASV)\\nwrite.table(nis_headers_asv,\\\"nis_headers_asv_nomin5.txt\\\",sep=\\\"\\\\t\\\",row.names = F,quote=F,col.names = F)\\n\\n# Write mapping table of ASV to MOTU names to replace the ASV names in the fasta file with seqkit outside of R\\n\\nnew_headers\\u003c-cbind(asv_motu_counts$ASV,paste(asv_motu_counts$MOTU,asv_motu_counts$ASV,sep=\\\"_\\\"))\\nwrite.table(new_headers,\\\"nis_headers_motu_nomin5.txt\\\",sep=\\\"\\\\t\\\",row.names = F,quote=F,col.names = F)\\n\\n## Outisde of R, manually classify sequences again to check for confidence of assignments. In addition, check if occurences at respective location can actually be considered outside of native range.\",\"os_name\":\"\",\"os_version\":\"\"}},\"3\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"8694B23F683511EE86830A58A9FEAC02\",\"url\":\"https://v3.boldsystems.org/index.php/IDS_OpenIdEngine\"}},\"4\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"1694D910683611EE86830A58A9FEAC02\",\"url\":\"https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastn\\u0026PAGE_TYPE=BlastSearch\\u0026LINK_LOC=blasthome\"}},\"5\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"819613F2690011EEA7690A58A9FEAC02\",\"url\":\"https://app.pr2-database.org/pr2-database\"}},\"6\":{\"type\":\"file\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"\",\"id\":454196,\"original_name\":\"arms_coordinates.txt\",\"placeholder\":\"https://www.protocols.io/img/extensions/txt.png\",\"raw_source\":\"\",\"size\":2982,\"source\":\"https://content.protocols.io/files/p5twbimqf.txt\"}},\"7\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Create final NIS presence-absence and ARMS coordinates tables\",\"description\":\"\",\"guid\":\"8CA58928690511EEA7690A58A9FEAC02\",\"name\":\"library(plyr)\\nlibrary(dplyr)\\nlibrary(tidyr)\\n\\n# Get the tables manually curated in Excel\\n\\nnis_taxa_counts_coi\\u003c-read.table(\\\"COI/NIS_MOTU_ASV_counts_COI_curated.txt\\\",sep=\\\"\\\\t\\\",header=T,check.names = F)\\nnis_taxa_counts_coi\\u003c-nis_taxa_counts_coi %\\u003e% select(-ASV) # remove ASV column\\n\\nnis_taxa_counts_18s\\u003c-read.table(\\\"18S/NIS_MOTU_ASV_counts_18S_curated.txt\\\",sep=\\\"\\\\t\\\",header=T,check.names = F)\\nnis_taxa_counts_18s\\u003c-nis_taxa_counts_18s %\\u003e% select(-ASV) # remove ASV column\\n\\n# Aggregate counts based on MOTU\\n\\nnis_taxa_counts_coi\\u003c-aggregate(.~ MOTU + Species, data = nis_taxa_counts_coi,FUN=sum)\\nnis_taxa_counts_18s\\u003c-aggregate(.~ MOTU + Species, data = nis_taxa_counts_18s,FUN=sum)\\n\\n# Transform to presence-absence and write to file\\n\\nstr(nis_taxa_counts_coi) #check if counts are numeric or integer: they are integer, so choose \\\"is.integer\\\" in the next line\\nnis_taxa_counts_coi \\u003c- nis_taxa_counts_coi %\\u003e% mutate_if(is.integer, ~1 * (. \\u003e 0))\\nstr(nis_taxa_counts_18s) #check if counts are numeric or integer: they are integer, so choose \\\"is.integer\\\" in the next line\\nnis_taxa_counts_18s \\u003c- nis_taxa_counts_18s %\\u003e% mutate_if(is.integer, ~1 * (. \\u003e 0))\\n\\nwrite.table(nis_taxa_counts_coi,\\\"COI/COI_unfiltered_NIS_presence_absence_ARMS.txt\\\",sep=\\\"\\\\t\\\",row.names=F)\\nwrite.table(nis_taxa_counts_18s,\\\"18S/18S_unfiltered_NIS_presence_absence_ARMS.txt\\\",sep=\\\"\\\\t\\\",row.names=F)\\n\\n# Combine both tables\\nnis_all\\u003c-rbind.fill(nis_taxa_counts_coi,nis_taxa_counts_18s)\\n\\n# Species will be merged further downstream.\\n# However, there are MOTU assignments where a NIS could only be clearly determined on the genus level.\\n# Where multiple of the same Genus sp. classification remain, we keep them separate. So these need to be renamed as Genus sp. _1/_2/ etc.\\n\\nnis_all[grepl(\\\"sp\\\\\\\\.\\\", nis_all$Species),\\\"Species\\\" ] # check genus level assignments to see which ones are duplicated\\n\\nnis_all[nis_all$Species == \\\"Watersipora sp.\\\",] # Watersipora sp. is duplicated, get the respective rows\\n\\n# Manually add number to the respective assignments for these MOTUs\\nnis_all$Species\\u003c-ifelse(nis_all$MOTU==\\\"MOTU209\\\",\\\"Watersipora sp._1\\\",nis_all$Species)\\nnis_all$Species\\u003c-ifelse(nis_all$MOTU==\\\"MOTU53\\\",\\\"Watersipora sp._2\\\",nis_all$Species)\\nnis_all$Species\\u003c-ifelse(nis_all$MOTU==\\\"MOTU431\\\",\\\"Watersipora sp._3\\\",nis_all$Species)\\n\\n# Remove MOTU column\\nnis_all\\u003c-nis_all %\\u003e% select(-MOTU)\\n\\n# Set NAs in occurrences (got introduced during rbind.fill above) to zero and aggregate based on species name\\nnis_all[is.na(nis_all)]\\u003c-0\\nnis_all \\u003c- aggregate(. ~ Species, data = nis_all, FUN = sum)\\n\\n# Set all numeric values above zero to 1\\nnis_all\\u003c-nis_all %\\u003e% mutate_if(is.numeric, ~1 * (. \\u003e 0))\\n\\n# Sort columns alphabetically and bring Species column to front\\nnis_all\\u003c-nis_all[,order(colnames(nis_all))]\\nnis_all\\u003c-nis_all %\\u003e% relocate(Species)\\n\\n# Write to file\\nwrite.table(nis_all,\\\"ARMS_final_NIS_presence_absence.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\n## Write table with coordinates of ARMS locations\\n\\narms_events\\u003c-as.data.frame(colnames(nis_all[,-1]))\\ncolnames(arms_events)\\u003c-\\\"event\\\"\\narms_events$event2 \\u003c- arms_events$event\\narms_events\\u003c-separate_wider_delim(arms_events,event2,delim=\\\"_\\\",names_sep=\\\"\\\")\\ncolnames(arms_events)[2:4]\\u003c-c(\\\"ARMS\\\",\\\"Deployment\\\",\\\"Retrieval\\\")\\ncoord\\u003c-read.table(\\\"arms_coordinates.txt\\\",sep=\\\"\\\\t\\\",header=T)\\ncoord\\u003c-coord %\\u003e% filter(ARMS %in% arms_events$ARMS)\\narms_events\\u003c-merge(arms_events,coord)\\narms_events\\u003c-arms_events %\\u003e% relocate(event)\\n\\nwrite.table(arms_events,\\\"ARMS_final_NIS_coordinates.txt\\\",row.names = F)\\n\\n\",\"os_name\":\"\",\"os_version\":\"\"}}}}","data":null,"protocol_id":86559,"case_id":0,"critical_ids":"","duration":0,"original_id":1151786,"number":"14","cases":[],"critical":null},{"id":1766862,"guid":"E04253FE3C5411EE882A0A58A9FEAC02","previous_id":1766860,"previous_guid":"E040D7FA3C5411EE882A0A58A9FEAC02","section":"\u003cp\u003e\u003cem\u003eLULU\u003c/em\u003e curation\u003c/p\u003e","section_color":"#A492FF","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"847fq\",\"text\":\"The clustered MOTUs were curated using LULU  v0.1.0 (Frøslev et al., 2017) in R / RStudio. For details on LULU curation, see https://github.com/tobiasgf/lulu.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":39,\"length\":4},{\"style\":\"italic\",\"offset\":61,\"length\":6},{\"style\":\"italic\",\"offset\":78,\"length\":11},{\"style\":\"italic\",\"offset\":106,\"length\":4}],\"entityRanges\":[{\"key\":0,\"offset\":125,\"length\":32}],\"data\":{}},{\"key\":\"a8bvp\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"f2i2m\",\"text\":\"First, we generated MOTU tables based on the ASV read count tables resulting from blank correction (see above) and the swarm output files. See the following R script:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":119,\"length\":5},{\"style\":\"italic\",\"offset\":157,\"length\":2}],\"entityRanges\":[],\"data\":{}},{\"key\":\"38fab\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":1,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"fr48\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"h1vn\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"a1bdl\",\"text\":\"To obtain identical MOTU IDs in the MOTU tables and the fasta files with cluster representative sequences obtained from swarm, we removed the MOTU read abundance strings from the sequence headers in the latter from command line:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":120,\"length\":5}],\"entityRanges\":[],\"data\":{}},{\"key\":\"eju4c\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":2,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"7n7l5\",\"text\":\"Match lists meeting LULU's requirements were generated using BLASTn. For this, BLAST+ was installed as standalone version for Windows by downloading the corresponding win64.exe file from ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST (release 2.11.0 in this case). Please note that downloading via Mozilla lead to files being corrupted and not executable for some reason, this was not the case when using Chrome as browser. For each marker gene, a blastdatabase was then produced, followed by blasting the MOTUs against this database. We executed the following in command line (note that for the perc_identity parameter, different values were applied for COI and 18S):\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":20,\"length\":4},{\"style\":\"italic\",\"offset\":61,\"length\":8},{\"style\":\"italic\",\"offset\":77,\"length\":8},{\"style\":\"italic\",\"offset\":133,\"length\":1},{\"style\":\"italic\",\"offset\":303,\"length\":7},{\"style\":\"italic\",\"offset\":410,\"length\":6},{\"style\":\"italic\",\"offset\":581,\"length\":1},{\"style\":\"italic\",\"offset\":656,\"length\":1}],\"entityRanges\":[{\"key\":3,\"offset\":187,\"length\":51}],\"data\":{}},{\"key\":\"dnemd\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":4,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"2nm8b\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"bcpsr\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"39p1e\",\"text\":\"We then ran the actual LULU curation in R / RStudio with a minimium sequence similarity threshold of 0.84 for COI and 0.90 for 18S (this value has to be equal or above the perc_identity parameter used during the match list creation) and minimum co-occurrence rate of 0.95. For 18S, we changed the minimum_ratio to 100, from the default of 1. Curated MOTU tables as well as mapping files were generated, see the following script:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":23,\"length\":4},{\"style\":\"italic\",\"offset\":40,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"9r11t\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":5,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"3avqu\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"biv6t\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"6fa16\",\"text\":\"We then subset the representative sequences of MOTUs remaining after LULU curation from command line: \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":69,\"length\":4}],\"entityRanges\":[],\"data\":{}},{\"key\":\"9s6g3\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":6,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"6t92v\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{\"0\":{\"type\":\"link\",\"mutability\":\"\",\"data\":{\"guid\":\"1CCC67B2946B11EBB05A0A58A9FEAC02\",\"url\":\"https://github.com/tobiasgf/lulu\"}},\"1\":{\"type\":\"command\",\"mutability\":\"\",\"data\":{\"can_edit\":true,\"command_name\":\"MOTU tables for LULU using R\",\"description\":null,\"guid\":\"47C00F6A341F11EE9D4302C0B41BC903\",\"name\":\"### COI ###\\n\\nsetwd(\\\"~/COI\\\")\\n\\n# Read the uclust.txt table (from swarm output)\\n\\nuclust\\u003c-read.table(\\\"uclust.txt\\\",sep=\\\"\\\\t\\\",stringsAsFactors = F)\\n\\n# Delete the rows with value \\\"C\\\" in first column (to remove one of the S / C duplicate ASV names)\\n\\nuclust2\\u003c-subset(uclust, uclust[,1]!=\\\"C\\\")\\n\\n# Rename the 10th column if first column = S to give the most abundant ASv in a cluster a MOTU ID\\n\\nuclust2[,10] \\u003c-ifelse(uclust2[,1]==\\\"S\\\",uclust2[,9],uclust2[,10])\\n\\n# Subset the columns we need\\n\\nmotu_asv_list\\u003c-uclust2[,9:10]\\n\\n# Remove all characters after _ (incl. _) \\n\\nmotu_asv_list[] \\u003c- lapply(motu_asv_list, function(y) gsub(\\\"_.*\\\",\\\"\\\", y))\\n\\n# Rename columns\\n\\ncolnames(motu_asv_list)\\u003c-c(\\\"ASV\\\",\\\"MOTU\\\")\\n\\n# Read the ASV count table (output from blank correction)\\n\\nasv_counts\\u003c-read.table(\\\"asv_no_contaminants_COI.txt\\\",sep=\\\"\\\\t\\\",header = T,stringsAsFactors=F)\\n\\n# Sort motu_asv_list based on order in asv_counts\\n\\nmotu_asv_list\\u003c-motu_asv_list[order(match(motu_asv_list[,1],asv_counts[,1])),]\\n\\n# Bind corresponding MOTUs to the ASV count table\\n\\nasv_counts\\u003c-cbind(motu_asv_list$MOTU,asv_counts,stringsAsFactors=F)\\ncolnames(asv_counts)[1]\\u003c-\\\"MOTU\\\"\\n\\n# Sum ASV counts based on MOTU\\n# First, delete the original column containing the ASV names as they cannot be summed\\n\\nasv_counts\\u003c-asv_counts[-2]\\nmotu_table \\u003c- aggregate(. ~ MOTU, data=asv_counts, FUN=sum)\\n\\n# Write MOTU table to file\\n\\nwrite.table(motu_table,\\\"motu_table_COI.txt\\\",sep=\\\"\\\\t\\\",row.names=F,quote=F)\\n\\n### 18S ###\\n\\nsetwd(\\\"~/18S\\\")\\n\\n# Read the uclust.txt table (from swarm output)\\n\\nuclust\\u003c-read.table(\\\"uclust.txt\\\",sep=\\\"\\\\t\\\",stringsAsFactors = F)\\n\\n# Delete the rows with value \\\"C\\\" in first column (to remove one of the S / C duplicate ASV names)\\n\\nuclust2\\u003c-subset(uclust, uclust[,1]!=\\\"C\\\")\\n\\n# Rename the 10th column if first column = S to give the most abundant ASV in a cluster a MOTU ID\\n\\nuclust2[,10] \\u003c-ifelse(uclust2[,1]==\\\"S\\\",uclust2[,9],uclust2[,10])\\n\\n# Subset the columns we need\\n\\nmotu_asv_list\\u003c-uclust2[,9:10]\\n\\n# Remove all characters after _ (incl. _) \\n\\nmotu_asv_list[] \\u003c- lapply(motu_asv_list, function(y) gsub(\\\"_.*\\\",\\\"\\\", y))\\n\\n# Rename columns\\n\\ncolnames(motu_asv_list)\\u003c-c(\\\"ASV\\\",\\\"MOTU\\\")\\n\\n# Read the ASV count table (output from blank correction)\\n\\nasv_counts\\u003c-read.table(\\\"asv_no_contaminants_18S.txt\\\",sep=\\\"\\\\t\\\",header = T,stringsAsFactors=F)\\n\\n# Sort motu_asv_list based on order in asv_counts\\n\\nmotu_asv_list\\u003c-motu_asv_list[order(match(motu_asv_list[,1],asv_counts[,1])),]\\n\\n# Bind corresponding MOTUs to the ASV count table\\n\\nasv_counts\\u003c-cbind(motu_asv_list$MOTU,asv_counts,stringsAsFactors=F)\\ncolnames(asv_counts)[1]\\u003c-\\\"MOTU\\\"\\n\\n# Sum ASV counts based on MOTU\\n# First, delete the original column containing the ASV names as they cannot be summed\\n\\nasv_counts\\u003c-asv_counts[-2]\\nmotu_table \\u003c- aggregate(. ~ MOTU, data=asv_counts, FUN=sum)\\n\\n# Write MOTU table to file\\n\\nwrite.table(motu_table,\\\"motu_table_18S.txt\\\",sep=\\\"\\\\t\\\",row.names=F,quote=F)\\n\\n\",\"os_name\":null,\"os_version\":null}},\"2\":{\"type\":\"command\",\"mutability\":\"\",\"data\":{\"can_edit\":true,\"command_name\":\"Adjust sequence headers in the fasta files for LULU processing\",\"description\":null,\"guid\":\"47C00FFF341F11EE9D4302C0B41BC903\",\"name\":\"# COI\\ncd ~/COI\\nawk -F'_' '{print $1}' COI_cluster_reps.fa \\u003e COI_cluster_reps_lulu_ready.fa\\n\\n# 18S\\ncd ~/18S\\nawk -F'_' '{print $1}' 18S_cluster_reps.fa \\u003e 18S_cluster_reps_lulu_ready.fa\",\"os_name\":null,\"os_version\":null}},\"3\":{\"type\":\"link\",\"mutability\":\"\",\"data\":{\"guid\":\"286C5BDC7DB111ECAD1F0A58A9FEAC02\",\"url\":\"ftp://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST\"}},\"4\":{\"type\":\"command\",\"mutability\":\"\",\"data\":{\"can_edit\":true,\"command_name\":\"Produce match lists with BLASTn\",\"description\":null,\"guid\":\"47C0104A341F11EE9D4302C0B41BC903\",\"name\":\"# COI\\ncd ~/COI\\n\\nmakeblastdb -in COI_cluster_reps_lulu_ready.fa -parse_seqids -dbtype nucl\\n\\nblastn -db COI_cluster_reps_lulu_ready.fa -outfmt '6 qseqid sseqid pident' -out match_list.txt -qcov_hsp_perc 80 -perc_identity 84 -query COI_cluster_reps_lulu_ready.fa\\n\\n# 18S\\ncd ~/18S\\n\\nmakeblastdb -in 18S_cluster_reps_lulu_ready.fa -parse_seqids -dbtype nucl\\n\\nblastn -db 18S_cluster_reps_lulu_ready.fa -outfmt '6 qseqid sseqid pident' -out match_list.txt -qcov_hsp_perc 80 -perc_identity 90 -query 18S_cluster_reps_lulu_ready.fa\",\"os_name\":null,\"os_version\":null}},\"5\":{\"type\":\"command\",\"mutability\":\"\",\"data\":{\"can_edit\":true,\"command_name\":\"LULU curation in R\",\"description\":null,\"guid\":\"47C011AE341F11EE9D4302C0B41BC903\",\"name\":\"# library(devtools)\\n# install_github(\\\"tobiasgf/lulu\\\")\\n\\nlibrary(lulu)\\n\\n### COI ###\\n\\nsetwd(\\\"~/COI\\\")\\n\\n# Read MOTU table\\n\\nmotu_table\\u003c-read.table(\\\"motu_table_COI.txt\\\",sep=\\\"\\\\t\\\",header = T,row.names = 1,as.is=T)\\n\\n# Read match list\\n\\nmatchlist\\u003c-read.table(\\\"match_list.txt\\\",sep=\\\"\\\\t\\\",header=F,as.is = T,stringsAsFactors = F)\\n\\n# Run curation\\n\\ncurated_result \\u003c- lulu(motu_table, matchlist,minimum_match = 0.84,minimum_relative_cooccurence = 0.9)\\n\\n# Write curated MOTU table to file\\n\\nwrite.table(curated_result$curated_table,\\\"lulu_motu_table_COI.txt\\\",sep=\\\"\\\\t\\\",quote=F,col.names = NA)\\n\\n# Write data on how MOTUs where mapped to file\\n\\nwrite.table(curated_result$otu_map,\\\"motu_map_lulu_COI.txt\\\",sep=\\\"\\\\t\\\",quote=F,col.names = NA)\\n\\n# Write headers of curated MOTUs to file to subset the corresponding representative sequences of the swarm cluster fasta file later on\\n\\nlulu_curated_headers\\u003c-paste0(\\\"\\u003e\\\",row.names(curated_result$curated_table))\\nwrite.table(lulu_curated_headers,\\\"lulu_curated_headers.txt\\\",sep=\\\"\\\\t\\\",row.names = F,quote=F,col.names = F)\\n\\n### 18S ###\\n\\nsetwd(\\\"~/18S\\\")\\n\\n# Read MOTU table\\n\\nmotu_table\\u003c-read.table(\\\"motu_table_18S.txt\\\",sep=\\\"\\\\t\\\",header = T,row.names = 1,as.is=T)\\n\\n# Read match list\\n\\nmatchlist\\u003c-read.table(\\\"match_list.txt\\\",sep=\\\"\\\\t\\\",header=F,as.is = T,stringsAsFactors = F)\\n\\n# Run curation\\n\\ncurated_result \\u003c- lulu(motu_table, matchlist,minimum_match = 0.9,minimum_ratio=100,minimum_relative_cooccurence = 0.95)\\n\\n# Write curated MOTU table to file\\n\\nwrite.table(curated_result$curated_table,\\\"lulu_motu_table_18S.txt\\\",sep=\\\"\\\\t\\\",quote=F,col.names = NA)\\n\\n# Write data on how MOTUs where mapped to file\\n\\nwrite.table(curated_result$otu_map,\\\"motu_map_lulu_18S.txt\\\",sep=\\\"\\\\t\\\",quote=F,col.names = NA)\\n\\n# Write headers of curated MOTUs to file to subset the corresponding representative sequences of the swarm cluster fasta file later on\\n\\nlulu_curated_headers\\u003c-paste0(\\\"\\u003e\\\",row.names(curated_result$curated_table))\\nwrite.table(lulu_curated_headers,\\\"lulu_curated_headers.txt\\\",sep=\\\"\\\\t\\\",row.names = F,quote=F,col.names = F)\\n\",\"os_name\":null,\"os_version\":null}},\"6\":{\"type\":\"command\",\"mutability\":\"\",\"data\":{\"can_edit\":true,\"command_name\":\"Generate fasta files with MOTUs remaining after LULU curation\",\"description\":null,\"guid\":\"47C016B0341F11EE9D4302C0B41BC903\",\"name\":\"# COI\\ncd ~/COI\\ngrep -w -A 1 -f lulu_curated_headers.txt COI_cluster_reps_lulu_ready.fa --no-group-separator \\u003e COI_cluster_reps_lulu_curated.fa\\n\\n# 18S\\ncd ~/18S\\ngrep -w -A 1 -f lulu_curated_headers.txt 18S_cluster_reps_lulu_ready.fa --no-group-separator \\u003e 18S_cluster_reps_lulu_curated.fa\",\"os_name\":null,\"os_version\":null}}}}","data":null,"protocol_id":86559,"case_id":0,"critical_ids":"","duration":0,"original_id":1155114,"number":"8","cases":[],"critical":null},{"id":1766863,"guid":"E04308223C5411EE882A0A58A9FEAC02","previous_id":1766866,"previous_guid":"E044E0A13C5411EE882A0A58A9FEAC02","section":"\u003cp\u003eNegative control correction\u003c/p\u003e","section_color":"#E57785","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"5eanp\",\"text\":\"After nuMT filtering for COI, we removed 18S and COI ASVs for which the read count in negative control samples exceeded 10% of their total read count. This was done in R / RStudio using the following script: \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":168,\"length\":1},{\"style\":\"italic\",\"offset\":172,\"length\":7}],\"entityRanges\":[],\"data\":{}},{\"key\":\"d0mbo\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":0,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"en0fg\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7vif5\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"e06sf\",\"text\":\"We then subset the ASVs remaining after correction in command line:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"d5v2n\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":1,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"9ekoo\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{\"0\":{\"type\":\"command\",\"mutability\":\"\",\"data\":{\"can_edit\":true,\"command_name\":\"Blank correction in R\",\"description\":null,\"guid\":\"47C01738341F11EE9D4302C0B41BC903\",\"name\":\"### COI ###\\n\\nsetwd(\\\"~/COI\\\")\\n\\n# Read ASV count table (output from dada2)\\n\\nasv_table\\u003c-read.table(\\\"COI_ASV_counts_nosingle.txt\\\",sep=\\\"\\\\t\\\",header = T,row.names = 1,as.is=T,check.names=F)\\n\\n# Read list of ASVs which remained after NUMT removal\\n\\nasv_list\\u003c-read.table(\\\"nonpseudo.combined.names.txt\\\",sep=\\\"\\\\t\\\")\\n\\n# Remove \\\"\\u003e\\\" in asv_list\\n\\nasv_list[,1]\\u003c-gsub(\\\"\\u003e\\\",\\\"\\\",asv_list[,1])\\n\\n# Subset asv_table to non-numt ASVs\\n\\nasv_table\\u003c-subset(asv_table, rownames(asv_table) %in% asv_list[,1])\\n\\n# Subset rows where the ASV read count in the blank / negative samples  exceeds 10 % of an ASVs total read count \\n\\nblank1\\u003c-subset(asv_table,asv_table[,colnames(asv_table)==\\\"ERR3460471\\\"] \\u003e 0.1*rowSums(asv_table))\\nblank2\\u003c-subset(asv_table,asv_table[,colnames(asv_table)==\\\"ERR4018471\\\"] \\u003e 0.1*rowSums(asv_table))\\nblank3\\u003c-subset(asv_table,asv_table[,colnames(asv_table)==\\\"ERR4018737\\\"] \\u003e 0.1*rowSums(asv_table))\\nblank4\\u003c-subset(asv_table,asv_table[,colnames(asv_table)==\\\"ERR4914160\\\"] \\u003e 0.1*rowSums(asv_table))\\nblank5\\u003c-subset(asv_table,asv_table[,colnames(asv_table)==\\\"ERR7125534\\\"] \\u003e 0.1*rowSums(asv_table))\\nblank6\\u003c-subset(asv_table,asv_table[,colnames(asv_table)==\\\"ERR7125582\\\"] \\u003e 0.1*rowSums(asv_table))\\nblank7\\u003c-subset(asv_table,asv_table[,colnames(asv_table)==\\\"ERR9632065\\\"] \\u003e 0.1*rowSums(asv_table))\\nblank8\\u003c-subset(asv_table,asv_table[,colnames(asv_table)==\\\"ERR12541481\\\"] \\u003e 0.1*rowSums(asv_table))\\nblank9\\u003c-subset(asv_table,asv_table[,colnames(asv_table)==\\\"ERR12541482\\\"] \\u003e 0.1*rowSums(asv_table))\\n\\n# before combining the tables using rbind, add a column with ASV IDs. This is necessary to stop R from introducing new rownames adding zeros to ASV names if duplicate ASVs exist in the newly created blank dataframes\\n\\nblank1 \\u003c- cbind(ASV_ID = rownames(blank1), blank1)\\nblank2 \\u003c- cbind(ASV_ID = rownames(blank2), blank2)\\nblank3 \\u003c- cbind(ASV_ID = rownames(blank3), blank3)\\nblank4 \\u003c- cbind(ASV_ID = rownames(blank4), blank4)\\nblank5 \\u003c- cbind(ASV_ID = rownames(blank5), blank5)\\nblank6 \\u003c- cbind(ASV_ID = rownames(blank6), blank6)\\nblank7 \\u003c- cbind(ASV_ID = rownames(blank7), blank7)\\nblank8 \\u003c- cbind(ASV_ID = rownames(blank8), blank8)\\nblank9 \\u003c- cbind(ASV_ID = rownames(blank9), blank9)\\n\\n# Combine blank dataframes\\n\\nblanks\\u003c-rbind(blank1,blank2,blank3,blank4,blank5,blank6,blank7,blank8,blank9)\\n\\n# Remove ASVs if there are duplicates in the \\\"blanks\\\" table (in case an ASV's read count exceeded 10 % of the total read count in more than one blank sample)\\n\\nblanks \\u003c- blanks[!duplicated(blanks$ASV_ID), ]\\n\\n# Remove the potential contaminant ASVs from the ASV table\\n\\nasv_table\\u003c- cbind(ASV_ID = rownames(asv_table), asv_table)\\nasv_no_contams\\u003c-asv_table[!(asv_table$ASV_ID %in% blanks$ASV_ID),]\\n\\n# Write table of potential contaminant ASVs and ASV count table devoid of contaminants\\n\\nwrite.table(blanks,\\\"asv_contaminants_COI.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\nwrite.table(asv_no_contams,\\\"asv_no_contaminants_COI.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\n# Write headers of non-contaminant asvs to file to subset the corresponding sequences of non-NUMT fasta file later on\\n\\nno_contam_headers\\u003c-paste0(\\\"\\u003e\\\",asv_no_contams$ASV_ID)\\nwrite.table(no_contam_headers,\\\"no_contam_headers_COI.txt\\\",sep=\\\"\\\\t\\\",row.names = F,quote=F,col.names = F)\\n\\n### 18S ###\\n\\nsetwd(\\\"~/18S\\\")\\n\\n# Read ASV count table (output from dada2)\\n\\nasv_table\\u003c-read.table(\\\"18S_ASV_counts_nosingle.txt\\\",sep=\\\"\\\\t\\\",header = T,row.names = 1,as.is=T,check.names=F)\\n\\n# Subset rows where the ASV read count in the blank / negative samples  exceeds 10 % of an ASVs total read count \\n\\nblank1\\u003c-subset(asv_table,asv_table[,colnames(asv_table)==\\\"ERR4018472\\\"] \\u003e 0.1*rowSums(asv_table))\\nblank2\\u003c-subset(asv_table,asv_table[,colnames(asv_table)==\\\"ERR4605902\\\"] \\u003e 0.1*rowSums(asv_table))\\nblank3\\u003c-subset(asv_table,asv_table[,colnames(asv_table)==\\\"ERR4914102\\\"] \\u003e 0.1*rowSums(asv_table))\\nblank4\\u003c-subset(asv_table,asv_table[,colnames(asv_table)==\\\"ERR7125533\\\"] \\u003e 0.1*rowSums(asv_table))\\nblank5\\u003c-subset(asv_table,asv_table[,colnames(asv_table)==\\\"ERR7125581\\\"] \\u003e 0.1*rowSums(asv_table))\\nblank6\\u003c-subset(asv_table,asv_table[,colnames(asv_table)==\\\"ERR9632062\\\"] \\u003e 0.1*rowSums(asv_table))\\nblank7\\u003c-subset(asv_table,asv_table[,colnames(asv_table)==\\\"ERR12541383\\\"] \\u003e 0.1*rowSums(asv_table))\\nblank8\\u003c-subset(asv_table,asv_table[,colnames(asv_table)==\\\"ERR12541384\\\"] \\u003e 0.1*rowSums(asv_table))\\n\\n# before combining the tables using rbind, add a column with ASV IDs. This is necessary to stop R from introducing new rownames adding zeros to ASV names if duplicate ASVs exist in the newly created blank dataframes\\n\\nblank1 \\u003c- cbind(ASV_ID = rownames(blank1), blank1)\\nblank2 \\u003c- cbind(ASV_ID = rownames(blank2), blank2)\\nblank3 \\u003c- cbind(ASV_ID = rownames(blank3), blank3)\\nblank4 \\u003c- cbind(ASV_ID = rownames(blank4), blank4)\\nblank5 \\u003c- cbind(ASV_ID = rownames(blank5), blank5)\\nblank6 \\u003c- cbind(ASV_ID = rownames(blank6), blank6)\\nblank7 \\u003c- cbind(ASV_ID = rownames(blank7), blank7)\\nblank8 \\u003c- cbind(ASV_ID = rownames(blank8), blank8)\\n\\n# Combine blank dataframes\\n\\nblanks\\u003c-rbind(blank1,blank2,blank3,blank4,blank5,blank6,blank7,blank8)\\n\\n# Remove ASVs if there are duplicates in the \\\"blanks\\\" table (in case an ASV's read count exceeded 10 % of the total read count in more than one blank sample)\\n\\nblanks \\u003c- blanks[!duplicated(blanks$ASV_ID), ]\\n\\n# Remove the potential contaminant ASVs from the ASV table\\n\\nasv_table\\u003c- cbind(ASV_ID = rownames(asv_table), asv_table)\\nasv_no_contams\\u003c-asv_table[!(asv_table$ASV_ID %in% blanks$ASV_ID),]\\n\\n# Write table of potential contaminant ASVs and ASV count table devoid of contaminants\\n\\nwrite.table(blanks,\\\"asv_contaminants_18S.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\nwrite.table(asv_no_contams,\\\"asv_no_contaminants_18S.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\n# Write headers of non-contaminant ASVs to file to subset the corresponding sequences later on\\n\\nno_contam_headers\\u003c-paste0(\\\"\\u003e\\\",asv_no_contams$ASV_ID)\\nwrite.table(no_contam_headers,\\\"no_contam_headers_18S.txt\\\",sep=\\\"\\\\t\\\",row.names = F,quote=F,col.names = F)\\n\\n\",\"os_name\":null,\"os_version\":null}},\"1\":{\"type\":\"command\",\"mutability\":\"\",\"data\":{\"can_edit\":true,\"command_name\":\"Generate fastas with ASVs remaining after blank correction\",\"description\":null,\"guid\":\"47C016B0341F11EE9D4302C0B41BC903\",\"name\":\"# COI\\ncd ~/COI\\ngrep -w -A 1 -f no_contam_headers_COI.txt COI_nochim_nosingle_nopseudo.fa --no-group-separator \\u003e COI_nochim_nosingle_nopseudo_nocontam.fa\\n\\n# 18S\\ncd ~/18S\\ngrep -w -A 1 -f no_contam_headers_18S.txt 18S_nochim_nosingle_ASVs.fa --no-group-separator \\u003e 18S_nochim_nosingle_nocontam.fa\",\"os_name\":null,\"os_version\":null}}}}","data":null,"protocol_id":86559,"case_id":0,"critical_ids":"","duration":0,"original_id":1159628,"number":"6","cases":[],"critical":null},{"id":1766864,"guid":"E04370A93C5411EE882A0A58A9FEAC02","previous_id":1766862,"previous_guid":"E04253FE3C5411EE882A0A58A9FEAC02","section":"\u003cp\u003eSubset taxonomy of 18S MOTUs and taxonomy assignment for COI MOTUs\u003c/p\u003e","section_color":"#94EBFF","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"7u7sr\",\"text\":\"Taxonomy has previously been assigned to 18S ASVs (see above). With the R script below, the respective taxonomy of the representative ASV sequences of LULU-curated MOTUs will be subset from the initial taxonomy table:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":72,\"length\":1},{\"style\":\"italic\",\"offset\":151,\"length\":4}],\"entityRanges\":[],\"data\":{}},{\"key\":\"c52aa\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":0,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"19h7t\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"eb39s\",\"text\":\"Taxonomy was assigned to COI MOTUs using two assignment tools and reference databases. The Barcode Of Life Data System (i.e., BOLD) database was used via the Python tool BOLDigger-commandline (see https://github.com/DominikBuchner/BOLDigger-commandline and https://github.com/DominikBuchner/BOLDigger for details). This tool requires a BOLDSYSTEMS user account (https://v3.boldsystems.org/) and Python v3.6 or higher. In addition, taxonomy was assigned to the COI dataset using the MIDORI2 database via its web server tool (https://reference-midori.info/server.php; based on GenBank release 257 at the time of usage). Results from both databases were then compared and final classification determined as described below. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":91,\"length\":15},{\"style\":\"italic\",\"offset\":107,\"length\":11},{\"style\":\"italic\",\"offset\":126,\"length\":6},{\"style\":\"italic\",\"offset\":159,\"length\":5},{\"style\":\"italic\",\"offset\":170,\"length\":21},{\"style\":\"italic\",\"offset\":336,\"length\":11},{\"style\":\"italic\",\"offset\":395,\"length\":6},{\"style\":\"italic\",\"offset\":482,\"length\":7},{\"style\":\"italic\",\"offset\":575,\"length\":7}],\"entityRanges\":[{\"key\":1,\"offset\":197,\"length\":55},{\"key\":2,\"offset\":257,\"length\":43},{\"key\":3,\"offset\":362,\"length\":27},{\"key\":4,\"offset\":524,\"length\":40}],\"data\":{}},{\"key\":\"c0m5u\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"210a9\",\"text\":\"For MIDORI2, we uploaded the LULU-curated fasta file to the web server (http://reference-midori.info/server.php). Because the web server allows a maximum of 10,000 sequences to be classified in one job and our LULU-curated fasta file contained more sequences, we simply split the fasta file into two files in R/RStudio using the commands below:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":4,\"length\":7},{\"style\":\"italic\",\"offset\":29,\"length\":4},{\"style\":\"italic\",\"offset\":210,\"length\":4},{\"style\":\"italic\",\"offset\":308,\"length\":10}],\"entityRanges\":[{\"key\":5,\"offset\":72,\"length\":39}],\"data\":{}},{\"key\":\"dnr97\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":6,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"7unnu\",\"text\":\"Note: It seems that if the number of sequences in the input fasta file is odd-numbered, splitting into two files will create two new equally-sized fasta files containing the same even number of sequence) and a third fasta file with the final remaining sequence. you can just manually add this final sequence to the second fasta file and remove the third file.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"25dnv\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7g86p\",\"text\":\"We then simply ran two jobs on the server using the two fasta files as input. As Program, we used the \\\"RDP classifier\\\". As Database, we chose \\\"Unique\\\" and \\\"COI\\\". We applied a confidence cut-off of 0.7 with the output format \\\"allrank\\\" in the Parameters section. While there is an output format \\\"filterbyconf\\\" which automatically provides classification for each taxonomic level when the confidence threshold is met (and otherwise sets it as \\\"unclassified\\\"), it does not provide species level classification for some reason. Therefore, we ran classification with aforementioned \\\"allrank\\\" format and subsequently filtered the taxonomic classification to remove information with a classification confidence of below 0.7. Classification via the MIDORI2 webserver results in two files being provided via e-mail: ...hier_outfile.txt and ...usga_classified.txt. We used the ...usga_classified.txt files of the two jobs (re-named to midori2_GB_257_1.txt and midori2_GB_257_2.txt with 257 being the respective GenBank version which MIDORI2 applies at the time of usage) for threshold application. In a few cases, information is missing on some taxonomic levels while it is available on lower levels, which shifts the entries within the results table. This made it necessary to format the result files and subsequently merge both files. This was done in R / RStudio using the commands below: \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":81,\"length\":7},{\"style\":\"italic\",\"offset\":123,\"length\":8},{\"style\":\"italic\",\"offset\":210,\"length\":13},{\"style\":\"italic\",\"offset\":241,\"length\":10},{\"style\":\"italic\",\"offset\":279,\"length\":13},{\"style\":\"italic\",\"offset\":740,\"length\":7},{\"style\":\"italic\",\"offset\":1000,\"length\":7},{\"style\":\"italic\",\"offset\":1022,\"length\":7},{\"style\":\"italic\",\"offset\":1343,\"length\":12}],\"entityRanges\":[],\"data\":{}},{\"key\":\"btrhm\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":7,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"c3dkt\",\"text\":\"The final filtering to remove taxonomic information with a classification confidence of below 0.7 at the respective level from the formatted table (midori_pre_70.txt) was done in Excel. In order to do this, he column giving the confidence values for phylum level assignments was first sorted from highest to lowest. All assignments with a confidence value below our chosen threshold of 0.7 were set to NA (This was done for the phylum level confidence column, the two columns left of it and all columns right of it. The procedure was then repeated for all confidence value columns down to species level). The resulting file was saved as midori_70.txt (70 = confidence threshold of 0.7). \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":148,\"length\":17},{\"style\":\"italic\",\"offset\":179,\"length\":7},{\"style\":\"italic\",\"offset\":637,\"length\":13}],\"entityRanges\":[],\"data\":{}},{\"key\":\"14l0k\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"ckbqr\",\"text\":\"For BOLDigger, we ran a four-step protocol via its command line tool with the commands below (the non-commandline tool has more extensive documentation: https://github.com/DominikBuchner/BOLDigger):\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":4,\"length\":9}],\"entityRanges\":[{\"key\":8,\"offset\":153,\"length\":43}],\"data\":{}},{\"key\":\"ee2m9\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8jf9b\",\"text\":\"Taxonomy assignment with the fasta file containing the curated MOTUs using the BOLD identification engine with the COI database and a batch size of 50. It may happen that reaching BOLD fails with high batch sizes, try batch sizes of 5 or 10 in this case. Log in details for BOLD user account need to be provided (account can be created via BOLD's website prior to this).\",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":79,\"length\":26},{\"style\":\"italic\",\"offset\":180,\"length\":4},{\"style\":\"italic\",\"offset\":274,\"length\":4},{\"style\":\"italic\",\"offset\":340,\"length\":4}],\"entityRanges\":[],\"data\":{}},{\"key\":\"f908j\",\"text\":\"Searching for additional data with the BOLDresults file generated in step 1. \",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":0,\"length\":29}],\"entityRanges\":[],\"data\":{}},{\"key\":\"2u74k\",\"text\":\"Add a list of top hits with BOLDigger method.\",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":0,\"length\":23},{\"style\":\"italic\",\"offset\":28,\"length\":9}],\"entityRanges\":[],\"data\":{}},{\"key\":\"76rp6\",\"text\":\"BOLD API verification. \\\"This option scans the BOLDigger top hits for hits with high similarity (\\u003e= 98%) and missing species-level assignment. This can happen, if the top 20 hits are populated with high similarity hits and missing species-level assignment \\\"hide\\\" away better hits.\\\" (from: https://github.com/DominikBuchner/BOLDigger)\",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":0,\"length\":21}],\"entityRanges\":[{\"key\":9,\"offset\":288,\"length\":43}],\"data\":{}},{\"key\":\"b9f9h\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":10,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"94tmp\",\"text\":\"The classification step of BOLDigger-commandline may terminate with an error for sequences where entries in BOLD produce a database error on the BOLD website (see https://github.com/DominikBuchner/BOLDigger/issues/24 and https://github.com/DominikBuchner/BOLDigger/issues/14, for example). This happened twice with our fatsa file (for ASV858 and ASV7885). When this happened, we executed the classification step again with a batch_size of 1 until the script got terminated again. We then manually removed the sequence following the last classified sequence from the fasta file and executed the command again with a batch_size of 50 until the next error occurred. After the BOLDigger-commandline procedure, we manually added these two sequences to the BOLDigger hit - API corrected sheet of the resulting .xlsx file and set all rank assignments for these two cases as No Match. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":27,\"length\":21},{\"style\":\"italic\",\"offset\":673,\"length\":22},{\"style\":\"italic\",\"offset\":751,\"length\":29},{\"style\":\"italic\",\"offset\":867,\"length\":8}],\"entityRanges\":[{\"key\":11,\"offset\":163,\"length\":53},{\"key\":12,\"offset\":221,\"length\":53}],\"data\":{}},{\"key\":\"d94pr\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"6n6nm\",\"text\":\"We compared taxonomy resulting from MIDORI2 and BOLDigger and created a final taxonomy table. For MOTUs where MIDORI2 and BOLDigger agreed at the respective level, this information was kept. For levels of a MOTU where the two disagreed, the information of \\\"BOLDigger hit - API corrected\\\" was kept at Phylum, Class or Order level if the similarity was above 85, at Family level if similarity was above 90, at Genus level if similarity was above 95, and at Species level if similarity was above 98. Where BOLDigger had NA entries and MIDORI2 non-NA entries, MIDORI2 classification was kept for the respective level of a MOTU. Where the two disagreed but BOLDigger did not meet the threshold for the respective level of a MOTU, entries were set as NA. These steps were performed in R / RStudio with the following commands:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":36,\"length\":7},{\"style\":\"italic\",\"offset\":48,\"length\":10},{\"style\":\"italic\",\"offset\":110,\"length\":7},{\"style\":\"italic\",\"offset\":122,\"length\":9},{\"style\":\"italic\",\"offset\":503,\"length\":9},{\"style\":\"italic\",\"offset\":532,\"length\":7},{\"style\":\"italic\",\"offset\":556,\"length\":7},{\"style\":\"italic\",\"offset\":779,\"length\":1},{\"style\":\"italic\",\"offset\":783,\"length\":7}],\"entityRanges\":[],\"data\":{}},{\"key\":\"7vf7r\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":13,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"d1fk\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"1pbgh\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{\"0\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Subset 18S taxonomy in R\",\"description\":\"\",\"guid\":\"202C91CD3D1C11EE855C0A58A9FEAC02\",\"name\":\"library(dplyr)\\n\\nsetwd(\\\"~/18S\\\")\\n\\n# Read the ensemble taxonomy table generated after the dada2 pipeline\\n\\ntax_table\\u003c-read.table(\\\"18S_tax_table.txt\\\",sep=\\\"\\\\t\\\",header=T)\\n\\n# Read the headers of LULU curated MOTUs\\n\\nheaders\\u003c-read.table(\\\"lulu_curated_headers.txt\\\",sep=\\\"\\\\t\\\")\\nheaders[,1]\\u003c-gsub(\\\"\\u003e\\\",\\\"\\\",headers[,1]) # Remove the \\\"\\u003e\\\" from the MOTU names\\n\\n# Subset the taxonomy assignments of ASVs which are the representative ASV sequences of LULU curated MOTUs\\n\\ntaxa_lulu\\u003c-tax_table %\\u003e% filter(ASV %in% headers[,1])\\ncolnames(taxa_lulu)[1]\\u003c-\\\"MOTU\\\"\\n\\n# Write to file\\n\\nwrite.table(taxa_lulu,\\\"18S_final_tax_table.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\",\"os_name\":\"\",\"os_version\":\"\"}},\"1\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"387281AB279711EE867F0A58A9FEAC02\",\"url\":\"https://github.com/DominikBuchner/BOLDigger-commandline\"}},\"10\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Install and run BOLDigger commandline tool\",\"description\":null,\"guid\":\"47C6EEE2341F11EE9D4302C0B41BC903\",\"name\":\"pip install boldigger_cline\\n\\ncd ~/COI\\n\\n# Run classification with default batch size of 50 \\nboldigger-cline ie_coi username password COI_cluster_reps_lulu_curated.fa ~/COI 50\\n\\n# Download additional data from BOLD\\nboldigger-cline add_metadata BOLDResults_COI_cluster_reps_lulu_curated_part_1.xlsx\\n\\n# Get top hit with BOLDigger method\\nboldigger-cline digger_hit BOLDResults_COI_cluster_reps_lulu_curated_part_1.xlsx\\n\\n# Perform API correction\\nboldigger-cline api_verification BOLDResults_COI_cluster_reps_lulu_curated_part_1.xlsx  COI_cluster_reps_lulu_curated_done.fa\",\"os_name\":null,\"os_version\":null}},\"11\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"8DC7DCA8E1FE11EE999C0A58A9FEAC02\",\"url\":\"https://github.com/DominikBuchner/BOLDigger/issues/24\"}},\"12\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"E1AA27BEE1FE11EE999C0A58A9FEAC02\",\"url\":\"https://github.com/DominikBuchner/BOLDigger/issues/14\"}},\"13\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"COI taxonomy table based on BOLDigger and MIDORI2 in R\",\"description\":null,\"guid\":\"47C709BC341F11EE9D4302C0B41BC903\",\"name\":\"library(readxl)\\nlibrary(tidyr)\\n\\n# Set working directory\\n\\nsetwd(\\\"~/COI\\\")\\n\\n### Compare Midori2 and BOLDigger taxonomy and keep agreeing taxonomy\\n\\n# Read files\\n\\n# Midori file\\n# After removing the entries with confidence below 0.7, the file was saved as \\\"midori_70.txt\\\"\\nmidori\\u003c-read.table(\\\"midori_70.txt\\\",sep=\\\"\\\\t\\\",header=T)\\nmidori\\u003c-midori[,c(1:3,6,9,12,15,18)] # Keep only taxonomy level columns\\n\\n# Boldigger file\\n# Read the API corrected sheetand remove the \\\"\\u003e\\\" in MOTU IDs \\nbold\\u003c-as.data.frame(read_xlsx(\\\"BOLDResults_COI_cluster_reps_lulu_curated_part_1.xlsx\\\",sheet=\\\"BOLDigger hit - API corrected\\\"))\\nbold[,1]\\u003c-gsub(\\\"\\u003e\\\",\\\"\\\",bold[,1])\\ncolnames(bold)[1]\\u003c-\\\"MOTU\\\"\\n\\n# In case the midori table hasn't been reordered in Excel, order entries based on order in BOLDigger table\\nmidori\\u003c-midori[order(match(midori[,1],bold[,1])),]\\nwrite.table(midori,\\\"midori_70.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\n# Separate the Species column of midori table with separate_wider-delim with names_sep=\\\"\\\". \\n# As many columns as needed will be created, because some species name have a CMCxy suffix etc and we can remove them through this procedure.\\n\\nmidori\\u003c-midori %\\u003e% separate_wider_delim(Species, delim = \\\" \\\", names_sep=\\\"\\\",too_few = \\\"align_start\\\")\\nmidori\\u003c-as.data.frame(midori[,c(1:7,9)]) # remove unnecessary columns\\n\\n# Create column with genus and species level in one string\\nmidori$Species\\u003c-paste(midori$Genus,midori$Species2,sep=\\\" \\\") \\nmidori$Species\\u003c-gsub(\\\".*( NA|NA ).*\\\", NA, midori$Species) # Replace entries that now contain the \\\" NA\\\" or \\\"NA \\\" string with NA\\nmidori\\u003c-midori[,-8] # Remove Species2 column\\n\\nbold$Species\\u003c-paste(bold$Genus,bold$Species,sep=\\\" \\\") \\nbold$Species\\u003c-gsub(\\\".*( NA|NA ).*\\\", NA, bold$Species) # Replace entries that now contain the \\\" NA\\\" or \\\"NA \\\" string with NA\\n\\n# To simplify downstream code, set NA entries in the Midori table as character string (we chose \\\"unknown\\\")\\nmidori[is.na(midori)] \\u003c- \\\"unknown\\\"\\n\\n## Generate final taxonomy\\n# For MOTUs where MIDORI2 and BOLDigger agreed at the respective level, this information was kept. \\n# For levels of a MOTU were the two disagreed, the information of \\\"BOLDigger hit - API corrected\\\" was kept at Phylum, Class or Order level if the similarity was above 85, at Family level if similarity was above 90, at Genus level if similarity was above 95, and at Species level if similarity as above 98. \\n# Where BOLDigger had NA entries and MIDORI2 non-NA entries, MIDORI2 classification was kept for the respective level of a MOTU. \\n# Where the two disagreed but BOLDigger did not meet the threshold for the respective level of a MOTU, entries where set as NA.\\n\\nPhylum \\u003c-ifelse(midori$Phylum == bold$Phylum | (bold$Similarity\\u003e85 \\u0026 bold$Phylum!=midori$Phylum),bold$Phylum,ifelse((is.na(bold$Phylum) | bold$Phylum == \\\"No Match\\\") \\u0026 !is.na(midori$Phylum),midori$Phylum,\\\"NA\\\"))\\nClass \\u003c-ifelse(midori$Class == bold$Class | (bold$Similarity\\u003e85 \\u0026 bold$Class!=midori$Class),bold$Class,ifelse((is.na(bold$Class) | bold$Class == \\\"No Match\\\") \\u0026 !is.na(midori$Class),midori$Class,\\\"NA\\\"))\\nOrder \\u003c-ifelse(midori$Order == bold$Order | (bold$Similarity\\u003e85 \\u0026 bold$Order!=midori$Order),bold$Order,ifelse((is.na(bold$Order) | bold$Order == \\\"No Match\\\") \\u0026 !is.na(midori$Order),midori$Order,\\\"NA\\\"))\\nFamily \\u003c-ifelse(midori$Family == bold$Family | (bold$Similarity\\u003e90 \\u0026 bold$Family!=midori$Family),bold$Family,ifelse((is.na(bold$Family) | bold$Family == \\\"No Match\\\") \\u0026 !is.na(midori$Family),midori$Family,\\\"NA\\\"))\\nGenus \\u003c-ifelse(midori$Genus == bold$Genus | (bold$Similarity\\u003e95 \\u0026 bold$Genus!=midori$Genus),bold$Genus,ifelse((is.na(bold$Genus) | bold$Genus == \\\"No Match\\\") \\u0026 !is.na(midori$Genus),midori$Genus,\\\"NA\\\"))\\nSpecies \\u003c-ifelse(midori$Species == bold$Species | (bold$Similarity\\u003e98 \\u0026 bold$Species!=midori$Species),bold$Species,ifelse((is.na(bold$Species) | bold$Species == \\\"No Match\\\") \\u0026 !is.na(midori$Species),midori$Species,\\\"NA\\\"))\\n\\n# Add column for Kingdom level\\ntax_table\\u003c-cbind(MOTU=bold$MOTU,Kingdom=rep(\\\"Eukaryota\\\",nrow(bold)),Phylum, Class, Order, Family,Genus,Species)\\n\\n# write final tax table to file\\nwrite.table(tax_table,\\\"boldigger_midori_tax_table.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\",\"os_name\":null,\"os_version\":null}},\"2\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"BD693A67279A11EE867F0A58A9FEAC02\",\"url\":\"https://github.com/DominikBuchner/BOLDigger\"}},\"3\":{\"type\":\"link\",\"mutability\":\"\",\"data\":{\"guid\":\"387281E6279711EE867F0A58A9FEAC02\",\"url\":\"https://v3.boldsystems.org\"}},\"4\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"38728222279711EE867F0A58A9FEAC02\",\"url\":\"http://reference-midori.info/server.php\"}},\"5\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"E3AE4FC9279711EE867F0A58A9FEAC02\",\"url\":\"http://reference-midori.info/server.php\"}},\"6\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Split fasta file in R\",\"description\":\"\",\"guid\":\"E29111D1B53311EEB16E0A58A9FEAC02\",\"name\":\"library(Biostrings)\\nlibrary(hiReadsProcessor)\\n\\nsetwd(\\\"~/COI\\\")\\n\\n# read LULU-curated fasta file  \\n\\nfasta\\u003c-readDNAStringSet(\\\"COI_cluster_reps_lulu_curated.fa \\\")\\n\\n# Split fasta into two files and save them\\n\\nsplitSeqsToFiles(fasta, 2, \\\"fasta\\\",\\\"COI_cluster_reps_lulu_curated\\\")\",\"os_name\":\"\",\"os_version\":\"\"}},\"7\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Format MIDORI2 results in R for confidence threshold application\",\"description\":null,\"guid\":\"47C6EF71341F11EE9D4302C0B41BC903\",\"name\":\"library(dplyr)\\n\\nsetwd(\\\"~/COI\\\")\\n\\n### Format tax table from Midori2 web server to remove information with confidence below 0.7\\n\\n# Load the ...usga_classified.txt files (output of the Midori2 web server)\\n# Files have been renamed to midori_GB_257_1.txt and midori_GB_257_2.txt after downloading them\\n\\nmidori1\\u003c-read.table(\\\"midori2_GB_257_1.txt\\\",sep=\\\"\\\\t\\\",header=F,fill=T)\\nmidori2\\u003c-read.table(\\\"midori2_GB_257_2.txt\\\",sep=\\\"\\\\t\\\",header=F,fill=T)\\n\\n# remove unnecessary columns\\n\\nmidori1\\u003c-midori1[,-c(2:5,7,8)]\\nmidori2\\u003c-midori2[,-c(2:5,7,8)]\\n\\n# For some reason, there are rows where information on taxonomic levels is missing even though it is present on lower taxonomic levels.\\n# In these cases, the entries got shifted to the left and need to be shifted back to the correct position.\\n\\n# see where species level information got shifted and correct it\\n\\n# Identifying which rows to shift\\nrows_to_change1 \\u003c- midori1$V22 %in% \\\"species\\\"\\nrows_to_change2 \\u003c- midori2$V22 %in% \\\"species\\\"\\n\\n# Moving columns one space to the right\\nmidori1[rows_to_change1,18:20] \\u003c- midori1[rows_to_change1,15:17]\\nmidori2[rows_to_change2,18:20] \\u003c- midori2[rows_to_change2,15:17] \\n\\n# Deleting wrong values\\nmidori1[rows_to_change1,15:17] \\u003c- NA\\nmidori2[rows_to_change2,15:17] \\u003c- NA\\n\\n# Identifying which rows to shift\\nrows_to_change1 \\u003c- midori1$V19 %in% \\\"species\\\"\\nrows_to_change2 \\u003c- midori2$V19 %in% \\\"species\\\"\\n\\n# Moving columns one space to the right\\nmidori1[rows_to_change1,18:20] \\u003c- midori1[rows_to_change1,12:14] \\nmidori2[rows_to_change2,18:20] \\u003c- midori2[rows_to_change2,12:14] \\n\\n# Deleting wrong values\\nmidori1[rows_to_change1,12:14] \\u003c- NA\\nmidori2[rows_to_change2,12:14] \\u003c- NA\\n\\n# Identifying which rows to shift\\nrows_to_change1 \\u003c- midori1$V16 %in% \\\"species\\\"\\nrows_to_change2 \\u003c- midori2$V16 %in% \\\"species\\\"\\n\\n# Moving columns one space to the right\\nmidori1[rows_to_change1,18:20] \\u003c- midori1[rows_to_change1,9:11]\\nmidori2[rows_to_change2,18:20] \\u003c- midori2[rows_to_change2,9:11]\\n\\n# Deleting wrong values\\nmidori1[rows_to_change1,9:11] \\u003c- NA\\nmidori2[rows_to_change2,9:11] \\u003c- NA\\n\\n# Identifying which rows to shift\\nrows_to_change1 \\u003c- midori1$V13 %in% \\\"species\\\"\\nrows_to_change2 \\u003c- midori2$V13 %in% \\\"species\\\"\\n\\n# Moving columns one space to the right\\nmidori1[rows_to_change1,18:20] \\u003c- midori1[rows_to_change1,6:8]\\nmidori2[rows_to_change2,18:20] \\u003c- midori2[rows_to_change2,6:8]\\n\\n# Deleting wrong values\\nmidori1[rows_to_change1,6:8] \\u003c- NA\\nmidori2[rows_to_change2,6:8] \\u003c- NA\\n\\n# see where genus level information got shifted and correct it\\n\\n# Identifying which rows to shift\\nrows_to_change1 \\u003c- midori1$V19 %in% \\\"genus\\\"\\nrows_to_change2 \\u003c- midori2$V19 %in% \\\"genus\\\"\\n\\n# Moving columns one space to the right\\nmidori1[rows_to_change1,15:17] \\u003c- midori1[rows_to_change1,12:14]\\nmidori2[rows_to_change2,15:17] \\u003c- midori2[rows_to_change2,12:14] \\n\\n# Deleting wrong values\\nmidori1[rows_to_change1,12:14] \\u003c- NA\\nmidori2[rows_to_change2,12:14] \\u003c- NA\\n\\n# Identifying which rows to shift\\nrows_to_change1 \\u003c- midori1$V16 %in% \\\"genus\\\"\\nrows_to_change2 \\u003c- midori2$V16 %in% \\\"genus\\\"\\n\\n# Moving columns one space to the right\\nmidori1[rows_to_change1,15:17] \\u003c- midori1[rows_to_change1,9:11]\\nmidori2[rows_to_change2,15:17] \\u003c- midori2[rows_to_change2,9:11]\\n\\n# Deleting wrong values\\nmidori1[rows_to_change1,9:11] \\u003c- NA\\nmidori2[rows_to_change2,9:11] \\u003c- NA\\n\\n# Identifying which rows to shift\\nrows_to_change1 \\u003c- midori1$V13 %in% \\\"genus\\\"\\nrows_to_change2 \\u003c- midori2$V13 %in% \\\"genus\\\"\\n\\n# Moving columns one space to the right\\nmidori1[rows_to_change1,15:17] \\u003c- midori1[rows_to_change1,6:8]\\nmidori2[rows_to_change2,15:17] \\u003c- midori2[rows_to_change2,6:8] \\n\\n# Deleting wrong values\\nmidori1[rows_to_change1,6:8] \\u003c- NA\\nmidori2[rows_to_change2,6:8] \\u003c- NA\\n\\n# see where family level information got shifted and correct it\\n\\n# Identifying which rows to shift\\nrows_to_change1 \\u003c- midori1$V16 %in% \\\"family\\\"\\nrows_to_change2 \\u003c- midori2$V16 %in% \\\"family\\\"\\n\\n# Moving columns one space to the right\\nmidori1[rows_to_change1,12:14] \\u003c- midori1[rows_to_change1,9:11]\\nmidori2[rows_to_change2,12:14] \\u003c- midori2[rows_to_change2,9:11]\\n\\n# Deleting wrong values\\nmidori1[rows_to_change1,9:11] \\u003c- NA\\nmidori2[rows_to_change2,9:11] \\u003c- NA\\n\\n# Identifying which rows to shift\\nrows_to_change1 \\u003c- midori1$V13 %in% \\\"family\\\"\\nrows_to_change2 \\u003c- midori2$V13 %in% \\\"family\\\"\\n\\n# Moving columns one space to the right\\nmidori1[rows_to_change1,12:14] \\u003c- midori1[rows_to_change1,6:8] \\nmidori2[rows_to_change2,12:14] \\u003c- midori2[rows_to_change2,6:8] \\n\\n# Deleting wrong values\\nmidori1[rows_to_change1,6:8] \\u003c- NA\\nmidori2[rows_to_change2,6:8] \\u003c- NA\\n\\n# see where order level information got shifted and correct it\\n\\n# Identifying which rows to shift\\nrows_to_change1 \\u003c- midori1$V13 %in% \\\"order\\\"\\nrows_to_change2 \\u003c- midori2$V13 %in% \\\"order\\\"\\n\\n# Moving columns one space to the right\\nmidori1[rows_to_change1,9:11] \\u003c- midori1[rows_to_change1,6:8]\\nmidori2[rows_to_change2,9:11] \\u003c- midori2[rows_to_change2,6:8]\\n\\n# Deleting wrong values\\nmidori1[rows_to_change1,6:8] \\u003c- NA\\nmidori2[rows_to_change2,6:8] \\u003c- NA\\n\\n# Check if class level info got shifted\\n\\ndim(midori1[!midori1$V10 %in% \\\"phylum\\\",]) # zero rows\\ndim(midori2[!midori2$V10 %in% \\\"phylum\\\",]) # zero rows\\n\\n# Information for class level and above did not get shifted #\\n\\n# Combine the two tables\\n\\nmidori\\u003c-rbind(midori1,midori2)\\n\\n# Some empty \\\"\\\" cells remain after processing, set them as NA\\n\\nmidori[midori==\\\"\\\"] \\u003c- NA\\n\\n# Name the taxonomy columns\\n\\ncolnames(midori)[c(1,2,3,6,9,12,15,18)]\\u003c-c(\\\"MOTU\\\",\\\"Kingdom\\\",\\\"Phylum\\\",\\\"Class\\\",\\\"Order\\\",\\\"Family\\\",\\\"Genus\\\",\\\"Species\\\")\\n\\n# Write table to file\\n\\nwrite.table(midori,\\\"midori_pre_70.txt\\\",sep=\\\"\\\\t\\\",row.names = F) \\n\",\"os_name\":null,\"os_version\":null}},\"8\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"0CC46746279B11EE867F0A58A9FEAC02\",\"url\":\"https://github.com/DominikBuchner/BOLDigger\"}},\"9\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"B6FB5FBA279A11EE867F0A58A9FEAC02\",\"url\":\"https://github.com/DominikBuchner/BOLDigger\"}}}}","data":null,"protocol_id":86559,"case_id":0,"critical_ids":"","duration":0,"original_id":1159967,"number":"9","cases":[],"critical":null},{"id":1766865,"guid":"E044BC123C5411EE882A0A58A9FEAC02","previous_id":1766864,"previous_guid":"E04370A93C5411EE882A0A58A9FEAC02","section":"\u003cp\u003eMerging same-species MOTUs \u003c/p\u003e","section_color":"#84CE84","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"cv174\",\"text\":\"MOTUs which could be classified down to species level and which displayed the same species level assignment were merged. Here, read counts of corresponding same-species MOTUs were summed up and the representative sequence of the most abundant MOTU prior to merging was kept. The following script was executed in R / RStudio: \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":312,\"length\":11}],\"entityRanges\":[],\"data\":{}},{\"key\":\"b6fda\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":0,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"d822t\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{\"0\":{\"type\":\"command\",\"mutability\":\"\",\"data\":{\"can_edit\":true,\"command_name\":\"Merge same-species MOTUs in R\",\"description\":null,\"guid\":\"47C02C4A341F11EE9D4302C0B41BC903\",\"name\":\"library(stringr)\\nlibrary(dplyr)\\n\\n### COI ###\\n\\nsetwd(\\\"~/COI/\\\")\\n\\n# Read the Midori/ BOLDIgger taxonomy file\\n\\ntax_table\\u003c-read.table(\\\"boldigger_midori_tax_table.txt\\\",sep=\\\"\\\\t\\\",header = T,stringsAsFactors = F)\\n\\n# Read LULU curated MOTU count table.\\n\\nmotu_tab\\u003c-read.table(\\\"lulu_motu_table_COI.txt\\\",header=T,check.names=F,stringsAsFactors = F, sep=\\\"\\\\t\\\")\\n\\n# Sort the count table based on the order in the tax table and combine both tables\\n\\nmotu_tab\\u003c-motu_tab[order(match(motu_tab[,1],tax_table[,1])),]\\n\\nmotu_tax_tab\\u003c-as.data.frame(cbind(tax_table,motu_tab[,-1]),stringsAsFactors=F)\\n\\n# Write this MOTU table to file\\n\\nwrite.table(motu_tax_tab,\\\"COI_motu_table_tax_counts_species_fullname.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\n## Aggregate MOTUs with same species assignment, adding read counts of same-species MOTUs to the most abundant same-species MOTU. ##\\n\\n# Sort motu_tax_tab by read abundance \\n\\nmotu_tax_tab \\u003c- motu_tax_tab[order(rowSums(motu_tax_tab[,9:ncol(motu_tax_tab)]),decreasing=TRUE),]\\n\\n# Create a mapping file for subsequent downstream analysis to track which MOTUs are going to be merged.\\n\\nmotu_map\\u003c-aggregate(MOTU ~ Species, data = motu_tax_tab, paste, collapse = \\\",\\\")\\nwrite.table(motu_map,\\\"motu_map_identical_species.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\n# Aggregate rows by species entries and sum read counts.\\n\\ntax_sum\\u003c-aggregate(.~ motu_tax_tab$Species, data = motu_tax_tab[,9:ncol(motu_tax_tab)], sum)\\n\\n# Combine this table with the mapping table\\n\\nmotu_unique\\u003c-as.data.frame(cbind(motu_map,tax_sum[,-1]))\\n\\n# Where same-species MOTUs have been merged, we only want to keep the MOTU ID of the most abundant one.\\n# Where ID entries with aggregated MOTUs exist, the first entry equals the most abundant MOTU so we delete the characters after (and incl.) the comma.\\n\\nmotu_unique$MOTU\\u003c-gsub(\\\",.*\\\",\\\"\\\",motu_unique$MOTU)\\n\\n# Filter the previously generated motu_tax_tab table to only contain the MOTUs now listed in the motu_unique table.\\n\\nmotu_tax_tab_uniq\\u003c-motu_tax_tab[motu_tax_tab$MOTU %in% motu_unique$MOTU,]\\n\\n# Order the motu_unique table to match the MOTU ID order of motu_tax_tab_uniq and create a final MOTU table with the taxonomy info of motu_tax_tab_uniq and the summed up read counts of motu_unique \\n\\nmotu_unique\\u003c-motu_unique[order(match(motu_unique[,2],motu_tax_tab_uniq[,1])),]\\nfinal_motu_tab\\u003c-as.data.frame(cbind(motu_tax_tab_uniq[,1:8],motu_unique[,-c(1,2)]))\\n\\n# Combine it with the entries that were not part of the aggregation procedure, e.g. MOTUs with no species level assignment\\n\\nfinal_motu_tab\\u003c-rbind(final_motu_tab,motu_tax_tab[is.na(motu_tax_tab$Species),])\\n\\n# Write count table to file for phyloseq processing\\n\\nwrite.table(final_motu_tab[,c(1,9:ncol(final_motu_tab))],\\\"COI_motu_count_table_merged_species.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\n# Write taxonomy table to file for phyloseq processing\\n\\nwrite.table(final_motu_tab[,1:8],\\\"COI_motu_tax_table_merged_species.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\n\\n### 18S ###\\n\\nsetwd(\\\"~/18S\\\")\\n\\n# Read the taxonomy file (taxonomy was previously subset for LULU-curated MOTUs)\\n\\ntax_table\\u003c-read.table(\\\"18S_final_tax_table.txt\\\",sep=\\\"\\\\t\\\",header = T,stringsAsFactors = F)\\n\\n# Read LULU curated MOTU count table.\\n\\nmotu_tab\\u003c-read.table(\\\"lulu_motu_table_18S.txt\\\",header=T,check.names=F,stringsAsFactors = F, sep=\\\"\\\\t\\\")\\n\\n# Sort the count table based on the order in the tax table and combine both tables\\n\\nmotu_tab\\u003c-motu_tab[order(match(motu_tab[,1],tax_table[,1])),]\\n\\nmotu_tax_tab\\u003c-as.data.frame(cbind(tax_table,motu_tab[,-1]),stringsAsFactors=F)\\n\\n# Write this MOTU table to file\\n\\nwrite.table(motu_tax_tab,\\\"18S_motu_table_tax_counts_species_fullname.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\n## Aggregate MOTUs with same species assignment, adding read counts of same-species MOTUs to the most abundant same-species MOTU. ##\\n\\n# Sort motu_tax_tab by read abundance \\n\\nmotu_tax_tab \\u003c- motu_tax_tab[order(rowSums(motu_tax_tab[,12:ncol(motu_tax_tab)]),decreasing=TRUE),]\\n\\n# Create a mapping file for subsequent downstream analysis to track which MOTUs are going to be merged.\\n\\nmotu_map\\u003c-aggregate(MOTU ~ Species, data = motu_tax_tab, paste, collapse = \\\",\\\")\\nwrite.table(motu_map,\\\"motu_map_identical_species.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\n# Aggregate rows by species entries and sum read counts.\\n\\ntax_sum\\u003c-aggregate(.~ motu_tax_tab$Species, data = motu_tax_tab[,12:ncol(motu_tax_tab)], sum)\\n\\n# Combine this table with the mapping table\\n\\nmotu_unique\\u003c-as.data.frame(cbind(motu_map,tax_sum[,-1]))\\n\\n# Where same-species MOTUs have been merged, we only want to keep the MOTU ID of the most abundant one.\\n# Where ID entries with aggregated MOTUs exist, the first entry equals the most abundant MOTU so we delete the characters after (and incl.) the comma.\\n\\nmotu_unique$MOTU\\u003c-gsub(\\\",.*\\\",\\\"\\\",motu_unique$MOTU)\\n\\n# Filter the previously generated motu_tax_tab table to only contain the MOTUs now listed in the motu_unique table.\\n\\nmotu_tax_tab_uniq\\u003c-motu_tax_tab[motu_tax_tab$MOTU %in% motu_unique$MOTU,]\\n\\n# Order the motu_unique table to match the MOTU ID order of motu_tax_tab_uniq and create a final MOTU table with the taxonomy info of motu_tax_tab_uniq and the summed up read counts of motu_unique \\n\\nmotu_unique\\u003c-motu_unique[order(match(motu_unique[,2],motu_tax_tab_uniq[,1])),]\\nfinal_motu_tab\\u003c-as.data.frame(cbind(motu_tax_tab_uniq[,1:11],motu_unique[,-c(1,2)]))\\n\\n# Combine it with the entries that were not part of the aggregation procedure, e.g. MOTUs with no species level assignment\\n\\nfinal_motu_tab\\u003c-rbind(final_motu_tab,motu_tax_tab[is.na(motu_tax_tab$Species),])\\n\\n# write to file\\n\\nwrite.table(final_motu_tab,\\\"18S_final_motu_table_merged_species.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\n# Write count table to file for phyloseq processing\\n\\nwrite.table(final_motu_tab[,c(1,12:ncol(final_motu_tab))],\\\"18S_motu_count_table_merged_species.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\n# Write taxonomy table to file for phyloseq processing\\n\\nwrite.table(final_motu_tab[,1:11],\\\"18S_motu_tax_table_merged_species.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\",\"os_name\":null,\"os_version\":null}}}}","data":null,"protocol_id":86559,"case_id":0,"critical_ids":"","duration":0,"original_id":1163089,"number":"10","cases":[],"critical":null},{"id":1766866,"guid":"E044E0A13C5411EE882A0A58A9FEAC02","previous_id":1766867,"previous_guid":"E0452E513C5411EE882A0A58A9FEAC02","section":"\u003cp\u003eRemoving nuclear mitochondrial DNA pseudogenes (nuMTs) for COI using \u003cem\u003eMACSE\u003c/em\u003e\u003c/p\u003e","section_color":"#EA9F6C","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"d9n2c\",\"text\":\"Download latest release of MACSE from here: https://bioweb.supagro.inra.fr/macse/index.php?menu=releases. We used MACSE v2.05.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":27,\"length\":5},{\"style\":\"italic\",\"offset\":114,\"length\":5}],\"entityRanges\":[{\"key\":0,\"offset\":44,\"length\":60}],\"data\":{}},{\"key\":\"alkk3\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"9dsa0\",\"text\":\"To identify putative numts, we aligned our COI non-singleton ASVs generated with dada2 against a pre-aligned set of COI sequences. The developers of MACSE have generated alignments of COI sequences derived from the BOLD database for particular taxonomic groups. To create overall reference alignments for marine taxa with amino acid translations corresponding to the NCBI genetic codes (see https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi), we downloaded these alignment fasta files for the reference sequence alignments (ref.Align fasta files) of most groups known to include marine taxa from here https://bioweb.supagro.inra.fr/macse/index.php?menu=download_Barcoding (save target as...). The refAlign.fasta files are curated alignments of reference sequences which represent the sequence diversity of the respective taxonomic group. The refAlign.fasta files for the following taxonomic groups were downloaded (refSeq.fasta for groups lackign a refAlign.fasta):\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":81,\"length\":5},{\"style\":\"italic\",\"offset\":149,\"length\":5},{\"style\":\"italic\",\"offset\":215,\"length\":4},{\"style\":\"italic\",\"offset\":367,\"length\":4}],\"entityRanges\":[{\"key\":1,\"offset\":391,\"length\":56},{\"key\":2,\"offset\":608,\"length\":70}],\"data\":{}},{\"key\":\"4ugb4\",\"text\":\" \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"dqrt3\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":3,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"em9ac\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"56scq\",\"text\":\"Subsequently, the alignments of the various taxonomic groups were merged based on their respective genetic code. We used MACSE's alignTwoProfiles function for this. After executing the macse_vX.XX.jar file downloaded as detailed above (Java needs to be installed on your machine to run MACSE), a GUI opens up. Navigate to Programs and choose alignTwoProfiles. Under ALL OPTIONS, set the respective genetic code for gc_def (not necessary for Tunicata, genetic code 13: only one taxonomic group with one refAlign.fasta):\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":121,\"length\":5},{\"style\":\"italic\",\"offset\":129,\"length\":16},{\"style\":\"italic\",\"offset\":185,\"length\":15},{\"style\":\"italic\",\"offset\":236,\"length\":4},{\"style\":\"italic\",\"offset\":286,\"length\":5},{\"style\":\"italic\",\"offset\":322,\"length\":8},{\"style\":\"italic\",\"offset\":343,\"length\":15},{\"style\":\"italic\",\"offset\":366,\"length\":11},{\"style\":\"italic\",\"offset\":415,\"length\":7}],\"entityRanges\":[],\"data\":{}},{\"key\":\"h6nd\",\"text\":\" \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"ahobc\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":4,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"45s8k\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"b0j85\",\"text\":\"Then choose two of the refAlign.fasta files for p1 and p2. We didn't change the other default settings. Then, execute run alignTwoProfiles. As an example, for NCBI genetic code 2 (vertebrates), we ran the procedure with the files Actinopterygii_BOLD_COI_final_align_NT.aln and Chondrichthyes_BOLD_COI_final_align_NT.aln as p1 and p2. This resulted in two alignment files, one on nucleotide and one on amino acid level:  \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":48,\"length\":2},{\"style\":\"italic\",\"offset\":55,\"length\":2},{\"style\":\"italic\",\"offset\":118,\"length\":20},{\"style\":\"italic\",\"offset\":159,\"length\":4},{\"style\":\"italic\",\"offset\":230,\"length\":42},{\"style\":\"italic\",\"offset\":277,\"length\":42},{\"style\":\"italic\",\"offset\":323,\"length\":2},{\"style\":\"italic\",\"offset\":330,\"length\":2}],\"entityRanges\":[],\"data\":{}},{\"key\":\"dkff8\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"cr4bi\",\"text\":\"Actinopterygii_BOLD_COI_final_align_Chondrichthyes_BOLD_COI_final_align_AA.fasta\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":0,\"length\":80}],\"entityRanges\":[],\"data\":{}},{\"key\":\"a2r5u\",\"text\":\"Actinopterygii_BOLD_COI_final_align_Chondrichthyes_BOLD_COI_final_align_NT.fasta\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":0,\"length\":80}],\"entityRanges\":[],\"data\":{}},{\"key\":\"9iggu\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7p2a4\",\"text\":\"Subsequently, the next refAlign.fasta was added to the previously generated _NT.fasta file by running alignTwoProfiles again. This time, the _NT.fasta was used as p1 and e.g. the Sarcopterygii_BOLD_COI_final_align_NT.aln file as p2. The next refAlign.fasta file was added again to the resulting _NT.fasta with the same procedure, and so on. This was continued until an overall alignment for the respective taxonomic group belonging to a particular genetic code was achieved (not necessary for Tunicata, genetic code 13: only one taxonomic group with one refAlig.fasta). Resulting alignment files were named MACSE_BOLD_gcXX_marine_taxa_aligned_NT.fasta and can be found below:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":102,\"length\":16},{\"style\":\"italic\",\"offset\":163,\"length\":2},{\"style\":\"italic\",\"offset\":179,\"length\":41},{\"style\":\"italic\",\"offset\":229,\"length\":2},{\"style\":\"italic\",\"offset\":607,\"length\":45}],\"entityRanges\":[],\"data\":{}},{\"key\":\"1u1l8\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"eotud\",\"text\":\"  \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":0,\"length\":1}],\"entityRanges\":[{\"key\":5,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"c522f\",\"text\":\"  \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":6,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"f3i9p\",\"text\":\"  \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":7,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"3viq1\",\"text\":\"  \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":8,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"30h0m\",\"text\":\"  \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":9,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"4pkvr\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"ft3gq\",\"text\":\"To align our ASVs against these reference alignments using MACSE's enrichAlignment function, we ran MACSE from within R. The script below was run on an HPC cluster. A directory called pseudo containing the directories gc2, gc4, gc5, gc9 and gc13 was created prior to running the R script. In this script, the input fasta containing the ASVs was first split into smaller fractions to enable alignment computation. The process was first completed for genetic code 5, as most of the ASVs most likely represented invertebrate taxa.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":59,\"length\":5},{\"style\":\"italic\",\"offset\":67,\"length\":15},{\"style\":\"italic\",\"offset\":100,\"length\":5},{\"style\":\"italic\",\"offset\":118,\"length\":1},{\"style\":\"italic\",\"offset\":184,\"length\":6},{\"style\":\"italic\",\"offset\":218,\"length\":18},{\"style\":\"italic\",\"offset\":241,\"length\":4},{\"style\":\"italic\",\"offset\":279,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"9d8of\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":10,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"b4h68\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"84f5d\",\"text\":\" \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"6k9ov\",\"text\":\"The final gc13_pseudo.fasta contains the overall putative numt-ASV set after running MACSE with five translations (one for each genetic code). The resulting nonpseudo.combined.names.txt file contains the non-numt ASVs (in \\\"\\u003eASVxy\\\" format). \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":10,\"length\":17},{\"style\":\"italic\",\"offset\":85,\"length\":5}],\"entityRanges\":[],\"data\":{}},{\"key\":\"ek4rp\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"fa91m\",\"text\":\"Non-numt ASVs were then subset from the COI_nochim_nosingle.fa file (resulting from the dada2 workflow, see above) by running the following in command line:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":40,\"length\":22},{\"style\":\"italic\",\"offset\":88,\"length\":5}],\"entityRanges\":[],\"data\":{}},{\"key\":\"c6pbc\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":11,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"a86k2\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{\"0\":{\"type\":\"link\",\"mutability\":\"\",\"data\":{\"guid\":\"7CC2511240B911ECB1B50A58A9FEAC02\",\"url\":\"https://bioweb.supagro.inra.fr/macse/index.php?menu=releases\"}},\"1\":{\"type\":\"link\",\"mutability\":\"\",\"data\":{\"guid\":\"5786202140BB11ECB1B50A58A9FEAC02\",\"url\":\"https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi\"}},\"10\":{\"type\":\"command\",\"mutability\":\"\",\"data\":{\"can_edit\":true,\"command_name\":\"MACSE alignment and pseudogene (numt) identification in R\",\"description\":null,\"guid\":\"47C4270C341F11EE9D4302C0B41BC903\",\"name\":\"library(Biostrings)\\n# install.packages(\\\"remotes\\\") \\n# remotes::install_github(\\\"malnirav/hiReadsProcessor\\\")\\nlibrary(hiReadsProcessor)\\nlibrary(dplyr)\\nlibrary(seqinr)\\n\\n### Run MACSE for genetic code 5 first \\n\\npath \\u003c- \\\"~/pseudo/gc5\\\" \\npath.cut \\u003c- file.path(path, \\\"splitseqs\\\")\\nif(!dir.exists(path.cut)) dir.create(path.cut)\\nx \\u003c- readDNAStringSet(\\\"~/pseudo/COI_nochim_nosingle_ASVs.fa\\\")\\n\\n# Split the fasta file into 20 fractions. \\n# Make sure the input fasta is found in the respective directory\\n\\nsplitSeqsToFiles(x, 20, \\\"fasta\\\",\\\"splitseqs\\\",\\\"~/pseudo/gc5/splitseqs\\\")\\n\\n# Make list of the file names of these split sequences\\n\\npath \\u003c- \\\"~/pseudo/gc5/splitseqs\\\"\\nsplit.files \\u003c- sort(list.files(path, pattern = \\\".fasta\\\", full.names = TRUE))\\n\\n# Create some output file names to use for MACSE\\n\\nget.sample.name \\u003c- function(fname) strsplit(basename(fname), \\\".fasta\\\")[[1]][1]\\nsample.names \\u003c- unname(sapply(split.files, get.sample.name))\\nsample.names\\noutputAA \\u003c- file.path(paste0(path, sample.names, \\\"_AA_gc5.fa\\\"))\\noutputNT \\u003c- file.path(paste0(path, sample.names, \\\"_NT_gc5.fa\\\"))\\noutputstats \\u003c- file.path(paste0(path, sample.names, \\\"_stats_gc5.csv\\\"))\\n\\n# Run MACSE with enrichAlignment for genetic code 5\\n# Make sure the .jar file of MACSE and the respective reference alignment -NT.fasta are found in the repsective directories specified in the command\\n# Do not allow any in-frame stop codons, frameshifts or insertions and a maximum of 3 in-frame deletions (on amino acid level) in the enrichAlignment procedure.\\n\\nfor(i in seq_along(split.files)) {\\n  system2(\\\"java\\\", args = c(\\\" -jar ~/pseudo/macse_v2.05.jar -prog enrichAlignment -align ~/pseudo/MACSE_BOLD_gc5_marine_taxa_aligned_NT.fasta -seq \\\", split.files[i], \\\"-gc_def 5 -maxSTOP_inSeq 0 -output_only_added_seq_ON =TRUE -fixed_alignment_ON =TRUE -maxDEL_inSeq 3 -maxFS_inSeq 0 -maxINS_inSeq 0  -out_AA \\\", outputAA[i], \\\" -out_NT \\\", outputNT[i], \\\" -out_tested_seq_info \\\", outputstats[i]))\\n}\\n\\n# Make a table for pseudo and nonpseudo sequences.\\n# This is possible as running enrichAlignment will produce _stats.csv files for each alignment. These files show if an ASV has been added to the reference alignment under the specified settings regime. \\n\\npath \\u003c- \\\"~/pseudo/gc5\\\"\\nsplit.files.res \\u003c- sort(list.files(path, pattern = \\\"_stats_gc5.csv\\\", full.names = TRUE))\\nnonpseudo_gc5 = data.frame()\\nfor(i in seq_along(split.files.res)){\\n  splitseqres\\u003c-read.table(split.files.res[i], h=T, sep=\\\";\\\")\\n  splitseqres1 \\u003c- splitseqres %\\u003e%\\n    filter(added == \\\"yes\\\")\\n  df \\u003c- data.frame(splitseqres1)\\n  nonpseudo_gc5 \\u003c- rbind(nonpseudo_gc5,df)\\n}\\npseudo_gc5 = data.frame()\\nfor(i in seq_along(split.files.res)){\\n  splitseqres\\u003c-read.table(split.files.res[i], h=T, sep=\\\";\\\")\\n  splitseqres1 \\u003c- splitseqres %\\u003e%\\n    filter(added == \\\"no\\\")\\n  df \\u003c- data.frame(splitseqres1)\\n  pseudo_gc5 \\u003c- rbind(pseudo_gc5,df)\\n}\\n\\n# Subset those that were pseudogenes into a new fasta file\\n\\nfastafile \\u003c- read.fasta(\\\"~/pseudo/COI_nochim_nosingle_ASVs.fa\\\", seqtype=\\\"DNA\\\", as.string=TRUE)\\nfastafile1 \\u003c- fastafile[c(which(names(fastafile) %in% pseudo_gc5$name))]\\nwrite.fasta(fastafile1, names=names(fastafile1), file.out = \\\"~/pseudo/gc5/gc5_pseudo.fasta\\\")\\n\\n### Run MACSE for genetic code 4\\n\\n# Read the previously created fasta file with the putative pseudogenes for genetic code 5 back into R and align these sequences with genetic code 4 to see if some of these may not be identified as pseudogenes with this translation table.\\n\\npath \\u003c- \\\"~/pseudo/gc4\\\" \\npath.cut \\u003c- file.path(path, \\\"splitseqs\\\")\\nif(!dir.exists(path.cut)) dir.create(path.cut)\\nx \\u003c- readDNAStringSet(\\\"~/pseudo/gc5/gc5_pseudo.fasta\\\")\\n\\n# Split this file into smaller fractions\\n\\nsplitSeqsToFiles(x, 5, \\\"split.fasta\\\",\\\"splitseqs\\\", \\\"~/pseudo/gc4/splitseqs\\\")\\n\\npath \\u003c- \\\"~/pseudo/gc4/splitseqs\\\"\\nsplit.files \\u003c- sort(list.files(path, pattern = \\\".fasta\\\", full.names = TRUE))\\n\\n# Create some output file names to use for MACSE\\n\\nget.sample.name \\u003c- function(fname) strsplit(basename(fname), \\\".fasta\\\")[[1]][1]\\nsample.names \\u003c- unname(sapply(split.files, get.sample.name))\\nsample.names\\noutputAA \\u003c- file.path(paste0(path, sample.names, \\\"_AA_gc4.fa\\\"))\\noutputNT \\u003c- file.path(paste0(path, sample.names, \\\"_NT_gc4.fa\\\"))\\noutputstats \\u003c- file.path(paste0(path, sample.names, \\\"_stats_gc4.csv\\\"))\\n\\n# Run MACSE with enrichAlignment for genetic code 4\\n# Make sure the .jar file of MACSE and the respective reference alignment -NT.fasta are found in the repsective directories specified in the command\\n# Do not allow any in-frame stop codons, frameshifts or insertions and a maximum of 3 in-frame deletions (on amino acid level) in the enrichAlignment procedure.\\n\\nfor(i in seq_along(split.files)) {\\n  system2(\\\"java\\\", args = c(\\\" -jar ~/pseudo/macse_v2.05.jar -prog enrichAlignment -align ~//pseudo/MACSE_BOLD_gc4_marine_taxa_aligned_NT.fasta -seq \\\", split.files[i], \\\"-gc_def 4 -maxSTOP_inSeq 0 -output_only_added_seq_ON =TRUE -fixed_alignment_ON =TRUE -maxDEL_inSeq 3 -maxFS_inSeq 0 -maxINS_inSeq 0  -out_AA \\\", outputAA[i], \\\" -out_NT \\\", outputNT[i], \\\" -out_tested_seq_info \\\", outputstats[i]))\\n}\\n\\n# Make a table for pseudo and nonpseudo sequences.\\n# This is possible as running enrichAlignment will produce _stats.csv files for each alignment. These files show if an ASV has been added to the reference alignment under the specified settings regime. \\n\\npath \\u003c- \\\"~/pseudo/gc4\\\"\\nsplit.files.res \\u003c- sort(list.files(path, pattern = \\\"_stats_gc4.csv\\\", full.names = TRUE))\\nnonpseudo_gc4 = data.frame()\\nfor(i in seq_along(split.files.res)){\\n  splitseqres\\u003c-read.table(split.files.res[i], h=T, sep=\\\";\\\")\\n  splitseqres1 \\u003c- splitseqres %\\u003e%\\n    filter(added == \\\"yes\\\")\\n  df \\u003c- data.frame(splitseqres1)\\n  nonpseudo_gc4 \\u003c- rbind(nonpseudo_gc4,df)\\n}\\npseudo_gc4 = data.frame()\\nfor(i in seq_along(split.files.res)){\\n  splitseqres\\u003c-read.table(split.files.res[i], h=T, sep=\\\";\\\")\\n  splitseqres1 \\u003c- splitseqres %\\u003e%\\n    filter(added == \\\"no\\\")\\n  df \\u003c- data.frame(splitseqres1)\\n  pseudo_gc4 \\u003c- rbind(pseudo_gc4,df)\\n}\\n\\n# Subset those that were pseudogenes into a new fasta file\\n\\nfastafile \\u003c- read.fasta(\\\"~/pseudo/COI_nochim_nosingle_ASVs.fa\\\", seqtype=\\\"DNA\\\", as.string=TRUE)\\nfastafile1 \\u003c- fastafile[c(which(names(fastafile) %in% pseudo_gc4$name))]\\nwrite.fasta(fastafile1, names=names(fastafile1), file.out = \\\"~/pseudo/gc4/gc4_pseudo.fasta\\\")\\n\\n### Run MACSE for genetic code 2\\n\\n# Read the previously created fasta file with the putative pseudogenes for genetic code 4 back into R and align these sequences with genetic code 2 to see if some of these may not be identified as pseudogenes with this translation table.\\n\\npath \\u003c- \\\"~/pseudo/gc2\\\" \\npath.cut \\u003c- file.path(path, \\\"splitseqs\\\")\\nif(!dir.exists(path.cut)) dir.create(path.cut)\\nx \\u003c- readDNAStringSet(\\\"~/pseudo/gc4/gc4_pseudo.fasta\\\")\\n\\n# Split it into smaller fractions\\n\\nsplitSeqsToFiles(x, 2, \\\"split.fasta\\\",\\\"splitseqs\\\", \\\"~/pseudo/gc2/splitseqs\\\")\\n\\npath \\u003c- \\\"~/pseudo/gc2/splitseqs\\\"\\nsplit.files \\u003c- sort(list.files(path, pattern = \\\".fasta\\\", full.names = TRUE))\\n\\n# Create some output file names to use for MACSE\\n\\nget.sample.name \\u003c- function(fname) strsplit(basename(fname), \\\".fasta\\\")[[1]][1]\\nsample.names \\u003c- unname(sapply(split.files, get.sample.name))\\nsample.names\\noutputAA \\u003c- file.path(paste0(path, sample.names, \\\"_AA_gc2.fa\\\"))\\noutputNT \\u003c- file.path(paste0(path, sample.names, \\\"_NT_gc2.fa\\\"))\\noutputstats \\u003c- file.path(paste0(path, sample.names, \\\"_stats_gc2.csv\\\"))\\n\\n# Run MACSE with enrichAlignment for genetic code 2\\n# Make sure the .jar file of MACSE and the respective reference alignment -NT.fasta are found in the repsective directories specified in the command\\n# Do not allow any in-frame stop codons, frameshifts or insertions and a maximum of 3 in-frame deletions (on amino acid level) in the enrichAlignment procedure.\\n\\n\\nfor(i in seq_along(split.files)) {\\n  system2(\\\"java\\\", args = c(\\\" -jar ~/pseudo/macse_v2.05.jar -prog enrichAlignment -align ~//pseudo/MACSE_BOLD_gc2_marine_taxa_aligned_NT.fasta -seq \\\", split.files[i], \\\"-gc_def 2 -maxSTOP_inSeq 0 -output_only_added_seq_ON =TRUE -fixed_alignment_ON =TRUE -maxDEL_inSeq 3 -maxFS_inSeq 0 -maxINS_inSeq 0  -out_AA \\\", outputAA[i], \\\" -out_NT \\\", outputNT[i], \\\" -out_tested_seq_info \\\", outputstats[i]))\\n}\\n\\n# Make a table for pseudo and nonpseudo sequences.\\n# This is possible as running enrichAlignment will produce _stats.csv files for each alignment. These files show if an ASV has been added to the reference alignment under the specified settings regime.\\n\\npath \\u003c- \\\"~/pseudo/gc2\\\"\\nsplit.files.res \\u003c- sort(list.files(path, pattern = \\\"_stats_gc2.csv\\\", full.names = TRUE))\\nnonpseudo_gc2 = data.frame()\\nfor(i in seq_along(split.files.res)){\\n  splitseqres\\u003c-read.table(split.files.res[i], h=T, sep=\\\";\\\")\\n  splitseqres1 \\u003c- splitseqres %\\u003e%\\n    filter(added == \\\"yes\\\")\\n  df \\u003c- data.frame(splitseqres1)\\n  nonpseudo_gc2 \\u003c- rbind(nonpseudo_gc2,df)\\n}\\npseudo_gc2 = data.frame()\\nfor(i in seq_along(split.files.res)){\\n  splitseqres\\u003c-read.table(split.files.res[i], h=T, sep=\\\";\\\")\\n  splitseqres1 \\u003c- splitseqres %\\u003e%\\n    filter(added == \\\"no\\\")\\n  df \\u003c- data.frame(splitseqres1)\\n  pseudo_gc2 \\u003c- rbind(pseudo_gc2,df)\\n}\\n\\n# Subset those that were pseudogenes into a new fasta file\\n\\nfastafile \\u003c- read.fasta(\\\"~/pseudo/COI_nochim_nosingle_ASVs.fa\\\", seqtype=\\\"DNA\\\", as.string=TRUE)\\nfastafile1 \\u003c- fastafile[c(which(names(fastafile) %in% pseudo_gc2$name))]\\nwrite.fasta(fastafile1, names=names(fastafile1), file.out = \\\"~/pseudo/gc2/gc2_pseudo.fasta\\\")\\n\\n### Run MACSE for genetic code 9\\n\\n# Read the previously created fasta file with the putative pseudogenes for genetic code 2 back into R and align these sequences with genetic code 9 to see if some of these may not be identified as pseudogenes with this translation table.\\n\\n# Make list of the file names of these split sequences\\n\\npath \\u003c- \\\"~/pseudo/gc9\\\" \\npath.cut \\u003c- file.path(path, \\\"splitseqs\\\")\\nif(!dir.exists(path.cut)) dir.create(path.cut)\\nx \\u003c- readDNAStringSet(\\\"~/pseudo/gc2/gc2_pseudo.fasta\\\")\\n\\n# Split it into smaller fractions\\n\\nsplitSeqsToFiles(x, 2, \\\"split.fasta\\\",\\\"splitseqs\\\", \\\"~/pseudo/gc9/splitseqs\\\")\\n\\npath \\u003c- \\\"~/pseudo/gc9/splitseqs\\\"\\nsplit.files \\u003c- sort(list.files(path, pattern = \\\".fasta\\\", full.names = TRUE))\\n\\n# Create some output file names to use for MACSE\\n\\nget.sample.name \\u003c- function(fname) strsplit(basename(fname), \\\".fasta\\\")[[1]][1]\\nsample.names \\u003c- unname(sapply(split.files, get.sample.name))\\nsample.names\\noutputAA \\u003c- file.path(paste0(path, sample.names, \\\"_AA_gc9.fa\\\"))\\noutputNT \\u003c- file.path(paste0(path, sample.names, \\\"_NT_gc9.fa\\\"))\\noutputstats \\u003c- file.path(paste0(path, sample.names, \\\"_stats_gc9.csv\\\"))\\n\\n# Run MACSE with enrichAlignment for genetic code 9\\n# Make sure the .jar file of MACSE and the respective reference alignment -NT.fasta are found in the repsective directories specified in the command\\n# Do not allow any in-frame stop codons, frameshifts or insertions and a maximum of 3 in-frame deletions (on amino acid level) in the enrichAlignment procedure.\\n\\nfor(i in seq_along(split.files)) {\\n  system2(\\\"java\\\", args = c(\\\" -jar ~/pseudo/macse_v2.05.jar -prog enrichAlignment -align ~//pseudo/MACSE_BOLD_gc9_marine_taxa_aligned_NT.fasta -seq \\\", split.files[i], \\\"-gc_def 9 -maxSTOP_inSeq 0 -output_only_added_seq_ON =TRUE -fixed_alignment_ON =TRUE -maxDEL_inSeq 3 -maxFS_inSeq 0 -maxINS_inSeq 0  -out_AA \\\", outputAA[i], \\\" -out_NT \\\", outputNT[i], \\\" -out_tested_seq_info \\\", outputstats[i]))\\n}\\n\\n# Make a table for pseudo and nonpseudo sequences.\\n# This is possible as running enrichAlignment will produce _stats.csv files for each alignment. These files show if an ASV has been added to the reference alignment under the specified settings regime.\\n\\npath \\u003c- \\\"~/pseudo/gc9\\\"\\nsplit.files.res \\u003c- sort(list.files(path, pattern = \\\"_stats_gc9.csv\\\", full.names = TRUE))\\nnonpseudo_gc9 = data.frame()\\nfor(i in seq_along(split.files.res)){\\n  splitseqres\\u003c-read.table(split.files.res[i], h=T, sep=\\\";\\\")\\n  splitseqres1 \\u003c- splitseqres %\\u003e%\\n    filter(added == \\\"yes\\\")\\n  df \\u003c- data.frame(splitseqres1)\\n  nonpseudo_gc9 \\u003c- rbind(nonpseudo_gc9,df)\\n}\\npseudo_gc9 = data.frame()\\nfor(i in seq_along(split.files.res)){\\n  splitseqres\\u003c-read.table(split.files.res[i], h=T, sep=\\\";\\\")\\n  splitseqres1 \\u003c- splitseqres %\\u003e%\\n    filter(added == \\\"no\\\")\\n  df \\u003c- data.frame(splitseqres1)\\n  pseudo_gc9 \\u003c- rbind(pseudo_gc9,df)\\n}\\n\\n# Subset those that were pseudogenes into a new fasta file\\n\\nfastafile \\u003c- read.fasta(\\\"~/pseudo/COI_nochim_nosingle_ASVs.fa\\\", seqtype=\\\"DNA\\\", as.string=TRUE)\\nfastafile1 \\u003c- fastafile[c(which(names(fastafile) %in% pseudo_gc9$name))]\\nwrite.fasta(fastafile1, names=names(fastafile1), file.out = \\\"~/pseudo/gc9/gc9_pseudo.fasta\\\")\\n\\n### Run MACSE for genetic code 13\\n\\n# Read the previously created fasta file with the putative pseudogenes for genetic code 9 back into R and align these sequences with genetic code 13 to see if some of these may not be identified as pseudogenes with this translation table.\\n\\npath \\u003c- \\\"~/pseudo/gc13\\\" \\npath.cut \\u003c- file.path(path, \\\"splitseqs\\\")\\nif(!dir.exists(path.cut)) dir.create(path.cut)\\nx \\u003c- readDNAStringSet(\\\"~/pseudo/gc9/gc9_pseudo.fasta\\\")\\n\\n# Split it into smaller fractions \\n\\nsplitSeqsToFiles(x, 2, \\\"split.fasta\\\",\\\"splitseqs\\\", \\\"~/pseudo/gc13/splitseqs\\\")\\n\\npath \\u003c- \\\"~/pseudo/gc13/splitseqs\\\"\\nsplit.files \\u003c- sort(list.files(path, pattern = \\\".fasta\\\", full.names = TRUE))\\n\\n# Create some output file names to use for MACSE\\n\\nget.sample.name \\u003c- function(fname) strsplit(basename(fname), \\\".fasta\\\")[[1]][1]\\nsample.names \\u003c- unname(sapply(split.files, get.sample.name))\\nsample.names\\noutputAA \\u003c- file.path(paste0(path, sample.names, \\\"_AA_gc13.fa\\\"))\\noutputNT \\u003c- file.path(paste0(path, sample.names, \\\"_NT_gc13.fa\\\"))\\noutputstats \\u003c- file.path(paste0(path, sample.names, \\\"_stats_gc13.csv\\\"))\\n\\n# Run MACSE with enrichAlignment for genetic code 13\\n# Make sure the .jar file of MACSE and the respective reference alignment -NT.fasta are found in the repsective directories specified in the command\\n# Do not allow any in-frame stop codons, frameshifts or insertions and a maximum of 3 in-frame deletions (on amino acid level) in the enrichAlignment procedure.\\n\\nfor(i in seq_along(split.files)) {\\n  system2(\\\"java\\\", args = c(\\\" -jar ~/pseudo/macse_v2.05.jar -prog enrichAlignment -align ~//pseudo/MACSE_BOLD_Tunicata_gc13_aligned_NT.fasta -seq \\\", split.files[i], \\\"-gc_def 13 -maxSTOP_inSeq 0 -output_only_added_seq_ON =TRUE -fixed_alignment_ON =TRUE -maxDEL_inSeq 3 -maxFS_inSeq 0 -maxINS_inSeq 0  -out_AA \\\", outputAA[i], \\\" -out_NT \\\", outputNT[i], \\\" -out_tested_seq_info \\\", outputstats[i]))\\n}\\n\\n# Make a table for pseudo and nonpseudo sequences\\n\\npath \\u003c- \\\"~/pseudo/gc13\\\"\\nsplit.files.res \\u003c- sort(list.files(path, pattern = \\\"_stats_gc13.csv\\\", full.names = TRUE))\\nnonpseudo_gc13 = data.frame()\\nfor(i in seq_along(split.files.res)){\\n  splitseqres\\u003c-read.table(split.files.res[i], h=T, sep=\\\";\\\")\\n  splitseqres1 \\u003c- splitseqres %\\u003e%\\n    filter(added == \\\"yes\\\")\\n  df \\u003c- data.frame(splitseqres1)\\n  nonpseudo_gc13 \\u003c- rbind(nonpseudo_gc13,df)\\n}\\npseudo_gc13 = data.frame()\\nfor(i in seq_along(split.files.res)){\\n  splitseqres\\u003c-read.table(split.files.res[i], h=T, sep=\\\";\\\")\\n  splitseqres1 \\u003c- splitseqres %\\u003e%\\n    filter(added == \\\"no\\\")\\n  df \\u003c- data.frame(splitseqres1)\\n  pseudo_gc13 \\u003c- rbind(pseudo_gc13,df)\\n}\\n\\n# Make a table for pseudo and nonpseudo sequences.\\n# This is possible as running enrichAlignment will produce _stats.csv files for each alignment. These files show if an ASV has been added to the reference alignment under the specified settings regime.\\n\\nfastafile \\u003c- read.fasta(\\\"~/pseudo/COI_nochim_nosingle_ASVs.fa\\\", seqtype=\\\"DNA\\\", as.string=TRUE)\\nfastafile1 \\u003c- fastafile[c(which(names(fastafile) %in% pseudo_gc13$name))]\\nwrite.fasta(fastafile1, names=names(fastafile1), file.out = \\\"~/pseudo/gc13/gc13_pseudo.fasta\\\")\\n\\n# Output those ASVs that are potential pseudos and the ones that are not\\n\\npseudo.combined \\u003c- rbind(pseudo_gc5, pseudo_gc4,pseudo_gc2,pseudo_gc9,pseudo_gc13)\\npseudo.combined.names \\u003c- as.character(unique(pseudo.combined$name))\\npseudo.combined.names\\u003c-paste0(\\\"\\u003e\\\",pseudo.combined.names)\\nnonpseudo.combined \\u003c- rbind(nonpseudo_gc5, nonpseudo_gc4,nonpseudo_gc2,nonpseudo_gc9,nonpseudo_gc13)\\nnonpseudo.combined.names \\u003c- as.character(unique(nonpseudo.combined$name))\\nnonpseudo.combined.names\\u003c-paste0(\\\"\\u003e\\\",nonpseudo.combined.names)\\n\\nwrite.table(pseudo.combined.names, \\\"~/pseudo/pseudo.combined.names.txt\\\",row.names = F,col.names = F,quote = F)\\nwrite.table(nonpseudo.combined.names, \\\"~/pseudo/nonpseudo.combined.names.txt\\\",row.names = F,col.names = F,quote = F)\",\"os_name\":null,\"os_version\":null}},\"11\":{\"type\":\"command\",\"mutability\":\"\",\"data\":{\"can_edit\":true,\"command_name\":\"Subset non-numt ASVs\",\"description\":null,\"guid\":\"47C4487E341F11EE9D4302C0B41BC903\",\"name\":\"grep -w -A 1 -f nonpseudo.combined.names.txt COI_nochim_nosingle_ASVs.fa \\u003e COI_nochim_nosingle_nopseudo.fa --no-group-separator\",\"os_name\":null,\"os_version\":null}},\"2\":{\"type\":\"link\",\"mutability\":\"\",\"data\":{\"guid\":\"E1F3B2EA40BB11ECB1B50A58A9FEAC02\",\"url\":\"https://bioweb.supagro.inra.fr/macse/index.php?menu=download_Barcoding\"}},\"3\":{\"type\":\"tables\",\"mutability\":\"\",\"data\":{\"cellsMeta\":{\"0\":{\"0\":\"A\"},\"1\":{\"0\":\"_\"},\"2\":{\"0\":\"5\"},\"A_1\":{\"className\":\"_dt-h-center\"},\"A_2\":{\"className\":\"_dt-h-center\"},\"A_3\":{\"className\":\"_dt-h-center\"},\"A_4\":{\"className\":\"_dt-h-center\"},\"A_5\":{\"className\":\"_dt-h-center\"},\"B_1\":{\"className\":\"_dt-h-center\"},\"B_2\":{\"className\":\"_dt-h-center\"},\"B_3\":{\"className\":\"_dt-h-center\"},\"B_4\":{\"className\":\"_dt-h-center\"},\"B_5\":{\"className\":\"_dt-h-center\"}},\"colTitles\":[\"Taxonomic groups\",\"NCBI genetic code\"],\"colWidths\":[\"365\",\"137\"],\"data\":[[\"Actinopterygii, Chondrichthyes, Cyclostomata, Sarcopterygii\",\"2\"],[\"Cnidaria, Porifera, Ctenophora\",\"4\"],[\"Mollusca, Malacostraca, Bryozoa, Annelida, Sipuncula, Nemertea, Chaetognatha, Rotifera, Acanthocephala, Kinorhyncha, Nematoda, Tardigrada, Pycnogonida, Ostracoda, Brachiopoda (refSeq.fasta instead of refAlign.fasta), Gastrotricha (refSeq.fasta instead of refAlign.fasta)\",\"5\"],[\"Echinodermata, Platyhelminthes, Hemichordata (refSeq.fasta instead of refAlign.fasta)\",\"9\"],[\"Tunicata\",\"13\"]],\"guid\":\"94A1792A40BC11ECB1B50A58A9FEAC02\",\"isJexcelDataFormat\":true,\"legend\":{\"blocks\":[{\"data\":{},\"depth\":0,\"entityRanges\":[],\"inlineStyleRanges\":[],\"key\":\"2ldc3\",\"text\":\"Taxonomic groups for which reference sequence alignments were downlaoded. These alignment fasta files were then merged based on the genetic codes for amino acid translation.  \",\"type\":\"unstyled\"}],\"entityMap\":{}},\"mergeCells\":{},\"printData\":[[\"Actinopterygii, Chondrichthyes, Cyclostomata, Sarcopterygii\",\"2\"],[\"Cnidaria, Porifera, Ctenophora\",\"4\"],[\"Mollusca, Malacostraca, Bryozoa, Annelida, Sipuncula, Nemertea, Chaetognatha, Rotifera, Acanthocephala, Kinorhyncha, Nematoda, Tardigrada, Pycnogonida, Ostracoda, Brachiopoda (refSeq.fasta instead of refAlign.fasta), Gastrotricha (refSeq.fasta instead of refAlign.fasta)\",\"5\"],[\"Echinodermata, Platyhelminthes, Hemichordata (refSeq.fasta instead of refAlign.fasta)\",\"9\"],[\"Tunicata\",\"13\"]],\"rowHeights\":[23,23,23,23,23],\"url\":\"https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi\"}},\"4\":{\"type\":\"tables\",\"mutability\":\"\",\"data\":{\"cellsMeta\":{\"0\":{\"0\":\"A\"},\"1\":{\"0\":\"_\"},\"2\":{\"0\":\"5\"},\"A_1\":{\"className\":\"_dt-h-center\"},\"A_2\":{\"className\":\"_dt-h-center\"},\"A_3\":{\"className\":\"_dt-h-center\"},\"A_4\":{\"className\":\"_dt-h-center\"},\"A_5\":{\"className\":\"_dt-h-center\"},\"B_1\":{\"className\":\"_dt-h-center\"},\"B_2\":{\"className\":\"_dt-h-center\"},\"B_3\":{\"className\":\"_dt-h-center\"},\"B_4\":{\"className\":\"_dt-h-center\"},\"B_5\":{\"className\":\"_dt-h-center\"}},\"colTitles\":[\"gc_def setting\",\"NCBI genetic code\"],\"colWidths\":[\"365\",\"137\"],\"data\":[[\"The_Vertebrate_Mitochondrial_Code\",\"2\"],[\"The_Mold_Protozoan_and_Coelenterate_Mitochondrial_Code_and_the_Mycoplasma_Sprioplasma_Code\",\"4\"],[\"The_Invertebrate_Mitochondrial_Code\",\"5\"],[\"The_Echinoderm_and_Flatworm_Mitochondrial_Code\",\"9\"]],\"guid\":\"CF4C760540BF11ECB1B50A58A9FEAC02\",\"isJexcelDataFormat\":true,\"legend\":{\"blocks\":[{\"data\":{},\"depth\":0,\"entityRanges\":[],\"inlineStyleRanges\":[{\"length\":16,\"offset\":46,\"style\":\"italic\"},{\"length\":5,\"offset\":68,\"style\":\"italic\"}],\"key\":\"2ldc3\",\"text\":\"Genetic code settings for gc_def when running alignTwoProfiles with MACSE (not necessary for Tunicata, genetic code 13: only one taxonomic group with one refAlig.fasta).\",\"type\":\"unstyled\"}],\"entityMap\":{}},\"mergeCells\":{},\"printData\":[[\"The_Vertebrate_Mitochondrial_Code\",\"2\"],[\"The_Mold_Protozoan_and_Coelenterate_Mitochondrial_Code_and_the_Mycoplasma_Sprioplasma_Code\",\"4\"],[\"The_Invertebrate_Mitochondrial_Code\",\"5\"],[\"The_Echinoderm_and_Flatworm_Mitochondrial_Code\",\"9\"]],\"rowHeights\":[23,23,23,23],\"url\":\"https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi\"}},\"5\":{\"type\":\"file\",\"mutability\":\"MUTABLE\",\"data\":{\"forceSelection\":true,\"guid\":\"\",\"id\":405126,\"original_name\":\"MACSE_BOLD_gc2_marine_taxa_aligned_NT.fasta\",\"placeholder\":\"\",\"raw_source\":\"\",\"size\":358252,\"source\":\"https://content.protocols.io/files/nmwgbimqf.fasta\"}},\"6\":{\"type\":\"file\",\"mutability\":\"MUTABLE\",\"data\":{\"forceSelection\":true,\"guid\":\"\",\"id\":405127,\"original_name\":\"MACSE_BOLD_gc4_marine_taxa_aligned_NT.fasta\",\"placeholder\":\"\",\"raw_source\":\"\",\"size\":250184,\"source\":\"https://content.protocols.io/files/nmwhbimqf.fasta\"}},\"7\":{\"type\":\"file\",\"mutability\":\"MUTABLE\",\"data\":{\"forceSelection\":true,\"guid\":\"\",\"id\":405128,\"original_name\":\"MACSE_BOLD_gc5_marine_taxa_aligned_NT.fasta\",\"placeholder\":\"\",\"raw_source\":\"\",\"size\":1305498,\"source\":\"https://content.protocols.io/files/nmwibimqf.fasta\"}},\"8\":{\"type\":\"file\",\"mutability\":\"MUTABLE\",\"data\":{\"forceSelection\":true,\"guid\":\"\",\"id\":405129,\"original_name\":\"MACSE_BOLD_gc9_marine_taxa_aligned_NT.fasta\",\"placeholder\":\"\",\"raw_source\":\"\",\"size\":335315,\"source\":\"https://content.protocols.io/files/nmwjbimqf.fasta\"}},\"9\":{\"type\":\"file\",\"mutability\":\"MUTABLE\",\"data\":{\"forceSelection\":true,\"guid\":\"\",\"id\":405130,\"original_name\":\"MACSE_BOLD_Tunicata_gc13_aligned_NT.fasta\",\"placeholder\":\"\",\"raw_source\":\"\",\"size\":71889,\"source\":\"https://content.protocols.io/files/nmwkbimqf.fasta\"}}}}","data":null,"protocol_id":86559,"case_id":0,"critical_ids":"","duration":0,"original_id":1242178,"number":"5","cases":[],"critical":null},{"id":1766867,"guid":"E0452E513C5411EE882A0A58A9FEAC02","previous_id":1768013,"previous_guid":"19AB30703D0E11EEBF2B812AFE632E09","section":"\u003cp\u003eLinearization of fasta files and problems with files in DOS CRLF format\u003c/p\u003e","section_color":"#FFED92","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"9gisp\",\"text\":\"For all downstream steps, please make sure all fasta files used as input are linearized, i.e., sequences are stored in a single line below a header, not in multiple lines. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4ko5u\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8af8k\",\"text\":\"If this is not the case for any of the fasta files, run the following in command line:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"ca49o\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":0,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"agvgi\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"esb1v\",\"text\":\"In case you are working with Windows OS and the result of any of the downstream command line operations is an empty fasta file, one of the input files may be formatted in Windows DOS CRLF format. Check if this is the case by opening the file using Editor (or any other text editing tools), then check at the bottom. You may convert the respective file to Unix LF format in command line as shown below: \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":29,\"length\":7},{\"style\":\"italic\",\"offset\":171,\"length\":7}],\"entityRanges\":[],\"data\":{}},{\"key\":\"d3ko8\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":1,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"dn6jj\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{\"0\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":1,\"command_name\":\"Remove line breaks\",\"description\":null,\"guid\":\"47C2D23F341F11EE9D4302C0B41BC903\",\"name\":\"awk '/^\\u003e/ { print (NR==1 ? \\\"\\\" : RS) $0; next } { printf \\\"%s\\\", $0 } END { printf RS }' input.fasta \\u003e tmp \\u0026\\u0026 mv tmp output.fasta\",\"os_name\":null,\"os_version\":null}},\"1\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Change file format from DOS CRLF to UNIX\",\"description\":\"\",\"guid\":\"D417665E3D0A11EEBF5D0A58A9FEAC02\",\"name\":\"dos2unix xy.file\",\"os_name\":\"\",\"os_version\":\"\"}}}}","data":null,"protocol_id":86559,"case_id":0,"critical_ids":"","duration":0,"original_id":1285430,"number":"4","cases":[],"critical":null},{"id":1766868,"guid":"F8FCA3903C5411EEA943654F37AFF3AC","previous_id":0,"previous_guid":null,"section":"\u003cp\u003eObtain ARMS-MBON 18S and COI metabarcoding data\u003c/p\u003e","section_color":"#A492FF","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"5vk5g\",\"text\":\"Information on all 18S and COI sequencing data of ARMS MBON available as of February 2024 were obtained from the respective files of the ARMS MBON data_workspace GitHub repo:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"f1mln\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"75hsq\",\"text\":\"https://github.com/arms-mbon/data_workspace/blob/main/qualitycontrolled_data/combined/combined_OmicsData.csv\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":0,\"offset\":0,\"length\":108}],\"data\":{}},{\"key\":\"9dka5\",\"text\":\"https://github.com/arms-mbon/data_workspace/blob/main/qualitycontrolled_data/combined/combined_SamplingEventData.csv\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":1,\"offset\":0,\"length\":116}],\"data\":{}},{\"key\":\"femn7\",\"text\":\"https://github.com/arms-mbon/data_workspace/blob/main/qualitycontrolled_data/combined/demultiplexing_details_OmicsData.csv\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":2,\"offset\":0,\"length\":122}],\"data\":{}},{\"key\":\"3o9oe\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8pmlk\",\"text\":\"The accession numbers for genetic data deposited on the European Nucleotide Archive (ENA) are found in the Gene_COI and Gene_18S columns of the combined_Omicsdata.csv file. Accession numbers of negative control samples for each sequencing run are found in columns Gene_COI_negative_control and Gene_18S_negative_control. Info on which accession stems from which sequencing run and how samples where demultiplexed post-sequencing are found in demultiplexing_details_OmicsData.csv. Based on these files, separate files for COI and 18S were generated containing all accession numbers of each marker gene, separated by sequencing runs. The COI and 18S files containing the respective ENA numbers are found below. As ENA accession numbers of Run_7 (i.e., the sequencing batch of August 2023) contained eight digits instead of seven digits as was the case for all previous sequencing runs, two separate files were generated for each gene:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":107,\"length\":8},{\"style\":\"italic\",\"offset\":120,\"length\":8},{\"style\":\"italic\",\"offset\":144,\"length\":22},{\"style\":\"italic\",\"offset\":264,\"length\":26},{\"style\":\"italic\",\"offset\":294,\"length\":26},{\"style\":\"italic\",\"offset\":442,\"length\":38}],\"entityRanges\":[],\"data\":{}},{\"key\":\"fs9k0\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"86om7\",\"text\":\"  \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":3,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"8b6tj\",\"text\":\"  \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":4,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"im52\",\"text\":\"  \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":5,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"co8d5\",\"text\":\"  \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":6,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"3trl6\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"9dsnr\",\"text\":\"All fastq.gz files were downloaded from ENA in February 2024. For runs listed in the XX_ENA_accessions.txt file (i.e., for accessions with seven digits), the R script below (ENADownload.R) was executed via command line using the .txt files as input :\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":158,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"20urn\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"77oho\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":7,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"cjbhk\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"a7jke\",\"text\":\"For sequencing runs listed in the XX_ENA_accessions_8digits.txt file (i.e., for accessions with eight digits), the R script below (ENADownload_8digits.R) was executed via command line using the _8digits.txt files as input :\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":115,\"length\":1},{\"style\":\"italic\",\"offset\":194,\"length\":12}],\"entityRanges\":[],\"data\":{}},{\"key\":\"chvmi\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":8,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"qvuf\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8j3vk\",\"text\":\"When running the download scripts via command line, -f points to file containing the ENA accession numbers separated by sequencing run and -d sets the directory path the fastq files will be downloaded to (create the fastq_files directories prior to this; the sub-directories for the fastq files of each sequencing run will be created automatically within the fastq_files directories):\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":50,\"length\":1},{\"style\":\"italic\",\"offset\":52,\"length\":2},{\"style\":\"italic\",\"offset\":139,\"length\":2},{\"style\":\"italic\",\"offset\":216,\"length\":11},{\"style\":\"italic\",\"offset\":359,\"length\":11}],\"entityRanges\":[],\"data\":{}},{\"key\":\"b56rr\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":9,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"dv7ci\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"f8h27\",\"text\":\"In case of problems, the download script will terminate with an error message. This was also case here. Sometimes, this is caused by issues when files on ENA are generated based on the originally submitted ones (see the different files available for each accession number under Submitted files: FTP and Generated FASTQ files: FTP). For us, this was the case for the 18S accession ERR7125542 (problem persisted as of February 2024, and it is not clear when ENA will fix this). There were no files under Generated FASTQ files: FTP for this accession. We removed this accession from 18S_ENA_accessions.txt (this accession is still included in the file provided above) and ran the download script again for 18S. Then, we manually downloaded the forward and reverse read files provided under Submitted files: FTP of this accession (https://www.ebi.ac.uk/ena/browser/view/ERR7125542). The downloaded files were renamed to ERR7125542_1.fastq.gz and ERR7125542_2.fastq.gz and placed in the respective directory with the other downloaded files of this sequencing run. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":278,\"length\":20},{\"style\":\"italic\",\"offset\":303,\"length\":26},{\"style\":\"italic\",\"offset\":502,\"length\":27},{\"style\":\"italic\",\"offset\":787,\"length\":21},{\"style\":\"italic\",\"offset\":904,\"length\":1}],\"entityRanges\":[{\"key\":10,\"offset\":827,\"length\":49}],\"data\":{}},{\"key\":\"3aq8a\",\"text\":\"There also seemed to be an incorrect read pairing in all of the 18S Generated FASTQ files (as of February 2024) of Run_1 (sequencing run July 2019). This became clear later on in the pipeline during primer removal using cutadapt (see below). We removed these files, manually downloaded all files of this run provided under Submitted files: FTP and manually renamed them with their accession numbers as described above for the other problematic 18S accession. The submitted files did not show the read-pairing issue.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":68,\"length\":21},{\"style\":\"italic\",\"offset\":220,\"length\":8},{\"style\":\"italic\",\"offset\":323,\"length\":21},{\"style\":\"italic\",\"offset\":463,\"length\":9}],\"entityRanges\":[],\"data\":{}},{\"key\":\"euums\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{\"0\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"4721DEDBB12011EE91AB0A58A9FEAC02\",\"url\":\"https://github.com/arms-mbon/data_workspace/blob/main/qualitycontrolled_data/combined/combined_OmicsData.csv\"}},\"1\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"549829E9B12111EE91AB0A58A9FEAC02\",\"url\":\"https://github.com/arms-mbon/data_workspace/blob/main/qualitycontrolled_data/combined/combined_SamplingEventData.csv\"}},\"10\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"90D2ADAEC5B811EE9AD20A58A9FEAC02\",\"url\":\"https://www.ebi.ac.uk/ena/browser/view/ERR7125542\"}},\"2\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"1B10E07FC5C711EE9AD20A58A9FEAC02\",\"url\":\"https://github.com/arms-mbon/data_workspace/blob/main/qualitycontrolled_data/combined/demultiplexing_details_OmicsData.csv\"}},\"3\":{\"type\":\"file\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"\",\"id\":447083,\"original_name\":\"COI_ENA_accessions.txt\",\"placeholder\":\"https://www.protocols.io/img/extensions/txt.png\",\"raw_source\":\"\",\"size\":3176,\"source\":\"https://content.protocols.io/files/pwvmbimqf.txt\"}},\"4\":{\"type\":\"file\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"\",\"id\":447084,\"original_name\":\"COI_ENA_accessions_8digits.txt\",\"placeholder\":\"https://www.protocols.io/img/extensions/txt.png\",\"raw_source\":\"\",\"size\":1280,\"source\":\"https://content.protocols.io/files/pwvnbimqf.txt\"}},\"5\":{\"type\":\"file\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"\",\"id\":447085,\"original_name\":\"18S_ENA_accessions.txt\",\"placeholder\":\"https://www.protocols.io/img/extensions/txt.png\",\"raw_source\":\"\",\"size\":3232,\"source\":\"https://content.protocols.io/files/pwvpbimqf.txt\"}},\"6\":{\"type\":\"file\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"\",\"id\":447086,\"original_name\":\"18S_ENA_accessions_8digits.txt\",\"placeholder\":\"https://www.protocols.io/img/extensions/txt.png\",\"raw_source\":\"\",\"size\":1282,\"source\":\"https://content.protocols.io/files/pwvqbimqf.txt\"}},\"7\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"ENADownload.R\",\"description\":\"\",\"guid\":\"E003804ADF8E11EEA7150A58A9FEAC02\",\"name\":\"#!/usr/bin/env Rscript\\n\\n# List of packages you will need\\nrequired_packages \\u003c- c('argparse')\\n\\n# Determine already installed packages\\ninstalled_packages \\u003c- installed.packages()[, 'Package']\\n\\n# Loop through required packages\\nfor (package in required_packages) {\\n  \\n  if (!(package %in% installed_packages)) {  # If package not installed...\\n    options(repos = \\\"https://cran.rstudio.com/\\\")  # ...set the CRAN mirror...\\n    install.packages(package)                     # ...and download the package.\\n  }\\n  \\n  suppressPackageStartupMessages(library(package, character.only = TRUE))  # Load package silently.\\n}\\n\\n\\n########################################\\n## PARSING THE COMMAND LINE ARGUMENTS ##\\n########################################\\n\\n# Initialize command line argument parser\\nparser \\u003c- ArgumentParser(description = 'DOWNLOAD ENA ACCESSIONS')\\n\\n# All of your command line arguments\\nparser$add_argument('-f', '--file', metavar = 'fileName', type = 'character', required = TRUE, help = 'Specify the file that contains the ENA accessions.')\\nparser$add_argument('-d', '--directory', metavar = 'directory', type = 'character', required = TRUE, help = 'Specify the folder that should be downloaded into.')\\n\\n# Parse the arguments\\nargs \\u003c- parser$parse_args()\\n\\n# Access the arguments\\nENAFile \\u003c- args$file\\ndirectory \\u003c- args$directory\\n\\n\\n##########################\\n## FILE EXISTENCE CHECK ##\\n##########################\\n\\n# Check if your file exists\\nif(!file.exists(ENAFile)){ # If the file does not exist...\\n  stop(paste(\\\"Error:\\\", ENAFile, \\\"not found.\\\")) # ... stop the script.\\n}\\n\\n\\n############################\\n## FILE CONTENTS HANDLING ##\\n############################\\n\\n# Read the contents of the ENA file\\nlines \\u003c- readLines(ENAFile, warn = F)\\n\\n# Make an empty variable that, later on, will contain information about the sequencing runs and their samples\\nruns_samples \\u003c- list()\\n\\n# Make an empty variable that, later on, will help us remember what sequencing run we are currently handling\\ncurrent_run \\u003c- NULL\\n\\n# Loop over all of the ENA file lines\\nfor(line in lines){\\n  \\n  # Remove all of the leading and trailing white space characters (spaces, tab and newline)\\n  line \\u003c- trimws(line)\\n  \\n  # Remove all of the other white space characters (spaces)\\n  line \\u003c- gsub(pattern = ' ', replacement = '', line) \\n  \\n  # Determine what the first character of the line is\\n  firstCharacter = substr(line,1,1)\\n  \\n  if(firstCharacter == '\\u003e'){                                # If the first character is a \\u003e...\\n    current_run \\u003c- sub(pattern = '\\u003e', replacement = '', line) # ... this line is the name of the current sequencing run...\\n    runs_samples[[current_run]] \\u003c- c()                       # ... this line is the key of a vector that will, later on, contain its samples.\\n  }\\n  \\n  else if(line == ''){ # If the line contains no information... \\n    next              # ... skip this line.\\n  }\\n  \\n  else{                                                                 # If none of the conditions is valid...\\n    runs_samples[[current_run]] = c(runs_samples[[current_run]], line)   # ... the line is a sample name that is added to its corresponding sequencing run\\n  }\\n}\\n\\n\\n#####################################\\n## FETCHING ONLINE ENA INFORMATION ##\\n#####################################\\n\\n# Loop over all sequencing runs\\nfor(run in names(runs_samples)){\\n  \\n  # Create the path into which the samples of the current sequencing run should be downloaded\\n  path.download \\u003c- file.path(directory, run)\\n  \\n  if(!dir.exists(path.download)){ # If the directory does not exist yet...\\n    dir.create(path.download)     # ... create the directory.\\n  }\\n  \\n  # Loop over all the samples belonging to the current sequencing run.\\n  for(sample in runs_samples[[run]]){\\n    \\n    # Imagine that our current sample is ERR4914118\\n    # The link for this ENA sample is ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR491/008/ERR4914118/ERR4914118_1.fastq.gz\\n    # You can see that there is a 6 letter code in this link (ERR491)\\n    \\n    # We extract this 6 character code from the sample name\\n    six_letter_code \\u003c- substr(sample, start = 1, stop = 6)\\n    \\n    # You can also see that there is a 1 character code in this link, preceded by '00' (008)\\n    \\n    # We extact this 1 number code\\n    one_letter_code \\u003c- substr(sample, start = nchar(sample), stop = nchar(sample))\\n    \\n    # We can see the forward read filename in the link (ERR4914118_1.fastq.gz), so we construct this one\\n    fwd_file_name = paste0(sample, '_1.fastq.gz')\\n    \\n    # There is also a reverse read filename in the link for the reverse read file, so we construct this one\\n    rev_file_name = paste0(sample, '_2.fastq.gz')\\n    \\n    # Construct the entire link for the forward read\\n    url_fwd = paste0('ftp://ftp.sra.ebi.ac.uk/vol1/fastq/', six_letter_code, '/', '00', one_letter_code, '/', sample, '/', fwd_file_name)\\n   \\n    # Construct the entire link for the reverse read\\n    url_rev = paste0('ftp://ftp.sra.ebi.ac.uk/vol1/fastq/', six_letter_code, '/', '00', one_letter_code, '/', sample, '/', rev_file_name)\\n    \\n    # Define the location and filename for the files that will be downloaded from ENA\\n    dest_fwd \\u003c- file.path(path.download, fwd_file_name)\\n    dest_rev \\u003c- file.path(path.download, rev_file_name)\\n    \\n    # Define how many times we can attempt to download an ENA accession\\n    max_retries \\u003c- 3\\n    \\n    # Define a variable that defines the number of current retries\\n    retry_count \\u003c- 0\\n    \\n    # Define a variable that will specify if the download of the ENA file was succesful or not\\n    download_success \\u003c- FALSE\\n    \\n    while (!download_success \\u0026\\u0026 retry_count \\u003c max_retries) { # Keep on trying as long as downloading did not succeed (download_succes == FALSE) and the maximum amount of retries is lower than 3\\n      retry_count \\u003c- retry_count + 1                         # Add 1 to the amount of current retries\\n      tryCatch({                                  \\n        download.file(url_fwd, dest_fwd)                     # Download the information from the url to the defined destination\\n        download_success \\u003c- TRUE                             # Mark the download as a succes\\n      }, error = function(e) {                               # If something within the tryCatch statement gave an error...\\n        message(paste(\\\"Error downloading file, retrying (\\\", retry_count, \\\"/\\\", max_retries, \\\")\\\")) # ... print an error message...\\n        Sys.sleep(10)                                                                            # ... and wait 10 seconds.\\n      })\\n    }\\n    \\n    if (!download_success) { # If the download was not a succes (download_succes == FALSE)...\\n      stop(\\\"Failed to download file after\\\", max_retries, \\\"attempts\\\") # stop the script and mention what accession did not work.\\n    }\\n    \\n    \\n    # The exact same thing as above, but for the reverse reads\\n    max_retries \\u003c- 3\\n    retry_count \\u003c- 0\\n    download_success \\u003c- FALSE\\n    \\n    while (!download_success \\u0026\\u0026 retry_count \\u003c max_retries) {\\n      retry_count \\u003c- retry_count + 1\\n      tryCatch({\\n        download.file(url_rev, dest_rev)\\n        download_success \\u003c- TRUE\\n      }, error = function(e) {\\n        message(paste(\\\"Error downloading file, retrying (\\\", retry_count, \\\"/\\\", max_retries, \\\")\\\"))\\n        Sys.sleep(10)\\n      })\\n    }\\n    \\n    if (!download_success) {\\n      stop(\\\"Failed to download file after\\\", max_retries, \\\"attempts\\\")\\n    }\\n  }\\n}\",\"os_name\":\"\",\"os_version\":\"\"}},\"8\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"ENADownload_8digits.R script\",\"description\":null,\"guid\":\"21317C2A403611EE9A840A58A9FEAC02\",\"name\":\"#!/usr/bin/env Rscript\\n\\n# List of packages you will need\\nrequired_packages \\u003c- c('argparse')\\n\\n# Determine already installed packages\\ninstalled_packages \\u003c- installed.packages()[, 'Package']\\n\\n# Loop through required packages\\nfor (package in required_packages) {\\n  \\n  if (!(package %in% installed_packages)) {  # If package not installed...\\n    options(repos = \\\"https://cran.rstudio.com/\\\")  # ...set the CRAN mirror...\\n    install.packages(package)                     # ...and download the package.\\n  }\\n  \\n  suppressPackageStartupMessages(library(package, character.only = TRUE))  # Load package silently.\\n}\\n\\n\\n########################################\\n## PARSING THE COMMAND LINE ARGUMENTS ##\\n########################################\\n\\n# Initialize command line argument parser\\nparser \\u003c- ArgumentParser(description = 'DOWNLOAD ENA ACCESSIONS')\\n\\n# All of your command line arguments\\nparser$add_argument('-f', '--file', metavar = 'fileName', type = 'character', required = TRUE, help = 'Specify the file that contains the ENA accessions.')\\nparser$add_argument('-d', '--directory', metavar = 'directory', type = 'character', required = TRUE, help = 'Specify the folder that should be downloaded into.')\\n\\n# Parse the arguments\\nargs \\u003c- parser$parse_args()\\n\\n# Access the arguments\\nENAFile \\u003c- args$file\\ndirectory \\u003c- args$directory\\n\\n\\n##########################\\n## FILE EXISTENCE CHECK ##\\n##########################\\n\\n# Check if your file exists\\nif(!file.exists(ENAFile)){ # If the file does not exist...\\n  stop(paste(\\\"Error:\\\", ENAFile, \\\"not found.\\\")) # ... stop the script.\\n}\\n\\n\\n############################\\n## FILE CONTENTS HANDLING ##\\n############################\\n\\n# Read the contents of the ENA file\\nlines \\u003c- readLines(ENAFile, warn = F)\\n\\n# Make an empty variable that, later on, will contain information about the sequencing runs and their samples\\nruns_samples \\u003c- list()\\n\\n# Make an empty variable that, later on, will help us remember what sequencing run we are currently handling\\ncurrent_run \\u003c- NULL\\n\\n# Loop over all of the ENA file lines\\nfor(line in lines){\\n  \\n  # Remove all of the leading and trailing white space characters (spaces, tab and newline)\\n  line \\u003c- trimws(line)\\n  \\n  # Remove all of the other white space characters (spaces)\\n  line \\u003c- gsub(pattern = ' ', replacement = '', line) \\n  \\n  # Determine what the first character of the line is\\n  firstCharacter = substr(line,1,1)\\n  \\n  if(firstCharacter == '\\u003e'){                                # If the first character is a \\u003e...\\n    current_run \\u003c- sub(pattern = '\\u003e', replacement = '', line) # ... this line is the name of the current sequencing run...\\n    runs_samples[[current_run]] \\u003c- c()                       # ... this line is the key of a vector that will, later on, contain its samples.\\n  }\\n  \\n  else if(line == ''){ # If the line contains no information... \\n    next              # ... skip this line.\\n  }\\n  \\n  else{                                                                 # If none of the conditions is valid...\\n    runs_samples[[current_run]] = c(runs_samples[[current_run]], line)   # ... the line is a sample name that is added to its corresponding sequencing run\\n  }\\n}\\n\\n\\n#####################################\\n## FETCHING ONLINE ENA INFORMATION ##\\n#####################################\\n\\n# Loop over all sequencing runs\\nfor(run in names(runs_samples)){\\n  \\n  # Create the path into which the samples of the current sequencing run should be downloaded\\n  path.download \\u003c- file.path(directory, run)\\n  \\n  if(!dir.exists(path.download)){ # If the directory does not exist yet...\\n    dir.create(path.download)     # ... create the directory.\\n  }\\n  \\n  # Loop over all the samples belonging to the current sequencing run.\\n  for(sample in runs_samples[[run]]){\\n    \\n    # Imagine that our current sample is ERR12541385\\n    # The link for this ENA sample is ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR125/085/ERR12541385/ERR12541385_1.fastq.gz\\n    # You can see that there is a 6 letter code in this link (ERR125)\\n    \\n    # We extract this 6 character code from the sample name\\n    six_letter_code \\u003c- substr(sample, start = 1, stop = 6)\\n    \\n    # You can also see that there is a 2 character code in this link, preceded by '0' (085)\\n    \\n    # We extact this 2 number code\\n    two_letter_code \\u003c- substr(sample, start = nchar(sample)-1, stop = nchar(sample))\\n    \\n    # We can see the forward read filename in the link (ERR12541385_1.fastq.gz), so we construct this one\\n    fwd_file_name = paste0(sample, '_1.fastq.gz')\\n    \\n    # There is also a reverse read filename in the link for the reverse read file, so we construct this one\\n    rev_file_name = paste0(sample, '_2.fastq.gz')\\n    \\n    # Construct the entire link for the forward read\\n    url_fwd = paste0('ftp://ftp.sra.ebi.ac.uk/vol1/fastq/', six_letter_code, '/', '0', two_letter_code, '/', sample, '/', fwd_file_name)\\n   \\n    # Construct the entire link for the reverse read\\n    url_rev = paste0('ftp://ftp.sra.ebi.ac.uk/vol1/fastq/', six_letter_code, '/', '0', two_letter_code, '/', sample, '/', rev_file_name)\\n    \\n    # Define the location and filename for the files that will be downloaded from ENA\\n    dest_fwd \\u003c- file.path(path.download, fwd_file_name)\\n    dest_rev \\u003c- file.path(path.download, rev_file_name)\\n    \\n    # Define how many times we can attempt to download an ENA accession\\n    max_retries \\u003c- 3\\n    \\n    # Define a variable that defines the number of current retries\\n    retry_count \\u003c- 0\\n    \\n    # Define a variable that will specify if the download of the ENA file was succesful or not\\n    download_success \\u003c- FALSE\\n    \\n    while (!download_success \\u0026\\u0026 retry_count \\u003c max_retries) { # Keep on trying as long as downloading did not succeed (download_succes == FALSE) and the maximum amount of retries is lower than 3\\n      retry_count \\u003c- retry_count + 1                         # Add 1 to the amount of current retries\\n      tryCatch({                                  \\n        download.file(url_fwd, dest_fwd)                     # Download the information from the url to the defined destination\\n        download_success \\u003c- TRUE                             # Mark the download as a succes\\n      }, error = function(e) {                               # If something within the tryCatch statement gave an error...\\n        message(paste(\\\"Error downloading file, retrying (\\\", retry_count, \\\"/\\\", max_retries, \\\")\\\")) # ... print an error message...\\n        Sys.sleep(10)                                                                            # ... and wait 10 seconds.\\n      })\\n    }\\n    \\n    if (!download_success) { # If the download was not a succes (download_succes == FALSE)...\\n      stop(\\\"Failed to download file after\\\", max_retries, \\\"attempts\\\") # stop the script and mention what accession did not work.\\n    }\\n    \\n    \\n    # The exact same thing as above, but for the reverse reads\\n    max_retries \\u003c- 3\\n    retry_count \\u003c- 0\\n    download_success \\u003c- FALSE\\n    \\n    while (!download_success \\u0026\\u0026 retry_count \\u003c max_retries) {\\n      retry_count \\u003c- retry_count + 1\\n      tryCatch({\\n        download.file(url_rev, dest_rev)\\n        download_success \\u003c- TRUE\\n      }, error = function(e) {\\n        message(paste(\\\"Error downloading file, retrying (\\\", retry_count, \\\"/\\\", max_retries, \\\")\\\"))\\n        Sys.sleep(10)\\n      })\\n    }\\n    \\n    if (!download_success) {\\n      stop(\\\"Failed to download file after\\\", max_retries, \\\"attempts\\\")\\n    }\\n  }\\n}\",\"os_name\":\"\",\"os_version\":\"\"}},\"9\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Download fastq files from ENA\",\"description\":\"\",\"guid\":\"28B88EAE403711EE9A840A58A9FEAC02\",\"name\":\"# Make sure Rscript.exe is in the environment variable PATH or call it directly with its path as shown below\\n\\n# COI\\ncd ~/COI\\n\\\"C:/Program Files/R/R-4.3.1/bin/Rscript.exe\\\" ~/ENADownload.R -f COI_ENA_accessions.txt -d ~/COI/fastq_files\\n\\\"C:/Program Files/R/R-4.3.1/bin/Rscript.exe\\\" ~/ENADownload_8digits.R -f COI_ENA_accessions_8digits.txt -d ~/COI/fastq_files\\n\\n# 18S\\ncd ~/18S\\n\\\"C:/Program Files/R/R-4.3.1/bin/Rscript.exe\\\" ~/ENADownload.R -f 18S_ENA_accessions.txt -d ~/18S/fastq_files\\n\\\"C:/Program Files/R/R-4.3.1/bin/Rscript.exe\\\" ~/ENADownload_8digits.R -f 18S_ENA_accessions_8digits.txt -d ~/18S/fastq_files\",\"os_name\":\"\",\"os_version\":\"\"}}}}","data":null,"protocol_id":86559,"case_id":0,"critical_ids":"","duration":0,"original_id":0,"number":"1","cases":[],"critical":null},{"id":1768013,"guid":"19AB30703D0E11EEBF2B812AFE632E09","previous_id":1766859,"previous_guid":"E0406CC83C5411EE882A0A58A9FEAC02","section":"\u003cp\u003eGenerate fasta file, count table and ensemble taxonomy for 18S ASVs\u003c/p\u003e","section_color":"#84CE84","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"8svmp\",\"text\":\"Taxonomy has been assigned to 18S ASVs in the previous step within the dada2 pipeline in R. For this, contributed dada2-formatted reference files of PR2 v5.0.0, Silva v132, and a specific Silva v132 Eukaryote set were used. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":71,\"length\":5},{\"style\":\"italic\",\"offset\":89,\"length\":1},{\"style\":\"italic\",\"offset\":114,\"length\":5},{\"style\":\"italic\",\"offset\":149,\"length\":3},{\"style\":\"italic\",\"offset\":161,\"length\":5},{\"style\":\"italic\",\"offset\":188,\"length\":5}],\"entityRanges\":[],\"data\":{}},{\"key\":\"a7o4d\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"e1mju\",\"text\":\"The R script below generates an ASV fasta file, an ASV count table and an ensemble taxonomy table based on the previous assignments of the three reference sets. To merge the different taxonomy assignments, we applied certain functions of the ensembleTax package and parts of its recommended pipeline (see https://github.com/dcat4/ensembleTax for more details).\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":4,\"length\":1},{\"style\":\"italic\",\"offset\":242,\"length\":11}],\"entityRanges\":[{\"key\":0,\"offset\":305,\"length\":36}],\"data\":{}},{\"key\":\"3v1bd\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7tuhj\",\"text\":\"Please note that resolving eukaryote taxonomy homogeneously according to clear rank hierarchy can be challenging. In addition, different reference sets use different taxonomic ranks and/or synonomous terms at a given rank for the same taxa. The ensemble taxonomy for 18S ASVs  resulting from the script below contains the following ranks: Domain, Supergroup, Division, Subdivision, Phylum, Class_X, Class_Order_Family, Order_Family_X, Genus, Species. Especially rank assignments between Subdivision and Order_Family_X should be treated with caution. Depending on the reference set or taxa, these can be actual subdivisions, phyla, classes, order or families, or any of their super, sub, or infra groups. The names of these ranks define that the majority of assignments correspond to this rank. So for example, most assignments in Class_Order_Family represent one of these three ranks. However, in some cases, this does not hold true. For example, the assignment of Class_X may actually be a sub phylum or an order level assignment, etc.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"aaj1g\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":1,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"9sv1q\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{\"0\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"9D8F32383D1911EE855C0A58A9FEAC02\",\"url\":\"https://github.com/dcat4/ensembleTax\"}},\"1\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Ensemble taxonomy for 18S ASVs in R\",\"description\":\"\",\"guid\":\"DD0763303D1111EEBF5D0A58A9FEAC02\",\"name\":\"library(devtools)\\ndevtools::install_github(\\\"dcat4/ensembleTax\\\", build_manual = FALSE, build_vignettes = TRUE) # Installation issue when build_manual was set to TRUE (problem with pdflatex) \\nlibrary(ensembleTax)\\nlibrary(tidyr)\\nlibrary(dplyr)\\nlibrary(Biostrings)\\nlibrary(dada2)\\nlibrary(stringr)\\n\\nsetwd(\\\"~/18S\\\")\\n\\n# Read non-chimeric, non-singleton sequence table\\n\\nseqtab.nochim.nosingle\\u003c-readRDS(\\\"seqtab_nochim_nosingle_18S.rds\\\")\\n\\n# Read taxonomy objects (dada2 output)\\n\\nsilva\\u003c-readRDS(\\\"taxa_18S_silva.rds\\\")\\nsilva.euk\\u003c-readRDS(\\\"taxa_18S_silva_euk.rds\\\")\\npr2\\u003c-readRDS(\\\"taxa_18S_pr2.rds\\\")\\n\\n## Combine the two Silva taxonomies ##\\n\\n# remove the Strain column of silva.euk\\n\\nsilva.euk$tax\\u003c-silva.euk$tax[,-8]\\nsilva.euk$boot\\u003c-silva.euk$boot[,-8]\\n\\n# Make dataframes of the tax objects of the two Silva taxonomies \\n\\nsilva_tax\\u003c-as.data.frame(silva$tax)\\nsilva_euk_tax\\u003c-as.data.frame(silva.euk$tax)\\n\\n# Create two vectors for Class and Order_Family level\\n# For ASVs where Silva classification was not NA, this information was kept. \\n# If Silva classification was NA, the classification of the Silva eukaryote reference set was kept.\\n# If both were NA, NA was set. \\n\\nclass \\u003c-ifelse(!is.na(silva_tax$Class),silva_tax$Class,silva_euk_tax$Class)\\n\\norder_family \\u003c-ifelse(!is.na(silva_tax$Order),silva_tax$Order,silva_euk_tax$Order_Family)\\n\\n# Combine columns of Silva and Silva eukaryote taxonomies\\n\\nsilva_taxonomy\\u003c-cbind(silva_euk_tax[,1:4],silva_tax[,1:2],class,order_family,silva_euk_tax[,7])\\ncolnames(silva_taxonomy)[7:9]\\u003c-c(\\\"Class\\\",\\\"Order_Family\\\",\\\"Species\\\") # Set some column names\\n\\n# One ASV was classified as Bacteria in the Silva classification. Taxonomy will be set to NA for all ranks\\n\\nsilva_taxonomy[which(silva_taxonomy$Kingdom==\\\"Bacteria\\\"),] \\u003c- NA\\n\\n# remove Kingdom column\\n\\nsilva_taxonomy\\u003c-silva_taxonomy[,-5]\\n\\n# Split Species column strings into separate columns\\n# First. make rownames as a column, as separate_wider_functions remove rownames (probably bug)\\n\\nsilva_taxonomy$seqs\\u003c-rownames(silva_taxonomy)\\nsilva_taxonomy\\u003c-silva_taxonomy[,c(9,1:8)] # quick re-arranging of columns\\nsilva_taxonomy\\u003c-silva_taxonomy %\\u003e% separate_wider_delim(Species, delim = \\\"_\\\", names_sep=\\\"\\\",too_few = \\\"align_start\\\")\\n\\n# Keep only first two columns of Species strings\\n\\nsilva_taxonomy\\u003c-silva_taxonomy[,-(11:ncol(silva_taxonomy))]\\n\\n# Set second column of species strings to NA if it says sp. or cf. or environmental\\n\\nsilva_taxonomy[which(silva_taxonomy[,10]==\\\"sp.\\\" | silva_taxonomy[,10]==\\\"cf.\\\" | silva_taxonomy[,10]==\\\"environmental\\\"),10] \\u003c- NA\\n\\n# Set species strings to NA if the first species string column contains \\\"U/uncultured\\\" or \\\"unidentified\\\"\\n\\nsilva_taxonomy[which(silva_taxonomy[,9]==\\\"uncultured\\\" | silva_taxonomy[,9]==\\\"Uncultured\\\" | silva_taxonomy[,9]==\\\"unidentified\\\"),9:10] \\u003c- NA\\n\\n# Add a column with genus and species as one string\\n\\nsilva_taxonomy\\u003c-as.data.frame(silva_taxonomy)\\nsilva_taxonomy$Species\\u003c-paste(silva_taxonomy[,9],silva_taxonomy[,10],sep=\\\"_\\\")\\n\\n# Set Species column to NA if the 10th column is NA\\n\\nsilva_taxonomy$Species\\u003c-ifelse(is.na(silva_taxonomy$Species2),NA,silva_taxonomy$Species)\\n\\n# Make a genus level column\\n\\ncolnames(silva_taxonomy)[9]\\u003c-\\\"Genus\\\"\\n\\n# Remove the remaining column of the second species string\\n\\nsilva_taxonomy\\u003c-silva_taxonomy[,-10]\\n\\n# Save file\\n\\nsaveRDS(silva_taxonomy,\\\"silva_taxonomy.rds\\\")\\n\\n# Generate a new Silva taxonomy object in the form of dada2's assignTaxonomy output\\n\\nsilva_taxonomy\\u003c-as.matrix(silva_taxonomy) # create character matrix\\nrownames(silva_taxonomy)\\u003c-silva_taxonomy[,1] # make sequences rownames\\nsilva_taxonomy\\u003c-silva_taxonomy[,-1] # remove sequences column\\n# Make dummy bootstrap table\\nboots \\u003c- matrix(rep(100,nrow(silva_taxonomy)*9), nrow = nrow(silva_taxonomy), ncol = 9, byrow = TRUE,dimnames = list(rownames(silva_taxonomy),colnames(silva_taxonomy)))\\nsilva_new\\u003c-silva\\nsilva_new$tax\\u003c-silva_taxonomy\\nsilva_new$boot\\u003c-boots\\n\\n## ensembleTax workflow ##\\n\\n# Make rubric with sequences mapped to short ASV IDs\\nrubric \\u003c- DNAStringSet(getSequences(seqtab.nochim.nosingle))\\n# this creates names (ASV1, ASV2, ..., ASVX) for each ASV\\nsnam \\u003c- vector(mode = \\\"character\\\", length = length(rubric))\\nfor (i in 1:length(rubric)) {\\n  snam[i] \\u003c- paste0(\\\"ASV\\\", as.character(i))\\n}\\nnames(rubric) \\u003c- snam\\n\\n# Pre-process with the bayes2taxdf function\\n# ensembleTax supports Silva v138 and PR2 v4.14.0\\n# because we used PR2 v5.0.0 and customized a Silva taxonomy here, we will set db = NULL and provide the ranks present in our taxonomy assignments\\n\\nsilva.pretty \\u003c- bayestax2df(silva_new, \\n                            db = NULL, \\n                            ranks = colnames(silva_new$tax),\\n                            boot = 70,\\n                            rubric = rubric,\\n                            return.conf = FALSE)\\n\\npr2.pretty \\u003c- bayestax2df(pr2, \\n                          db = NULL, \\n                          ranks = colnames(pr2$tax),\\n                          boot = 70,\\n                          rubric = rubric,\\n                          return.conf = FALSE)\\n\\n# Translate Silva taxonomic assignments onto the taxonomic nomenclature pf the PR2 assignments\\n\\n# Because ensembleTax does not yet support PR2 v5.0.0., we create a custom tax2map2 object for use with taxmapper function\\nff \\u003c- \\\"pr2_version_5.0.0_SSU_dada2.fasta.gz\\\" # read PR2 reference fasta\\nfastaFile \\u003c- readDNAStringSet(ff)\\nseq_name = names(fastaFile)\\ntaxmap \\u003c- str_split(seq_name, pattern = \\\";\\\", simplify = TRUE)\\ntaxmap \\u003c- as.data.frame(taxmap[, -ncol(taxmap)], stringsAsFactors = FALSE)\\ncolnames(taxmap) \\u003c- colnames(pr2$tax)\\ntaxmap \\u003c- unique(taxmap, MARGIN = 1)\\nany(is.na(taxmap)) # output should be FALSE\\n\\nsilva.mapped2pr2 \\u003c- taxmapper(silva.pretty,\\n                              tt.ranks = colnames(silva.pretty)[3:ncol(silva.pretty)],\\n                              tax2map2 = taxmap,\\n                              exceptions = c(\\\"Archaea\\\", \\\"Bacteria\\\"),\\n                              ignore.format = TRUE,\\n                              synonym.file = \\\"default\\\",\\n                              streamline = TRUE,\\n                              outfilez = NULL)\\n\\n## End ensembleTax workflow here. assign.ensembleTax function will not be used.\\n# assign.ensembleTax sets lower ranks to NA when a rank above is NA, even though two taxonomies may agree at this lower rank\\n# We therefore used alternative commands instead. See below.\\n\\n# Continue...\\n\\n# The info in the Order_Family column of the original Silva dada2 assignments are usually not present in the PR2 assignments\\n# Add this info as new column to the PR2 and mapped Silva assignments if the string in silva.pretty$Class matches the string in either pr2.pretty$Class_X or pr2.pretty$Class_Order_Family / silva.mapped2pr2$Class_X or silva.mapped2pr2$Class_Order_Family\\n\\norder_family_x_1\\u003c-ifelse(silva.pretty$Class==pr2.pretty$Class_X | silva.pretty$Class==pr2.pretty$Class_Order_Family,silva.pretty$Order_Family,NA)\\npr2.pretty\\u003c-cbind(pr2.pretty[,1:9],order_family_x_1,pr2.pretty[,10:11])\\ncolnames(pr2.pretty)[10]\\u003c-\\\"Order_Family_X\\\"\\norder_family_x_2\\u003c-ifelse(silva.pretty$Class==silva.mapped2pr2$Class_X | silva.pretty$Class==silva.mapped2pr2$Class_Order_Family,silva.pretty$Order_Family,NA)\\nsilva.mapped2pr2\\u003c-cbind(silva.mapped2pr2[,1:9],order_family_x_2,silva.mapped2pr2[,10:11])\\ncolnames(silva.mapped2pr2)[10]\\u003c-\\\"Order_Family_X\\\"\\n\\n# Perform some formatting of both the Silva and PR2 assignments\\n\\nis.na(silva.mapped2pr2) \\u003c- array(grepl('uncultured', as.matrix(silva.mapped2pr2)), dim(silva.mapped2pr2)) # Set as NA when \\\"uncultured_\\\" appears in Silva assignments\\nis.na(pr2.pretty) \\u003c- array(grepl('_X', as.matrix(pr2.pretty)), dim(pr2.pretty)) # Set as NA when \\\"_X\\\" appears in PR2 assignments\\nis.na(silva.mapped2pr2) \\u003c- array(grepl('_X', as.matrix(silva.mapped2pr2)), dim(silva.mapped2pr2)) # Set as NA when \\\"_X\\\" appears in Silva assignments\\nis.na(silva.mapped2pr2) \\u003c- array(grepl('_sp.', as.matrix(silva.mapped2pr2)), dim(silva.mapped2pr2)) # Set as NA when \\\"_sp.\\\" appears in Silva species level assignments\\nis.na(pr2.pretty) \\u003c- array(grepl('_sp.', as.matrix(pr2.pretty)), dim(pr2.pretty))# Set as NA when \\\"_sp.\\\" appears in PR2 species level assignments\\n\\n# Set NAs to character string \\\"NA\\\" to enable the following procedures\\n\\nsilva.mapped2pr2[is.na(silva.mapped2pr2)]\\u003c-\\\"NA\\\"\\npr2.pretty[is.na(pr2.pretty)]\\u003c-\\\"NA\\\"\\n\\n## Generate final taxonomy\\n# For ASVs where PR22 and Silva agreed at the respective level, this information was kept. \\n# For levels of an ASV were the two disagreed, the assignments were set to NA. \\n# Where one of the two taxonomies was NA, but the other one was not, the latter's assignment was kept.\\n\\nDomain\\u003c-ifelse(pr2.pretty$Domain==silva.mapped2pr2$Domain,pr2.pretty$Domain,ifelse(pr2.pretty$Domain==\\\"NA\\\",silva.mapped2pr2$Domain,ifelse(silva.mapped2pr2$Domain==\\\"NA\\\",pr2.pretty$Domain,\\\"NA\\\")))\\nSupergroup\\u003c-ifelse(pr2.pretty$Supergroup==silva.mapped2pr2$Supergroup,pr2.pretty$Supergroup,ifelse(pr2.pretty$Supergroup==\\\"NA\\\",silva.mapped2pr2$Supergroup,ifelse(silva.mapped2pr2$Supergroup==\\\"NA\\\",pr2.pretty$Supergroup,\\\"NA\\\")))\\nDivision\\u003c-ifelse(pr2.pretty$Division==silva.mapped2pr2$Division,pr2.pretty$Division,ifelse(pr2.pretty$Division==\\\"NA\\\",silva.mapped2pr2$Division,ifelse(silva.mapped2pr2$Division==\\\"NA\\\",pr2.pretty$Division,\\\"NA\\\")))\\nSubdivision\\u003c-ifelse(pr2.pretty$Subdivision==silva.mapped2pr2$Subdivision,pr2.pretty$Subdivision,ifelse(pr2.pretty$Subdivision==\\\"NA\\\",silva.mapped2pr2$Subdivision,ifelse(silva.mapped2pr2$Subdivision==\\\"NA\\\",pr2.pretty$Subdivision,\\\"NA\\\")))\\nPhylum\\u003c-ifelse(pr2.pretty$Phylum==silva.mapped2pr2$Phylum,pr2.pretty$Phylum,ifelse(pr2.pretty$Phylum==\\\"NA\\\",silva.mapped2pr2$Phylum,ifelse(silva.mapped2pr2$Phylum==\\\"NA\\\",pr2.pretty$Phylum,\\\"NA\\\")))\\nClass_X\\u003c-ifelse(pr2.pretty$Class_X==silva.mapped2pr2$Class_X,pr2.pretty$Class_X,ifelse(pr2.pretty$Class_X==\\\"NA\\\",silva.mapped2pr2$Class_X,ifelse(silva.mapped2pr2$Class_X==\\\"NA\\\",pr2.pretty$Class_X,\\\"NA\\\")))\\nClass_Order_Family\\u003c-ifelse(pr2.pretty$Class_Order_Family==silva.mapped2pr2$Class_Order_Family,pr2.pretty$Class_Order_Family,ifelse(pr2.pretty$Class_Order_Family==\\\"NA\\\",silva.mapped2pr2$Class_Order_Family,ifelse(silva.mapped2pr2$Class_Order_Family==\\\"NA\\\",pr2.pretty$Class_Order_Family,\\\"NA\\\")))\\nOrder_Family_X\\u003c-ifelse(pr2.pretty$Order_Family_X==silva.mapped2pr2$Order_Family_X,pr2.pretty$Order_Family_X,ifelse(pr2.pretty$Order_Family_X==\\\"NA\\\",silva.mapped2pr2$Order_Family_X,ifelse(silva.mapped2pr2$Order_Family_X==\\\"NA\\\",pr2.pretty$Order_Family_X,\\\"NA\\\")))\\nGenus\\u003c-ifelse(pr2.pretty$Genus==silva.mapped2pr2$Genus,pr2.pretty$Genus,ifelse(pr2.pretty$Genus==\\\"NA\\\",silva.mapped2pr2$Genus,ifelse(silva.mapped2pr2$Genus==\\\"NA\\\",pr2.pretty$Genus,\\\"NA\\\")))\\nSpecies\\u003c-ifelse(pr2.pretty$Species==silva.mapped2pr2$Species,pr2.pretty$Species,ifelse(pr2.pretty$Species==\\\"NA\\\",silva.mapped2pr2$Species,ifelse(silva.mapped2pr2$Species==\\\"NA\\\",pr2.pretty$Species,\\\"NA\\\")))\\n\\ntax_table\\u003c-cbind(pr2.pretty[,1:2],Domain,Supergroup,Division,Subdivision,Phylum,Class_X,Class_Order_Family,Order_Family_X,Genus,Species)\\n\\n\\n## Write a fasta file of the non-chimeric and non-singleton sequences with \\u003eASV... type headers\\n\\nasv_seqs \\u003c- tax_table$ASV\\nasv_headers \\u003c- paste0(\\\"\\u003e\\\",tax_table$svN)\\nasv_fasta \\u003c- c(rbind(asv_headers, asv_seqs))\\nwrite(asv_fasta, \\\"18S_nochim_nosingle_ASVs.fa\\\")\\n\\n# Write an ASV count table of the non-chimeric and non-singleton sequences with short \\u003eASV... type names\\n\\nASV_counts\\u003c-t(seqtab.nochim.nosingle) # transposing table\\n\\n# Sort count table based on sequence order in tax_table and write file with ASV names\\n\\nASV_counts\\u003c-ASV_counts[order(match(rownames(ASV_counts),tax_table[,2])),]\\n\\nrownames(ASV_counts) \\u003c- tax_table$svN\\n\\nwrite.table(ASV_counts,file=\\\"18S_ASV_counts_nosingle.txt\\\",sep=\\\"\\\\t\\\", quote=F,col.names=NA)\\n\\n# write final taxonomy table to file\\n\\ntax_table\\u003c-tax_table[,-2]\\ncolnames(tax_table)[1]\\u003c-\\\"ASV\\\"\\nwrite.table(tax_table,\\\"18S_tax_table.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\",\"os_name\":\"\",\"os_version\":\"\"}}}}","data":null,"protocol_id":86559,"case_id":0,"critical_ids":"","duration":0,"original_id":0,"number":"3","cases":[],"critical":null},{"id":1778236,"guid":"51DE4706E8FA442E83159F7015186731","previous_id":2009542,"previous_guid":"919A5CBA21F54344948DB65AF86B302B","section":"\u003cp\u003eData processing and cleaning with \u003cem\u003ephyloseq \u003c/em\u003eetc.\u003c/p\u003e","section_color":"#EA9F6C","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"fvodv\",\"text\":\"Further data processing and cleaning was done in R / RStudio using mainly the phyloseq package (and other packages, see script below). The COI and 18S data sets were processed separately. Below are the metadata files used as sample_data in the respective phyloseq objects (the information was taken from the respective files available on the ARMS-MBON GitHub repository: https://github.com/arms-mbon/data_workspace/blob/main/qualitycontrolled_data/combined/combined_ObservatoryData.csv, https://github.com/arms-mbon/data_workspace/blob/main/qualitycontrolled_data/combined/combined_SamplingEventData.csv and https://github.com/arms-mbon/data_workspace/blob/main/qualitycontrolled_data/combined/combined_OmicsData.csv):\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":49,\"length\":11},{\"style\":\"italic\",\"offset\":78,\"length\":8}],\"entityRanges\":[{\"key\":0,\"offset\":371,\"length\":114},{\"key\":1,\"offset\":487,\"length\":116},{\"key\":2,\"offset\":608,\"length\":108}],\"data\":{}},{\"key\":\"8ctti\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"35u9p\",\"text\":\"  \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":3,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"2ufth\",\"text\":\"      \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":4,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"dualv\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"a1ghe\",\"text\":\"The following steps were carried out for both data sets:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"9s3q6\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3bs98\",\"text\":\"Create phyloseq objects based on the count and taxonomy tables created during the merging of same-species MOTUs in the previous section, as well as a table containing metadata for the samples (see above).\",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"5uh24\",\"text\":\"The MOTUs are still named by their representative ASV sequences. They will be renamed to MOTUxy and a mapping file is created to link each MOTU name to its representative ASV sequence.\",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"471v8\",\"text\":\"Check and plot sample-wise sequencing depth (i.e., read numbers) for each sequencing run.\",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7cekq\",\"text\":\"Removal of certain sample types: blank / negative control samples; sediment / plankton samples which were sequenced as a trial during the initial phase of the ARMS MBON program; and any samples without any reads remaining after all previous pipeline steps. This is now the most unfiltered data set  for each marker gene which will be saved!\",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":257,\"length\":83}],\"entityRanges\":[],\"data\":{}},{\"key\":\"82b9k\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":5,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"pul0\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"c7k9s\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":6,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"1n39r\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"avb9n\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{\"0\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"2D3E0CD5E12711EEA5730A58A9FEAC02\",\"url\":\"https://github.com/arms-mbon/data_workspace/blob/main/qualitycontrolled_data/combined/combined_ObservatoryData.csv\"}},\"1\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"49EE8983E12711EEA5730A58A9FEAC02\",\"url\":\"https://github.com/arms-mbon/data_workspace/blob/main/qualitycontrolled_data/combined/combined_SamplingEventData.csv\"}},\"2\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"52BB86B5E12711EEA5730A58A9FEAC02\",\"url\":\"https://github.com/arms-mbon/data_workspace/blob/main/qualitycontrolled_data/combined/combined_OmicsData.csv\"}},\"3\":{\"type\":\"file\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"\",\"id\":454653,\"original_name\":\"COI_sample_data.txt\",\"placeholder\":\"https://www.protocols.io/img/extensions/txt.png\",\"raw_source\":\"\",\"size\":63208,\"source\":\"https://content.protocols.io/files/p597bimqf.txt\"}},\"4\":{\"type\":\"file\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"\",\"id\":454654,\"original_name\":\"18S_sample_data.txt\",\"placeholder\":\"https://www.protocols.io/img/extensions/txt.png\",\"raw_source\":\"\",\"size\":64069,\"source\":\"https://content.protocols.io/files/p598bimqf.txt\"}},\"5\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"18S - data processing and cleaning in R\",\"description\":\"\",\"guid\":\"32ABCCF447DF11EE9E890A58A9FEAC02\",\"name\":\"library(phyloseq)\\nlibrary(ggplot2)\\nlibrary(data.table)\\n\\n\\nsetwd(\\\"~/18S\\\")\\n\\n# Read MOTU counts\\n\\nMOTUcounts18S\\u003c-read.table(\\\"18S_motu_count_table_merged_species.txt\\\",header=T,check.names=F, sep=\\\"\\\\t\\\")\\n\\n# Read MOTU taxonomy (needs to be read as matrix, may cause problems otherwise when phyloseq object will be created)\\n\\nMOTUtaxa18S\\u003c-as.matrix(read.table(\\\"18S_motu_tax_table_merged_species.txt\\\",header=T,check.names=F, sep=\\\"\\\\t\\\"))\\n\\n# Sort count table based on order in tax table (precautionary measure)\\n\\nMOTUcounts18S\\u003c-MOTUcounts18S[order(match(MOTUcounts18S[,1],MOTUtaxa18S[,1])),]\\n\\n# MOTUs are still named \\\"ASVxy\\\". Replace them with MOTUxy and set them as rownames.\\n\\n# First, create and write mapping file (MOTUxy = ASVyz). \\nmapping\\u003c-cbind(MOTUcounts18S[,1],paste0(\\\"MOTU\\\", seq(1:nrow(MOTUcounts18S)))) \\ncolnames(mapping)[1:2]\\u003c-c(\\\"ASV\\\",\\\"MOTU\\\")\\nwrite.table(mapping,\\\"motu_asv_mapping.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\nrownames(MOTUcounts18S) \\u003c- mapping[,2]\\nMOTUcounts18S[,1] \\u003c- NULL\\n\\nrownames(MOTUtaxa18S) \\u003c- mapping[,2]\\nMOTUtaxa18S\\u003c-MOTUtaxa18S[,-1] # setting it as NULL does not work for matrix object\\n\\n# Read sample metadata\\n# Note that there may be more samples left with zero reads. In this case, these samples passed the filterAndTrim step with reads, but they ended up with zero reads during the subsequent dada2 steps.\\n\\nMOTUsample18S\\u003c-read.table(\\\"18S_sample_data.txt\\\",header=T,check.names=F,row.names=1,sep=\\\"\\\\t\\\",strip.white = T) \\n\\n# Create phyloseq object\\n\\nps18S \\u003c- phyloseq(otu_table(MOTUcounts18S,taxa_are_rows = TRUE), sample_data(MOTUsample18S), tax_table(MOTUtaxa18S))\\n\\n## Get a quick overview of sequencing depth per sequencing run ##\\n\\n# Make data.table for plot\\n\\nread_sums\\u003c- data.table(as(sample_data(ps18S), \\\"data.frame\\\"),\\n                       TotalReads = sample_sums(ps18S), keep.rownames = TRUE)\\nsetnames(read_sums,\\\"rn\\\",\\\"SampleID\\\")\\n\\n# Violin plot based on sequencing events\\n\\nreads_plot \\u003c- ggplot(read_sums, aes(y=TotalReads,x=Sequenced,color=Sequenced)) + \\n  geom_violin() + \\n  geom_jitter(shape=16, position=position_jitter(0.2),size=1) +\\n  scale_y_continuous(labels = scales::comma)+\\n  scale_x_discrete(limits=c(\\\"Jul-19\\\",\\\"Jan-20\\\",\\\"Sep-20\\\",\\\"Apr-21\\\",\\\"May-21\\\",\\\"Jan-22\\\",\\\"Aug-23\\\"))+\\n  theme_bw()+\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"Sequencing Depth\\\")\\n\\nreads_plot\\n\\nggsave(reads_plot,file=\\\"18S_sequencing_depth_runs.png\\\",height = 4,width = 6)\\n\\n##\\n\\n# Remove the negative controls\\n\\nps18S_cleaned \\u003c- subset_samples(ps18S,ARMS!=\\\"blank\\\")\\n\\n# Remove the sediment and plankton samples (some sediment and plankton samples were sequenced as a trial during the initial phase of the ARMS program)\\n\\nps18S_cleaned \\u003c- subset_samples(ps18S_cleaned,Fraction!=\\\"SED\\\" \\u0026 Fraction!=\\\"PS\\\")\\n\\n# Remove samples with a read number of zero\\n\\nps18S_cleaned \\u003c- prune_samples(sample_sums(ps18S_cleaned) \\u003e 0, ps18S_cleaned)\\n\\n# Remove MOTUs which have a total abundance of zero after removing samples during all of the previous steps\\n\\nps18S_cleaned\\u003c-prune_taxa(rowSums(otu_table(ps18S_cleaned))\\u003e0,ps18S_cleaned)\\n\\n# Save this phyloseq object as the most unfiltered ARMS data set\\n\\nsaveRDS(ps18S_cleaned,\\\"ps18S_unfiltered_ARMS.rds\\\")\\n\\n# get number of remaining samples and MOTUs\\n\\nps18S_cleaned\\n\\n# get number of reads\\n\\nsum(sample_sums(ps18S_cleaned))\\n\\n# Get percentage of MOTUs classified at phylum level\\n\\nsubset_taxa(ps18S_cleaned,!is.na(Phylum))\\n\\n6465/13771\\n\\n# get percentage of reads classified at phylum level\\n\\nsum(sample_sums(subset_taxa(ps18S_cleaned,!is.na(Phylum))))/sum(sample_sums(ps18S_cleaned))\\n\\n# Get percentage of MOTUs classified at species level\\n\\nsubset_taxa(ps18S_cleaned,!is.na(Species))\\n\\n1076/13771\\n\\n# get percentage of reads classified at phylum level\\n\\nsum(sample_sums(subset_taxa(ps18S_cleaned,!is.na(Species))))/sum(sample_sums(ps18S_cleaned))\\n\",\"os_name\":\"\",\"os_version\":\"\"}},\"6\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"COI - data processing and cleaning in R\",\"description\":\"\",\"guid\":\"527F6D9247E211EE9E890A58A9FEAC02\",\"name\":\"library(phyloseq)\\nlibrary(ggplot2)\\nlibrary(data.table)\\n\\nsetwd(\\\"~/COI\\\")\\n\\n# Read MOTU counts \\n\\nMOTUcountsCOI\\u003c-read.table(\\\"COI_motu_count_table_merged_species.txt\\\",header=T,check.names=F, sep=\\\"\\\\t\\\")\\n\\n# Read MOTU taxonomy (needs to be read as matrix, may cause problems otherwise when phyloseq object will be created)\\n\\nMOTUtaxaCOI\\u003c-as.matrix(read.table(\\\"COI_motu_tax_table_merged_species.txt\\\",header=T,check.names=F, sep=\\\"\\\\t\\\"))\\n\\n# Sort count table based on order in tax table (precautionary measure)\\n\\nMOTUcountsCOI\\u003c-MOTUcountsCOI[order(match(MOTUcountsCOI[,1],MOTUtaxaCOI[,1])),]\\n\\n# MOTUs are still named \\\"ASVxy\\\". Replace them with MOTUxy and set them as rownames.\\n\\n# First, create and write mapping file (MOTUxy = ASVyz). \\nmapping\\u003c-cbind(MOTUcountsCOI[,1],paste0(\\\"MOTU\\\", seq(1:nrow(MOTUcountsCOI)))) \\ncolnames(mapping)[1:2]\\u003c-c(\\\"ASV\\\",\\\"MOTU\\\")\\nwrite.table(mapping,\\\"motu_asv_mapping.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\nrownames(MOTUcountsCOI) \\u003c- mapping[,2]\\nMOTUcountsCOI[,1] \\u003c- NULL\\n\\nrownames(MOTUtaxaCOI) \\u003c- mapping[,2]\\nMOTUtaxaCOI\\u003c-MOTUtaxaCOI[,-1] # setting it as NULL does not work for matrix object\\n\\n# Read sample metadata\\n\\nMOTUsampleCOI\\u003c-read.table(\\\"COI_sample_data.txt\\\",header=T,row.names=1,check.names=F, sep=\\\"\\\\t\\\",strip.white = T)\\n\\n# Create phyloseq object\\n\\npsCOI \\u003c- phyloseq(otu_table(MOTUcountsCOI,taxa_are_rows = TRUE), sample_data(MOTUsampleCOI), tax_table(MOTUtaxaCOI))\\n\\n## Get a quick overview of sequencing depth per sequencing run ##\\n\\n# Make data.table for plot\\n\\nread_sums\\u003c- data.table(as(sample_data(psCOI), \\\"data.frame\\\"),\\n                       TotalReads = sample_sums(psCOI), keep.rownames = TRUE)\\nsetnames(read_sums,\\\"rn\\\",\\\"SampleID\\\")\\n\\n# Violin plot based on sequencing events\\n\\nreads_plot \\u003c- ggplot(read_sums, aes(y=TotalReads,x=Sequenced,color=Sequenced)) + \\n  geom_violin() + \\n  geom_jitter(shape=16, position=position_jitter(0.2),size=1) +\\n  scale_x_discrete(limits=c(\\\"Jul-19\\\",\\\"Jan-20\\\",\\\"Sep-20\\\",\\\"Apr-21\\\",\\\"May-21\\\",\\\"Jan-22\\\",\\\"Aug-23\\\"))+\\n  theme_bw()+\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"Sequencing Depth\\\")\\n\\nreads_plot\\n\\nggsave(reads_plot,file=\\\"COI_sequencing_depth_runs.png\\\",height = 4,width = 6)\\n\\n##\\n\\n# Remove the negative controls\\n\\npsCOI_cleaned \\u003c- subset_samples(psCOI,ARMS!=\\\"blank\\\")\\n\\n# Remove the sediment samples and plankton samples (some sediment and plankton samples were sequenced as a trial during the initial phase of the ARMS program)\\n\\npsCOI_cleaned \\u003c- subset_samples(psCOI_cleaned,Fraction!=\\\"SED\\\" \\u0026 Fraction!=\\\"PS\\\")\\n\\n# Remove samples with a read number of zero\\n\\npsCOI_cleaned \\u003c- prune_samples(sample_sums(psCOI_cleaned) \\u003e 0, psCOI_cleaned)\\n\\n# Remove MOTUs which have a total abundance of zero after removing samples during all of the previous steps\\n\\npsCOI_cleaned\\u003c-prune_taxa(rowSums(otu_table(psCOI_cleaned))\\u003e0,psCOI_cleaned)\\n\\n# Save this phyloseq object as the most unfiltered ARMS data set\\n\\nsaveRDS(psCOI_cleaned,\\\"psCOI_unfiltered_ARMS.rds\\\")\\n\\n# get number of remaining samples and MOTUs\\n\\npsCOI_cleaned\\n\\n# get number of reads\\n\\nsum(sample_sums(psCOI_cleaned))\\n\\n# Get percentage of MOTUs classified at phylum level\\n\\nsubset_taxa(psCOI_cleaned,!is.na(Phylum))\\n\\n2889/10646\\n\\n# get percentage of reads classified at phylum level\\n\\nsum(sample_sums(subset_taxa(psCOI_cleaned,!is.na(Phylum))))/sum(sample_sums(psCOI_cleaned))\\n\\n# Get percentage of MOTUs classified at species level\\n\\nsubset_taxa(psCOI_cleaned,!is.na(Species))\\n\\n807/10646\\n\\n# get percentage of reads classified at phylum level\\n\\nsum(sample_sums(subset_taxa(psCOI_cleaned,!is.na(Species))))/sum(sample_sums(psCOI_cleaned))\\n\",\"os_name\":\"\",\"os_version\":\"\"}}}}","data":null,"protocol_id":86559,"case_id":0,"critical_ids":"","duration":0,"original_id":0,"number":"12","cases":[],"critical":null},{"id":1778491,"guid":"E2516226E92F42F58F15C4EC9C3F3A37","previous_id":1778236,"previous_guid":"51DE4706E8FA442E83159F7015186731","section":"\u003cp\u003eIdentify non-indigenous species based on WRiMS\u003c/p\u003e","section_color":"#E57785","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"a6l6m\",\"text\":\"Putative non-indigenous species (NIS) were identified from the MOTUs which could be classified to species level. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"a75qt\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"15igr\",\"text\":\"Species listed in the World Register of Introduced Marine Species (WRiMS) were obtained for the regions corresponding to the ARMS locations. WRiMS' Distributions browser was used for this (https://www.marinespecies.org/introduced/aphia.php?p=checklist). Multiple searches were performed with default settings and the respective region specified in the field Geonuit (see image below).\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":148,\"length\":13},{\"style\":\"italic\",\"offset\":358,\"length\":8}],\"entityRanges\":[{\"key\":0,\"offset\":189,\"length\":62}],\"data\":{}},{\"key\":\"blqmt\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"2cxk6\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":1,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"df51m\",\"text\":\"To keep the search standardized, it was carried out considering IHO Sea Areas if possible. However, there were no entries for some IHO Areas, e.g. the IHO Sea Area Skagerrak was not listed in WRiMS. Hence, in such cases Marine Regions were selected for the search instead. For the case of Skagerrak, this was the Marine Region Swedish part of the Skagerrak for example. In other cases, the IHO Areas comprised too large a geographical area (e.g., North Atlantic Ocean) containing regions irrelevant for this study. Therefore, the search was performed for subordinate IHO Sea Areas or, alternatively, Marine Regions within the respective region. WRiMS only allows downloads of files with a maximum of 1,000 entries. The IHO Sea Area Mediterranean Sea - Eastern Basin had more than 1,000 entries, so subordinate regions had to be selected for separate search queries (Adriatic Sea and Aegean Sea in this case). The table below shows the regions and their PlaceType according to Marine Regions (https://www.marineregions.org/) for which the taxa listed in WRiMS were obtained.  \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":164,\"length\":9},{\"style\":\"italic\",\"offset\":327,\"length\":30},{\"style\":\"italic\",\"offset\":447,\"length\":20},{\"style\":\"italic\",\"offset\":732,\"length\":33},{\"style\":\"italic\",\"offset\":866,\"length\":12},{\"style\":\"italic\",\"offset\":883,\"length\":10},{\"style\":\"italic\",\"offset\":953,\"length\":9}],\"entityRanges\":[{\"key\":2,\"offset\":992,\"length\":30}],\"data\":{}},{\"key\":\"ct27j\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"9rivt\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":3,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"9puiv\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8iahq\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"aa04r\",\"text\":\"The results for each search were downloaded as .xlsx files (with the pre-selected default columns included). See images below.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"b6oug\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"d0kvr\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":4,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"v5ld2\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":5,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"uheo8\",\"text\":\" \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8e3lv\",\"text\":\"The downloaded results were then combined in R / RStudio to an overall list of taxa listed in WRiMS for the study regions. See the script below:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":45,\"length\":11},{\"style\":\"italic\",\"offset\":94,\"length\":5}],\"entityRanges\":[],\"data\":{}},{\"key\":\"bt79u\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":6,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"6vj91\",\"text\":\"The result of this was the following file:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"e2s8h\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7kei2\",\"text\":\"   \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":7,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"cv7dp\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"8u2r3\",\"text\":\"The 18S and COI MOTUs which could be classified to species level were then scanned for taxa found in the WRiMS list. This was done for the phyloseq data sets created in the previous section. The taxa names in the reference sets used for taxonomic classification do not always equal the accepted names in WRiMS / WoRMS. Therefore, the species-level assignments in the 18S and COI data sets were first matched against the WoRMS database to obtain the respective accepted names for the taxa. LifeWatch Belgium's web services were used for the taxon matching (see https://www.lifewatch.be/data-services/). These web services actually include a taxon match service for WRiMS, but is was not functional at the time of usage. This may have been fixed by the time of publication of this protocol. First, a .txt file fulfilling the input requirements of the web service (i.e., MOTU IDs and the respective species names) was generated in R / RStudio for each marker gene. See script below:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":139,\"length\":8},{\"style\":\"italic\",\"offset\":489,\"length\":17},{\"style\":\"italic\",\"offset\":928,\"length\":11}],\"entityRanges\":[{\"key\":8,\"offset\":560,\"length\":39}],\"data\":{}},{\"key\":\"cejpv\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":9,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"2t1k\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"fpkrj\",\"text\":\"To run LifeWatch Belgium's e-Lab services, an account is required. Under \\\"Run Services\\\", the files just created were used as input and the service \\\"Taxon match services\\\" -\\u003e \\\"Taxon match World Register of Marine Species (WoRMS)\\\" was applied. The results were downloaded and then used to scan the phyloseq data sets in R / RStudio for taxa found in the WRiMS list. See script below:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":295,\"length\":8},{\"style\":\"italic\",\"offset\":317,\"length\":11}],\"entityRanges\":[],\"data\":{}},{\"key\":\"ftnk0\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":10,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"eco2k\",\"text\":\"The results of this script are count tables of MOTUs with species assignments (one each for COI and 18S) for taxa listed in WRiMS for the ARMS locations. Counts are given per ARMS sampling event (so all fraction samples were merged for each single ARMS deployment).  \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"fnanv\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{\"0\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"85D49AA947E311EE902A0A58A9FEAC02\",\"url\":\"https://www.marinespecies.org/introduced/aphia.php?p=checklist\"}},\"1\":{\"type\":\"image\",\"mutability\":\"MUTABLE\",\"data\":{\"height\":338,\"id\":400867,\"is_video\":false,\"legend\":\"{\\\"blocks\\\":[{\\\"key\\\":\\\"6n9c4\\\",\\\"text\\\":\\\"The interface of WRiMS' Distributions browser.\\\",\\\"type\\\":\\\"unstyled\\\",\\\"depth\\\":0,\\\"inlineStyleRanges\\\":[{\\\"style\\\":\\\"italic\\\",\\\"offset\\\":24,\\\"length\\\":13}],\\\"entityRanges\\\":[],\\\"data\\\":{}}],\\\"entityMap\\\":{}}\",\"mime\":\"image/png\",\"original_name\":\"image.png\",\"placeholder\":\"https://protocols-files.s3.amazonaws.com/files/nhrebimqf.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\\u0026X-Amz-Credential=AKIAJYFAX46LHRVQMGOA%2F20241028%2Fus-east-1%2Fs3%2Faws4_request\\u0026X-Amz-Date=20241028T200101Z\\u0026X-Amz-Expires=604800\\u0026X-Amz-SignedHeaders=host\\u0026X-Amz-Signature=b019cb16bf3f58df7e62156d860c7fcb2921a4acfd07b76a5b7cb71325a54843\",\"source\":\"https://protocols-files.s3.amazonaws.com/files/nhrdbimqf.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\\u0026X-Amz-Credential=AKIAJYFAX46LHRVQMGOA%2F20241028%2Fus-east-1%2Fs3%2Faws4_request\\u0026X-Amz-Date=20241028T200101Z\\u0026X-Amz-Expires=604800\\u0026X-Amz-SignedHeaders=host\\u0026X-Amz-Signature=437a3c994c88d01c1260666f30606a8e7285b85bd6bcae83dd9d5941fba5fc96\",\"type\":1,\"width\":550}},\"10\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Scan 18S and COI for NIS recorded in WRiMS\",\"description\":\"\",\"guid\":\"1B885C28480111EE902A0A58A9FEAC02\",\"name\":\"library(phyloseq)\\nlibrary(dplyr)\\n\\n### COI ###\\n\\nsetwd(\\\"~/COI\\\")\\n\\n# Load phyloseq object\\n\\nunfiltered\\u003c-readRDS(\\\"psCOI_unfiltered_ARMS.rds\\\")\\n\\n# read the results file from the Taxon match World Register of Marine Species (WoRMS) service of LifeWatch\\n\\nworms_results\\u003c-read.table(\\\"result_arms_coi_species_check_worms.txt\\\",sep=\\\"\\\\t\\\",header=T)\\n\\n# In some cases, there were some fuzzy or non-exact matches of our taxa names vs. the ones found in WoRMS\\n# In such cases, the entry in the column \\\"accepted_name_aphia_worms\\\" will be empty\\n# replace these entries with our original names from the \\\"scientificname\\\" column\\n\\nworms_results$accepted_name_aphia_worms\\u003c-ifelse(worms_results$accepted_name_aphia_worms==\\\"\\\",worms_results$scientificname,worms_results$accepted_name_aphia_worms)\\n\\n# Read prepared NIS list \\n\\nnis\\u003c-read.table(\\\"WRiMS_ARMS_locations_NIS_List.txt\\\",sep=\\\"\\\\t\\\",header=T)\\n\\n# Make table with NIS listed in WRiMS which are found in our data set\\n\\nnis_found\\u003c-worms_results %\\u003e% filter(accepted_name_aphia_worms %in% nis$ScientificName)\\n\\n# Subset phyloseq object to these MOTUs and write them to file\\n\\nunfiltered_nis\\u003c-prune_taxa(nis_found$motu,unfiltered)\\nsaveRDS(unfiltered_nis,\\\"unfiltered_COI_set_NIS.rds\\\")\\n\\n# Make tables with NIS found in the phyloseq object with relevant info and write to file\\n\\ntaxa_unfiltered \\u003c-worms_results %\\u003e% filter(motu %in% taxa_names(unfiltered_nis))\\ncolnames(nis)[2]\\u003c-\\\"accepted_name_aphia_worms\\\"\\ntaxa_unfiltered\\u003c-merge(taxa_unfiltered, nis, by=\\\"accepted_name_aphia_worms\\\")\\ntaxa_unfiltered\\u003c-taxa_unfiltered[,-c(3:11)]\\n\\nwrite.table(taxa_unfiltered,\\\"unfiltered_COI_set_NIS_identified.txt\\\",sep = \\\"\\\\t\\\",row.names = F,quote=F)\\n\\n## Get distribution and abundance information for the most unfiltered NIS data set ##\\n\\n# Make phyloseq object with samples merged for each ARMS sampling event\\n\\nvariable1 = as.character(get_variable(unfiltered_nis, \\\"ARMS\\\"))\\nvariable2 = as.character(get_variable(unfiltered_nis, \\\"Deployment\\\"))\\nvariable3 = as.character(get_variable(unfiltered_nis, \\\"Retrieval\\\"))\\nsample_data(unfiltered_nis)$sample_event \\u003c- paste(variable1, variable2, variable3,sep=\\\"_\\\")\\nsaveRDS(unfiltered_nis,\\\"unfiltered_nis.rds\\\")\\nunfiltered_nis_arms\\u003c-merge_samples(unfiltered_nis,\\\"sample_event\\\") # ATTENTION: this transposes otu_table\\notu_table(unfiltered_nis_arms)\\u003c-t(otu_table(unfiltered_nis_arms))\\nsaveRDS(unfiltered_nis_arms,\\\"unfiltered_nis_arms.rds\\\")\\n\\n# Combine count and taxonomy table and replace species name with accepted Aphia name \\n\\nnis_taxa_counts\\u003c-data.frame(tax_table(unfiltered_nis_arms)[,7],otu_table(unfiltered_nis_arms),check.names = F)\\nfor (i in 1:nrow(nis_found)) {\\n  nis_taxa_counts$Species[rownames(nis_taxa_counts) == nis_found$motu[i]] \\u003c- nis_found$accepted_name_aphia_worms[i]\\n}\\n\\nwrite.table(nis_taxa_counts,\\\"nis_taxa_counts_COI.txt\\\",sep=\\\"\\\\t\\\",col.names=NA)\\n\\n### 18S ###\\n\\nsetwd(\\\"~/18S\\\")\\n\\n# Load phyloseq object\\n\\nunfiltered\\u003c-readRDS(\\\"ps18S_unfiltered_ARMS.rds\\\")\\n\\n# read the results file from the Taxon match World Register of Marine Species (WoRMS) service of LifeWatch\\n\\nworms_results\\u003c-read.table(\\\"result_arms_18S_species_check_worms.txt\\\",sep=\\\"\\\\t\\\",header=T)\\n\\n# In some cases, there were some fuzzy or non-exact matches of our taxa names vs. the ones found in WoRMS\\n# In such cases, the entry in the column \\\"accepted_name_aphia_worms\\\" will be empty\\n# replace these entries with our original names from the \\\"scientificname\\\" column\\n\\nworms_results$accepted_name_aphia_worms\\u003c-ifelse(worms_results$accepted_name_aphia_worms==\\\"\\\",worms_results$scientificname,worms_results$accepted_name_aphia_worms)\\n\\n# Read prepared list of NIS registered in WRiMS for our regions of interest\\n\\nnis\\u003c-read.table(\\\"WRiMS_ARMS_locations_NIS_List.txt\\\",sep=\\\"\\\\t\\\",header=T)\\n\\n# Make table with NIS listed in WRiMS which are found in our data set\\n\\nnis_found\\u003c-worms_results %\\u003e% filter(accepted_name_aphia_worms %in% nis$ScientificName)\\n\\n# Subset phyloseq object to these MOTUs and write them to files\\n\\nunfiltered_nis\\u003c-prune_taxa(nis_found$motu,unfiltered)\\n\\nsaveRDS(unfiltered_nis,\\\"unfiltered_18S_set_NIS.rds\\\")\\n\\n# Make table with NIS found in each of the phyloseq object, with relevant info and write to file\\n\\ntaxa_unfiltered \\u003c-worms_results %\\u003e% filter(motu %in% taxa_names(unfiltered_nis))\\ncolnames(nis)[2]\\u003c-\\\"accepted_name_aphia_worms\\\"\\ntaxa_unfiltered\\u003c-merge(taxa_unfiltered, nis, by=\\\"accepted_name_aphia_worms\\\")\\ntaxa_unfiltered\\u003c-taxa_unfiltered[,-c(3:11)]\\n\\nwrite.table(taxa_unfiltered,\\\"unfiltered_18S_set_NIS_identified.txt\\\",sep = \\\"\\\\t\\\",row.names = F,quote=F)\\n\\n## Get distribution and abundance information for the most unfiltered NIS data set ##\\n\\n# Make phyloseq object with samples merged for each ARMS sampling event\\n\\nvariable1 = as.character(get_variable(unfiltered_nis, \\\"ARMS\\\"))\\nvariable2 = as.character(get_variable(unfiltered_nis, \\\"Deployment\\\"))\\nvariable3 = as.character(get_variable(unfiltered_nis, \\\"Retrieval\\\"))\\nsample_data(unfiltered_nis)$sample_event \\u003c- paste(variable1, variable2, variable3,sep=\\\"_\\\")\\nsaveRDS(unfiltered_nis,\\\"unfiltered_nis.rds\\\")\\nunfiltered_nis_arms\\u003c-merge_samples(unfiltered_nis,\\\"sample_event\\\") # ATTENTION: this transposes otu_table\\notu_table(unfiltered_nis_arms)\\u003c-t(otu_table(unfiltered_nis_arms))\\nsaveRDS(unfiltered_nis_arms,\\\"unfiltered_nis_arms.rds\\\")\\n\\n# Combine count and taxonomy table and replace species name with accepted Aphia name in WoRMS / WRiMS\\n\\nnis_taxa_counts\\u003c-data.frame(tax_table(unfiltered_nis_arms)[,10],otu_table(unfiltered_nis_arms),check.names = F)\\nnis_taxa_counts$Species\\u003c-gsub(\\\"_\\\",\\\" \\\",nis_taxa_counts$Species)\\nfor (i in 1:nrow(nis_found)) {\\n  nis_taxa_counts$Species[rownames(nis_taxa_counts) == nis_found$motu[i]] \\u003c- nis_found$accepted_name_aphia_worms[i]\\n}\\n\\nwrite.table(nis_taxa_counts,\\\"nis_taxa_counts_18S.txt\\\",sep=\\\"\\\\t\\\",col.names = NA)\",\"os_name\":\"\",\"os_version\":\"\"}},\"2\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"303E75E847FC11EE902A0A58A9FEAC02\",\"url\":\"https://www.marineregions.org\"}},\"3\":{\"type\":\"tables\",\"mutability\":\"IMMUTABLE\",\"data\":{\"cellsMeta\":{\"B_1\":{\"className\":\"_dt-h-center\"},\"B_10\":{\"className\":\"_dt-h-center\"},\"B_11\":{\"className\":\"_dt-h-center\"},\"B_12\":{\"className\":\"_dt-h-center\"},\"B_2\":{\"className\":\"_dt-h-center\"},\"B_3\":{\"className\":\"_dt-h-center\"},\"B_4\":{\"className\":\"_dt-h-center\"},\"B_5\":{\"className\":\"_dt-h-center\"},\"B_6\":{\"className\":\"_dt-h-center\"},\"B_7\":{\"className\":\"_dt-h-center\"},\"B_8\":{\"className\":\"_dt-h-center\"},\"B_9\":{\"className\":\"_dt-h-center\"}},\"colTitles\":[\"Geounit\",\"Marine Regions PlaceType \"],\"colWidths\":[\"291\",\"197\"],\"data\":[[\"Adriatic Sea\",\"IHO Sea Area\"],[\"Aegean Sea \",\"IHO Sea Area\"],[\"Arctic Ocean\",\"IHO Sea Area\"],[\"Baltic Sea\",\"IHO Sea Area\"],[\"English Channel\",\"IHO Sea Area\"],[\"Irish part of the North Atlantic Ocean\",\"Marine Region\"],[\"North Sea\",\"IHO Sea Area\"],[\"Norwegian part of the Norwegian Sea \",\"Marine Region\"],[\"Red Sea\",\"IHO Sea Area\"],[\"Spanish part of the Bay of Biscay\",\"Marine Region\"],[\"Spanish part of the North Atlantic Ocean\",\"Marine Region\"],[\"Swedish part of the Skagerrak\",\"Marine Region\"]],\"guid\":\"45C5BB70FE864AC5BAD20A7392CA6AB8\",\"isJexcelDataFormat\":true,\"legend\":{\"blocks\":[{\"data\":{},\"depth\":0,\"entityRanges\":[{\"key\":0,\"length\":30,\"offset\":57}],\"inlineStyleRanges\":[{\"length\":9,\"offset\":18,\"style\":\"italic\"}],\"key\":\"amam9\",\"text\":\"Regions and their PlaceType according to Marine Regions (https://www.marineregions.org/) for which the taxa listed in WRiMS were obtained.\",\"type\":\"unstyled\"}],\"entityMap\":{\"0\":{\"data\":{\"guid\":\"303E75E847FC11EE902A0A58A9FEAC02\",\"url\":\"https://www.marineregions.org\"},\"mutability\":\"MUTABLE\",\"type\":\"link\"}}},\"mergeCells\":{},\"printData\":[[\"Adriatic Sea\",\"IHO Sea Area\"],[\"Aegean Sea \",\"IHO Sea Area\"],[\"Arctic Ocean\",\"IHO Sea Area\"],[\"Baltic Sea\",\"IHO Sea Area\"],[\"English Channel\",\"IHO Sea Area\"],[\"Irish part of the North Atlantic Ocean\",\"Marine Region\"],[\"North Sea\",\"IHO Sea Area\"],[\"Norwegian part of the Norwegian Sea \",\"Marine Region\"],[\"Red Sea\",\"IHO Sea Area\"],[\"Spanish part of the Bay of Biscay\",\"Marine Region\"],[\"Spanish part of the North Atlantic Ocean\",\"Marine Region\"],[\"Swedish part of the Skagerrak\",\"Marine Region\"]],\"rowHeights\":[23,23,23,23,23,23,23,23,23,23,23,23]}},\"4\":{\"type\":\"image\",\"mutability\":\"MUTABLE\",\"data\":{\"height\":306,\"id\":400869,\"is_video\":false,\"legend\":\"{\\\"blocks\\\":[{\\\"key\\\":\\\"3uhqf\\\",\\\"text\\\":\\\"Download link for the search results of each region's listed taxa.\\\",\\\"type\\\":\\\"unstyled\\\",\\\"depth\\\":0,\\\"inlineStyleRanges\\\":[],\\\"entityRanges\\\":[],\\\"data\\\":{}}],\\\"entityMap\\\":{}}\",\"mime\":\"image/png\",\"original_name\":\"image.png\",\"placeholder\":\"https://protocols-files.s3.amazonaws.com/files/nhrgbimqf.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\\u0026X-Amz-Credential=AKIAJYFAX46LHRVQMGOA%2F20241028%2Fus-east-1%2Fs3%2Faws4_request\\u0026X-Amz-Date=20241028T200101Z\\u0026X-Amz-Expires=604800\\u0026X-Amz-SignedHeaders=host\\u0026X-Amz-Signature=2f6bf1ed75db0a4a8f5f643f7c52f2ee1953a6c0e65d78f4985bcdb1eca20004\",\"source\":\"https://protocols-files.s3.amazonaws.com/files/nhrfbimqf.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\\u0026X-Amz-Credential=AKIAJYFAX46LHRVQMGOA%2F20241028%2Fus-east-1%2Fs3%2Faws4_request\\u0026X-Amz-Date=20241028T200101Z\\u0026X-Amz-Expires=604800\\u0026X-Amz-SignedHeaders=host\\u0026X-Amz-Signature=9a46d15d8a43fef3a7b7c72bde798351cf93d3dd312ae6207f2b7db28ee59f70\",\"type\":1,\"width\":550}},\"5\":{\"type\":\"image\",\"mutability\":\"MUTABLE\",\"data\":{\"height\":528,\"id\":400875,\"is_video\":false,\"legend\":\"{\\\"blocks\\\":[{\\\"key\\\":\\\"bh8gk\\\",\\\"text\\\":\\\"Download interface of WRiMS' Distributions browser.\\\",\\\"type\\\":\\\"unstyled\\\",\\\"depth\\\":0,\\\"inlineStyleRanges\\\":[{\\\"style\\\":\\\"italic\\\",\\\"offset\\\":29,\\\"length\\\":13}],\\\"entityRanges\\\":[],\\\"data\\\":{}}],\\\"entityMap\\\":{}}\",\"mime\":\"image/png\",\"original_name\":\"image.png\",\"placeholder\":\"https://protocols-files.s3.amazonaws.com/files/nhrnbimqf.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\\u0026X-Amz-Credential=AKIAJYFAX46LHRVQMGOA%2F20241028%2Fus-east-1%2Fs3%2Faws4_request\\u0026X-Amz-Date=20241028T200101Z\\u0026X-Amz-Expires=604800\\u0026X-Amz-SignedHeaders=host\\u0026X-Amz-Signature=9c3927b048406057e1adcb6a08746b00928c32e65381877b4cb8397e35b3d87f\",\"source\":\"https://protocols-files.s3.amazonaws.com/files/nhrmbimqf.png?X-Amz-Algorithm=AWS4-HMAC-SHA256\\u0026X-Amz-Credential=AKIAJYFAX46LHRVQMGOA%2F20241028%2Fus-east-1%2Fs3%2Faws4_request\\u0026X-Amz-Date=20241028T200101Z\\u0026X-Amz-Expires=604800\\u0026X-Amz-SignedHeaders=host\\u0026X-Amz-Signature=26ebbdedf5a2870eca4edaac7a96a3b40bbdf99a4600f61c2215cc6d8dc9584b\",\"type\":1,\"width\":550}},\"6\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Create overall WRiMS NIS list in R \",\"description\":\"\",\"guid\":\"9B5D12C347FE11EE902A0A58A9FEAC02\",\"name\":\"library(xlsx)\\nlibrary(tidyr)\\n\\nsetwd(\\\"~/NIS\\\")\\n\\n# Read the tables downloaded from WRiMS\\n\\nlist.files() # Briefly check the file names\\n\\narctic\\u003c-read.xlsx(\\\"Arctic Ocean IHO Sea Area.xlsx\\\",sheetIndex = 1)\\nnorway\\u003c-read.xlsx(\\\"Norwegian part of the Norwegian Sea Marine Region.xlsx\\\",sheetIndex = 1)\\nnorthsea\\u003c-read.xlsx(\\\"North Sea IHO Sea Area.xlsx\\\",sheetIndex = 1)\\nskagerrak\\u003c-read.xlsx(\\\"Swedish part of the Skagerrak Marine Region.xlsx\\\",sheetIndex = 1)\\nbaltic\\u003c-read.xlsx(\\\"Baltic Sea IHO Sea Area.xlsx\\\",sheetIndex = 1)\\nchannel\\u003c-read.xlsx(\\\"English Channel IHO Sea Area.xlsx\\\",sheetIndex = 1)\\nbiscaya\\u003c-read.xlsx(\\\"Spanish part of the Bay of Biscay Marine Region.xlsx\\\",sheetIndex = 1)\\nspain\\u003c-read.xlsx(\\\"Spanish part of the North Atlantic Ocean Marine Region.xlsx\\\",sheetIndex = 1)\\nadriatic\\u003c-read.xlsx(\\\"Adriatic Sea IHO Sea Area.xlsx\\\",sheetIndex = 1)\\naegean\\u003c-read.xlsx(\\\"Aegean Sea IHO Sea Area.xlsx\\\",sheetIndex = 1)\\nirish\\u003c-read.xlsx(\\\"Irish part of the North Atlantic Ocean Marine Region.xlsx\\\",sheetIndex = 1)\\n\\n# Combine all tables\\n\\nall_lists\\u003c-rbind(arctic,norway,northsea,skagerrak,baltic,channel,biscaya,spain,adriatic,aegean,redsea,irish)\\n\\n# Remove unnecessary columns\\n\\nnis\\u003c-all_lists[,-c(3,4,6:8)]\\n\\n# Aggregate by species and Aphia ID to get a column with a string containing all localities the species is considered a NIS\\n\\nnis\\u003c-aggregate(Locality ~ AphiaID + ScientificName, data = nis, paste, collapse = \\\",\\\")\\n\\n# Separate the strings in Locality column into single columns \\n\\nnis\\u003c-separate_wider_delim(nis,Locality, delim = \\\",\\\", names_sep=\\\"\\\",too_few = \\\"align_start\\\")\\n\\n# Write overall NIS table to file\\n\\nwrite.table(nis,\\\"WRiMS_ARMS_locations_NIS_List.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\",\"os_name\":\"\",\"os_version\":\"\"}},\"7\":{\"type\":\"file\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"\",\"id\":446956,\"original_name\":\"WRiMS_ARMS_locations_NIS_List.txt\",\"placeholder\":\"https://www.protocols.io/img/extensions/txt.png\",\"raw_source\":\"\",\"size\":207650,\"source\":\"https://content.protocols.io/files/pwrnbimqf.txt\"}},\"8\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"73939CDE480511EE902A0A58A9FEAC02\",\"url\":\"https://www.lifewatch.be/data-services\"}},\"9\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Generate files in R for LifeWatch's WoRMS Taxon Match service\",\"description\":\"\",\"guid\":\"0539E4EB480711EE902A0A58A9FEAC02\",\"name\":\"library(phyloseq)\\n\\n### COI ###\\n\\nsetwd(\\\"~/COI\\\")\\n\\n# Load most unfiltered phyloseq object\\n\\nunfiltered\\u003c-readRDS(\\\"psCOI_unfiltered_ARMS.rds\\\")\\n\\n# The unfiltered data set is the least exclusive, i.e., all MOTUs appearing in this data set are found in all potential filtered data sets\\n# Get a table of the unfiltered data set with MOTU names and Genus and Species assignments in one column for taxa classified down to species level\\n\\ntaxa\\u003c-cbind(rownames(tax_table(subset_taxa(unfiltered,!is.na(Species)))),tax_table(subset_taxa(unfiltered,!is.na(Species)))[,7])\\ncolnames(taxa)\\u003c-c(\\\"MOTU\\\",\\\"ScientificName\\\") # The LifeWatch e-Lab service used later on needs the Species column to be named \\\"ScientificName\\\"\\n\\n# Write table to file which will be checked for correct species names in WoRMS via LifeWatch Belgium's e-Lab services https://www.lifewatch.be/data-services/\\n\\nwrite.table(taxa,\\\"ARMS_COI_species_check_WoRMS.txt\\\",sep=\\\"\\\\t\\\",quote=F,row.names = F)\\n\\n### 18S ###\\n\\nsetwd(\\\"~/18S\\\")\\n\\n# Load most unfiltered phyloseq object\\n\\nunfiltered\\u003c-readRDS(\\\"ps18S_unfiltered_ARMS.rds\\\")\\n\\n# The unfiltered data set is the least exclusive, i.e., all MOTUs appearing in this data set are found in all potential filtered data sets\\n# Get a table of the unfiltered data set with MOTU names and Genus and Species assignments in one column for taxa classified down to species level\\n\\ngen_spec\\u003c-subset_taxa(unfiltered,!is.na(Species))\\ntax_table(gen_spec)[,10]\\u003c-gsub(\\\"_\\\",\\\" \\\",tax_table(gen_spec)[,10])\\ntaxa\\u003c-cbind(rownames(tax_table(gen_spec)),tax_table(gen_spec)[,10])\\ncolnames(taxa)\\u003c-c(\\\"MOTU\\\",\\\"ScientificName\\\") # The LifeWatch e-Lab service used later on needs the Species column to be named \\\"ScientificName\\\"\\n\\n# Write table to file which will be checked for correct species names in WoRMS via LifeWatch Belgium's e-Lab services https://www.lifewatch.be/data-services/\\n\\nwrite.table(taxa,\\\"ARMS_18S_species_check_WoRMS.txt\\\",sep=\\\"\\\\t\\\",quote=F,row.names = F)\",\"os_name\":\"\",\"os_version\":\"\"}}}}","data":null,"protocol_id":86559,"case_id":0,"critical_ids":"","duration":0,"original_id":0,"number":"13","cases":[],"critical":null},{"id":1987342,"guid":"3D75086CD214439B93EB371AE05D64E0","previous_id":1766861,"previous_guid":"E0422DAF3C5411EE882A0A58A9FEAC02","section":"\u003cp\u003eCreating data sets suitable for comparison of NIS prevalence\u003c/p\u003e","section_color":"#A492FF","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"10jq3\",\"text\":\"The previously created data sets represent the most unfiltered MOTU data sets for each marker gene. To be able to assess the prevalence of NIS a) among deployment sites, and b) among types of deployment sites (i.e., marinas/ports/harbours vs. all other types), we created comparable data sets of equal sampling and sequencing effort for each gene. The following steps are performed for each gene's data set in the R scripts found below:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":183,\"length\":5},{\"style\":\"italic\",\"offset\":414,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"bslf0\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"c9hlt\",\"text\":\"The starting point are the previously created unfiltered MOTU phyloseq objects, which are read into R.\",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":62,\"length\":8},{\"style\":\"italic\",\"offset\":100,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"37i72\",\"text\":\"In August 2023, some material samples were re-sequenced due to initially poor read yield. In most of those cases, two genetic samples for the respective MaterialSampleID are therefore present in the data set. Based on assessment of rarefaction curves, OTU counts and taxonomy profiles for each of these sample pairs, the sample with lower diversity or taxonomic resolution will be removed.\",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"ask0a\",\"text\":\"As a trial during the initial phase of the ARMS MBON program, some samples were preserved in DESS/DMSO as well as ethanol. Where both replicates remain in the data set, the sample preserved in ethanol will be removed. Where samples have been preserved in ethanol only or where only the enthanol replicate remained in the data set, those will be kept. \",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"14aa9\",\"text\":\"There are a couple of samples from Roscoff, France, which have been processed as duplicates. The replicate showing higher read counts and MOTU richness based on rarefaction curves will be kept.\",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"a79bn\",\"text\":\"Samples with less than 10,000 reads remaining are removed. \",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"3hmk6\",\"text\":\"Samples are rarefied to 10,000 reads to achieve even sequencing depth. This is now the cleaned and rarefied data set which will be saved!\",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":71,\"length\":66}],\"entityRanges\":[],\"data\":{}},{\"key\":\"26a9o\",\"text\":\"The previous step created a rarefied MOTU data set. However, we also need to rarefy the initial ASV data set (product of blank correction, see above) because we previously checked ASV occurrences within MOTUs to see if a NIS occurrence can actually be considered as such. So the blank-corrected ASV data sets will be subset to the samples remaining in the rarefied MOTU data sets and then rarefied to an equal sequencing depth of 10,000 reads.\",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"7mq3m\",\"text\":\"The curated and filtered NIS count tables (see section above) are read into R and subset to the MOTUs and ASVs remaining after the rarefaction procedures. Occurrences of NIS MOTUs which are now left with less than 10 reads per sampling event (because ASVs got removed) are set to zero.\",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":76,\"length\":1}],\"entityRanges\":[],\"data\":{}},{\"key\":\"7b717\",\"text\":\"Phyloseq objects containing just the NIS MOTUs and their curated and filtered occurrences as NIS in the samples remaining after rarefaction are generated.\",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":0,\"length\":8}],\"entityRanges\":[],\"data\":{}},{\"key\":\"282nd\",\"text\":\"Statistical tests are performed to assess differences in NIS richness between samples of marina / harbour/ port sites and all remaining sites.\",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":0,\"length\":142}],\"entityRanges\":[],\"data\":{}},{\"key\":\"935ab\",\"text\":\"Info on deployment duration in days is added to the phyloseq objects. In Addition, info on whether samples of all three size or both motile fractions remained for each ARMS unit is added. Five NIS MOTU phyloseq objects are generated with ARMS units for which the samples of a) all three fractions; b) both motile fractions; c) the sessile fraction; d) the motile 100um fraction; e) the motile 500um fraction is/are remaining. \",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":202,\"length\":8}],\"entityRanges\":[],\"data\":{}},{\"key\":\"526r7\",\"text\":\"Some ARMS units have been deployed for multiple, consecutive periods at the same spot (i.e., same ARMS ID, different sampling events). This will of course be the norm as the ARMS-MBON project progresses. However, as the samples processed here stem from the initial years of this monitoring network, some sites contain ARMS units which were only deployed once. Hence, to achieve similar sampling effort among sites for downstream analysis, the sampling events will be assessed visually and the data sets subset to keep only the samples of one sampling event per ARMS. For each of the five phyloseq objects (see 11.) only one sampling event is kept for each ARMS ID. The following procedure is applied for selecting sampling events: a) keep the sampling event that temporally overlaps with most of the sampling events of other ARMS, b) within a site = Field_Replicate group with multiple field replicates, keep the sampling events that temporally overlap even if that means some ARMS units will be removed as long as two ARMS units remain within this site = Field_Replicate group. The minimum number of two units is enforced because in the next step two ARMS units are randomly selected for each site = Field_Replicate group to achieve similar sampling effort (this number was chosen after assessing the remaining number of ARMS units for all site = Field_Replicate groups and determining that most sites had two or more units remaining). \",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":588,\"length\":8}],\"entityRanges\":[],\"data\":{}},{\"key\":\"8fea\",\"text\":\"Randomly select two ARMS units for each for site = Field_Replicate group to achieve similar sampling effort. For site = Field_Replicate groups with only one ARMS remaining, this unit is kept. These five phyloseq objects ( ARMS with samples of a) all three fractions; b) both motile fractions; c) the sessile fraction; d) the motile 100um fraction; e) the motile 500um fraction remaining) will be saved for each marker gene. \",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"bold\",\"offset\":192,\"length\":195},{\"style\":\"bold\",\"offset\":388,\"length\":35},{\"style\":\"italic\",\"offset\":203,\"length\":8}],\"entityRanges\":[],\"data\":{}},{\"key\":\"1b6ub\",\"text\":\"For each gene, an.xslx file is written. It contains two sheets for each of the five phyloseq objects. One sheet giving the number of NIS found at each site = Field_Replicate group and if this group had one or two ARMS replicates in the curated data set had remaining. The second sheet contains the full taxonomy of each NIS and a table showing presence-absence occurrences for each site = Field_Replicate group. \",\"type\":\"ordered-list-item\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"ejev4\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"5h786\",\"text\":\"All steps described above are performed in R / RStudio with the script below:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":43,\"length\":11}],\"entityRanges\":[],\"data\":{}},{\"key\":\"257vn\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":0,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"1haqn\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"eobce\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":1,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"bc66d\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{\"0\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Creating comparable COI NIS data set in R\",\"description\":\"\",\"guid\":\"A6D84E37F73711EEAF630A58A9FEAC02\",\"name\":\"library(phyloseq)\\nlibrary(dplyr)\\nlibrary(vegan)\\nlibrary(ggplot2)\\nlibrary(tidyr)\\nlibrary(ggpubr)\\nlibrary(data.table)\\nlibrary(xlsx)\\n\\nsetwd(\\\"~/COI\\\")\\n\\n# read the unfiltered phyloseqy object\\n\\npsCOI_cleaned\\u003c-readRDS(\\\"psCOI_unfiltered_ARMS.rds\\\")\\n\\n# Some samples were re-sequenced in August 2023 and in most of those cases two genetic samples for the respective MaterialSampleID are therefore present in the data set\\n# Assess rarefaction curves, OTU counts and taxonomy for each of those sample pairs and remove sample with lower diversity or taxonomic resolution\\n\\nsample_data(psCOI_cleaned)[duplicated(sample_data(psCOI_cleaned)$MaterialSampleID),\\\"MaterialSampleID\\\"] # Check which MaterialSampleIDs appear twice\\n\\nfornace\\u003c-subset_samples(psCOI_cleaned,MaterialSampleID==\\\"ARMS_GulfOfPiran_Fornace_20180815_20181118_SF_ETOH\\\")\\nfornace\\u003c-prune_taxa(rowSums(otu_table(fornace))\\u003e0,fornace)\\nrarecurve(t(otu_table(fornace)), step=50, cex=0.5)\\nfornace\\u003c-cbind(tax_table(fornace),otu_table(fornace))\\nfornace\\n\\nkatza\\u003c-subset_samples(psCOI_cleaned,MaterialSampleID==\\\"ARMS_Eilat_Katza1_20181024_20200706_MF500\\\")\\nkatza\\u003c-prune_taxa(rowSums(otu_table(katza))\\u003e0,katza)\\nrarecurve(t(otu_table(katza)), step=50, cex=0.5)\\nkatza\\u003c-cbind(tax_table(katza),otu_table(katza))\\nkatza\\n\\nkoster\\u003c-subset_samples(psCOI_cleaned,MaterialSampleID==\\\"ARMS_Koster_VH1_20190527_20200716_MF100\\\")\\nkoster\\u003c-prune_taxa(rowSums(otu_table(koster))\\u003e0,koster)\\nrarecurve(t(otu_table(koster)), step=50, cex=0.5)\\nkoster\\u003c-cbind(tax_table(koster),otu_table(koster))\\nkoster\\n\\ntorallaB\\u003c-subset_samples(psCOI_cleaned,MaterialSampleID==\\\"ARMS_Vigo_TorallaB_20190625_20191014_MF500\\\")\\ntorallaB\\u003c-prune_taxa(rowSums(otu_table(torallaB))\\u003e0,torallaB)\\nrarecurve(t(otu_table(torallaB)), step=50, cex=0.5)\\ntorallaB\\u003c-cbind(tax_table(torallaB),otu_table(torallaB))\\ntorallaB\\n\\ntorallaC\\u003c-subset_samples(psCOI_cleaned,MaterialSampleID==\\\"ARMS_Vigo_TorallaC_20190625_20191014_MF500\\\")\\ntorallaC\\u003c-prune_taxa(rowSums(otu_table(torallaC))\\u003e0,torallaC)\\nrarecurve(t(otu_table(torallaC)), step=50, cex=0.5)\\ntorallaC\\u003c-cbind(tax_table(torallaC),otu_table(torallaC))\\ntorallaC\\n\\nto_remove_samples\\u003c-c(\\\"ERR7127624\\\",\\\"ERR7127581\\\",\\\"ERR12541475\\\",\\\"ERR4018719\\\",\\\"ERR4018721\\\")\\n\\npsCOI_cleaned \\u003c- prune_samples(!(sample_names(psCOI_cleaned) %in% to_remove_samples), psCOI_cleaned)\\n\\n# Some samples have been preserved in DMSO as well as EtOH initially as a trial. Where samples have been preserved in both, keep only DMSO samples.\\n\\nsample_data(psCOI_cleaned)[order(sample_data(psCOI_cleaned)$MaterialSampleID),1] # Check were MaterialSampleID is present twice with two preservatives\\n\\nto_remove_pres \\u003c-c(\\\"ERR9632064\\\",\\\"ERR3460469\\\",\\\"ERR3460467\\\",\\\"ERR7127579\\\",\\\"ERR12541472\\\",\\\"ERR4018450\\\",\\\"ERR4018615\\\",\\\"ERR7125592\\\",\\\"ERR7125594\\\",\\\"ERR7125596\\\",\\\"ERR7125598\\\",\\\"ERR7125634\\\",\\\"ERR7125639\\\",\\\"ERR7125641\\\",\\\"ERR7125643\\\")\\n\\npsCOI_cleaned \\u003c- prune_samples(!(sample_names(psCOI_cleaned) %in% to_remove_pres), psCOI_cleaned)\\n\\n# Two samples from Roscoff, France have been processed as replicates.\\n\\n# Assess rarefaction curves to see which sample to keep\\nrarecurve(t(otu_table(subset_samples(psCOI_cleaned,Lab_Replicate!=\\\"\\\"))), step=50, cex=0.5)\\n\\n# Remove the sample with lower sample size/MOTU abundance\\nto_remove_rep \\u003c-\\\"ERR7125633\\\"\\npsCOI_cleaned \\u003c- prune_samples(!(sample_names(psCOI_cleaned) %in% to_remove_rep), psCOI_cleaned)\\n\\n# Remove samples with a read number of \\u003c 10,000\\n\\npsCOI_cleaned \\u003c- prune_samples(sample_sums(psCOI_cleaned) \\u003e 10000, psCOI_cleaned)\\n\\n### Rarefaction procedure ###\\n\\n# To get equal sequencing depth, rarefy samples of the MOTU data set to 10,000 reads with default set.seed(1) \\n\\npsCOI_cleaned_rarefied \\u003c- rarefy_even_depth(psCOI_cleaned, rngseed=1, sample.size=10000, replace=F)\\nsaveRDS(psCOI_cleaned_rarefied,\\\"psCOI_cleaned_rarefied.rds\\\")\\n\\n## Rarefaction of ASV data set to get equal sequencing depth also for the sequences which are WITHIN the MOTUs\\n\\n# Read the ASV count table (result of blank correction) \\n\\nasv_counts\\u003c-read.table(\\\"asv_no_contaminants_COI.txt\\\",sep=\\\"\\\\t\\\",header=T)\\n\\n# Set ASV names as rownames\\nrownames(asv_counts) \\u003c-asv_counts[,1] \\nasv_counts[,1] \\u003c- NULL\\n\\n# Subset to samples which are present in psCOI_cleaned_rarefied and remove ASVs that have a read count of zero as a result\\n\\nasv_counts_select\\u003c-asv_counts[,colnames(asv_counts) %in% sample_names(psCOI_cleaned_rarefied)]\\nasv_counts_select\\u003c-asv_counts_select[rowSums(asv_counts_select[])\\u003e0,]\\n\\n# Rarefy to 10,000 reads per sample and remove ASVs that have a read count of zero as a result\\n\\nset.seed(1)\\nasv_counts_rarefied\\u003c-t(rrarefy(t(asv_counts_select),sample=10000))\\nasv_counts_rarefied\\u003c-asv_counts_rarefied[rowSums(asv_counts_rarefied[])\\u003e0,]\\n\\n## Create NIS data set based on rarefied MOTU and ASV data sets\\n\\n# read the previously created and manually curated table with MOTU-ASV NIS counts per sampling event\\n# Subset to MOTUs, ASVs and sample events still present after rarefaction\\n\\nasv_motu_counts\\u003c-read.table(\\\"NIS_MOTU_ASV_counts_COI_curated_filtered.txt\\\",sep=\\\"\\\\t\\\",header=T,check.names = F)\\nasv_motu_counts\\u003c-asv_motu_counts %\\u003e% filter(MOTU %in% taxa_names(psCOI_cleaned_rarefied))\\nasv_motu_counts\\u003c-asv_motu_counts %\\u003e% filter(ASV %in% rownames(asv_counts_rarefied))\\n# make sample_event data\\nvariable1 = as.character(get_variable(psCOI_cleaned_rarefied, \\\"ARMS\\\"))\\nvariable2 = as.character(get_variable(psCOI_cleaned_rarefied, \\\"Deployment\\\"))\\nvariable3 = as.character(get_variable(psCOI_cleaned_rarefied, \\\"Retrieval\\\"))\\nsample_data(psCOI_cleaned_rarefied)$sample_event \\u003c- paste(variable1, variable2, variable3,sep=\\\"_\\\")\\ncommon_cols \\u003c- intersect(colnames(asv_motu_counts[,-(1:3)]),sample_data(psCOI_cleaned_rarefied)$sample_event)\\nasv_motu_counts \\u003c- asv_motu_counts %\\u003e%  select(c(MOTU,Species,ASV,all_of(common_cols)))\\n\\n# Remove MOTUs in case they are only made up of occurrences with less than 10 reads per sampling event\\n\\nabundance_check\\u003c-asv_motu_counts %\\u003e% select(-c(Species,ASV))\\nabundance_check\\u003c-aggregate(.~ MOTU,abundance_check,FUN=sum)\\nrownames(abundance_check) \\u003c-abundance_check[,1] \\nabundance_check[,1] \\u003c- NULL\\nabundance_check[abundance_check \\u003c 10] \\u003c- 0\\nabundance_check\\u003c-abundance_check[rowSums(abundance_check[])\\u003e0,]\\nabundance_check\\u003c-abundance_check[, colSums(abundance_check != 0) \\u003e 0]\\n\\n# The previously created and curated NIS data set is based on sample events\\n# we will create a NIS phyloseq object based on individual samples, however occurrences belonging to sample events where NIS MOTUs were not considered as NIS need to be set to zero \\n\\nps_nis \\u003c- prune_taxa(taxa_names(psCOI_cleaned_rarefied) %in% rownames(abundance_check),psCOI_cleaned_rarefied)\\n\\n# Make a dataframes of NIS MOTU counts in long format to subsequently add the actual presence-absence info of NIS MOTUs\\nnis_counts\\u003c-as.data.frame(otu_table(ps_nis))\\nnis_counts$MOTU\\u003c-rownames(nis_counts)\\nnis_counts\\u003c-nis_counts %\\u003e% pivot_longer(cols=-MOTU)\\nabundance_check$MOTU\\u003c-rownames(abundance_check)\\nabundance_check\\u003c-abundance_check %\\u003e% pivot_longer(cols=-MOTU)\\n\\nfor(i in 1:nrow(sample_data(ps_nis))) { # add sample_event info to nisc_counts\\n     nis_counts$event[nis_counts$name == rownames(sample_data(ps_nis))[i]] \\u003c- sample_data(ps_nis)$sample_event[i]\\n} # ignore warning\\n\\nfor(j in 1:nrow(abundance_check)) { # for each MOTU-sample_event combination, add the curated NIS MOTU count\\n    nis_counts$count_new[nis_counts$event == abundance_check$name[j] \\u0026 nis_counts$MOTU == abundance_check$MOTU[j]] \\u003c- abundance_check$value[j]\\n} # ignore warning\\n\\nnis_counts$count_new[is.na(nis_counts$count_new)]\\u003c-0 # where a sample event was not present in the curated NIS data set because it contained no NIS, the count_new entry will be NA. we set it to zero.\\nnis_counts$value\\u003c-ifelse(nis_counts$count_new\\u003e0,nis_counts$value,0) # Where curated NIS MOTU count is zero, set count of MOTU in the respective samples to zero\\n\\n# There will be cases where the curated NIS MOTU count for a sample event is not zero (i.e., in the abundance_check object), but is zero in the rarefied phyloseq object (i.e., in ps_nis/nis_counts).\\n# This is the case when all counts of a NIS MOTU stem from a sample which was removed previously because it contained less than 10,00 reads\\n# The zero count will therefore be kept.\\n\\nnis_counts\\u003c-nis_counts[,-(4:5)] %\\u003e% pivot_wider(names_from = name,values_from = value)\\nnis_counts\\u003c-as.data.frame(nis_counts)\\nrownames(nis_counts)\\u003c-nis_counts$MOTU\\nnis_counts$MOTU \\u003c- NULL\\n\\n# Replace the otu_table of the NIS phyloseq object with the table we just created\\notu_table(ps_nis)\\u003c-otu_table(nis_counts,taxa_are_rows = TRUE)\\n\\n## Do statistical tests to check if NIS prevalence is higher at locations that are marinas / harbours / ports ##\\n\\n# Check levels of Monitoring_Area\\nunique(sample_data(ps_nis)$Monitoring_Area)\\n\\n# Subset samples of marinas, ports, harbours and estimate NIS richness\\nports\\u003c-subset_samples(ps_nis,Monitoring_Area %in% c(\\\"Marina/Harbour\\\",\\\"Marina\\\",\\\"Industrial port\\\",\\\"Harbour\\\"))\\nports_richness\\u003c-estimate_richness(ports,measures = \\\"Observed\\\")\\nports_richness$type\\u003c-\\\"port\\\"\\n\\n# Subset samples which are NOT from marinas, ports, harbours and estimate NIS richness\\nno_ports\\u003c-subset_samples(ps_nis,!Monitoring_Area %in% c(\\\"Marina/Harbour\\\",\\\"Marina\\\",\\\"Industrial port\\\",\\\"Harbour\\\"))\\nno_ports_richness\\u003c-estimate_richness(no_ports,measures = \\\"Observed\\\")\\nno_ports_richness$type\\u003c-\\\"no_port\\\"\\n\\n# Combine tables and do statistical tests (manually copy result from output field to Excel file)\\n\\nrichness\\u003c-rbind(ports_richness,no_ports_richness)\\nrichness$gene\\u003c-\\\"COI\\\" # add gene info to make plot with 18S data later on\\nwrite.table(richness,\\\"NIS_richness_COI_port_no_port.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\n# For The next command, make sure the plyr package is detached from your R session in case you used it for something else. \\n# It will not run properly if the plyr package has been loaded after dplyr (plyr is not part of this script, but could be in your environment from another script. Run detach(package:plyr) )\\n# Could also happen with ggplot2 packagae loaded.\\nrichness %\\u003e%  group_by(type) %\\u003e% summarize(mean = mean(Observed),sd=sd(Observed)) # Calculate Means and SD\\n\\n# Test normality\\nshapiro.test(richness$Observed) # significant\\nshapiro.test(sqrt(richness$Observed)) # significant\\nshapiro.test(log1p(richness$Observed)) # significant\\n\\n# Do non-parametric test\\nkruskal.test(Observed~type,richness)\\n\\n####\\n\\n# Add info on deployment duration\\n\\n# Format respective columns in sample_data as dates\\nsample_data(ps_nis)\\u003c-transform(sample_data(ps_nis),Deployment = as.Date(as.character(Deployment), \\\"%Y%m%d\\\"))\\nsample_data(ps_nis)\\u003c-transform(sample_data(ps_nis),Retrieval = as.Date(as.character(Retrieval), \\\"%Y%m%d\\\"))\\n\\n# Calculate number of days between retrieval and deployment and add a column\\nsample_data(ps_nis)$Deployment_Duration\\u003c-sample_data(ps_nis)$Retrieval-sample_data(ps_nis)$Deployment\\n\\n## Identify ARMS deployments for which all three fractions are present ##\\n\\n# Count samples present for each unique ARMS - Deployment - Retrieval combo\\n\\nfractions\\u003c-sample_data(ps_nis)  %\\u003e% count(ARMS, Deployment,Retrieval)\\n\\n# Make data.frame with column \\\"n\\\" representing number of samples for ARMS - Deployment - Retrieval combos\\n\\nfractions\\u003c- data.frame(lapply(fractions, function(x) Reduce(c, x)))\\n\\n# Merge sample_data and fractions table \\n\\nsamples_fractions\\u003c-merge(sample_data(ps_nis), fractions, by=c(\\\"ARMS\\\",\\\"Deployment\\\",\\\"Retrieval\\\"))\\n\\n# Sort this table based on MaterialSampleID in sample_data of ps_nis\\n\\nsamples_fractions\\u003c-samples_fractions[order(match(samples_fractions$MaterialSampleID,sample_data(ps_nis)$MaterialSampleID)),]\\n\\n# Add \\\"n\\\" column to sample_data\\n\\nsample_data(ps_nis)$Fractions_Present\\u003c-samples_fractions$n\\n\\n## Identify ARMS deployments for which all motile (MF) fractions are present ##\\n\\n# Count samples present for each unique ARMS - Deployment - Retrieval - MF combo\\n\\nfractions2\\u003c-sample_data(ps_nis)  %\\u003e% count(ARMS, Deployment,Retrieval,Fraction)\\n\\n# Make data.frame with column \\\"mf\\\" representing number of samples for ARMS - Deployment - Retrieval - MF combos\\n\\nfractions2\\u003c- data.frame(lapply(fractions2, function(x) Reduce(c, x)))\\ncolnames(fractions2)[5]\\u003c-\\\"mf\\\"\\n\\n# Merge sample_data and fractions table \\n\\nsamples_fractions2\\u003c-merge(sample_data(ps_nis), fractions2, by=c(\\\"ARMS\\\",\\\"Deployment\\\",\\\"Retrieval\\\",\\\"Fraction\\\"))\\n\\n# Sort this table based on MaterialSampleID in sample_data of ps_nis\\n\\nsamples_fractions2\\u003c-samples_fractions2[order(match(samples_fractions2$MaterialSampleID,sample_data(ps_nis)$MaterialSampleID)),]\\n\\n# Add \\\"mf\\\" column to sample_data\\n\\nsample_data(ps_nis)$Fractions_Present_MF\\u003c-samples_fractions2$mf\\n\\n# Create separate phyloseq objects for ARMS where samples are present in all size fractions for each sampling event\\n\\npsCOI_all_frac\\u003c-subset_samples(ps_nis,Fractions_Present==3)\\n\\n# Create separate phyloseq objects for ARMS where samples are present in all motile (MF) fractions for each sampling event\\n\\npsCOI_all_mf_frac\\u003c-subset_samples(ps_nis,Fractions_Present_MF==2)\\n\\n# Create separate phyloseq objects for samples separated by each fraction and for t\\n\\npsCOI_mf100\\u003c-subset_samples(ps_nis,Fraction_Group==\\\"MF100\\\")\\n\\npsCOI_mf500\\u003c-subset_samples(ps_nis,Fraction_Group==\\\"MF500\\\")\\n\\npsCOI_sf40\\u003c-subset_samples(ps_nis,Fraction_Group==\\\"SF40\\\")\\n\\n# Checkout deployment duration of ARMS in these five groups \\n\\nduration_all_frac\\u003c-ggplot(sample_data(psCOI_all_frac), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"All fractions\\\")\\nduration_all_frac\\n\\nduration_all_mf_frac\\u003c-ggplot(sample_data(psCOI_all_mf_frac), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"All MF fractions\\\")\\nduration_all_mf_frac\\n\\nduration_sf40\\u003c-ggplot(sample_data(psCOI_sf40), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"SF40\\\")\\nduration_sf40\\n\\nduration_mf100\\u003c-ggplot(sample_data(psCOI_mf100), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"MF 100\\\")\\nduration_mf100\\n\\nduration_mf500\\u003c-ggplot(sample_data(psCOI_mf500), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"MF 500\\\")\\nduration_mf500\\n\\npsCOI_all_periods\\u003c-ggarrange(duration_all_frac,duration_all_mf_frac,duration_sf40,duration_mf100,duration_mf500,ncol=3,nrow=2)\\n\\nggsave(psCOI_all_periods,file=\\\"psCOI_all_periods.png\\\",height=10,width=12)\\n\\n# Some sites have multiple deployments at different time points. Keep samples of one period only (the one that overlaps with most other deployments).\\n\\npsCOI_all_frac\\u003c-subset_samples(psCOI_all_frac,Observatory!=\\\"Koster\\\" | c(Deployment!=\\\"2018-04-18\\\" \\u0026 Deployment!=\\\"2020-07-16\\\"))\\npsCOI_all_frac\\u003c-subset_samples(psCOI_all_frac,Observatory!=\\\"Limfjord\\\" | Deployment!=\\\"2019-06-18\\\")\\n\\npsCOI_all_mf_frac\\u003c-subset_samples(psCOI_all_mf_frac,Observatory!=\\\"Koster\\\" |  c(Deployment!=\\\"2018-04-18\\\" \\u0026 Deployment!=\\\"2020-07-16\\\"))\\npsCOI_all_mf_frac\\u003c-subset_samples(psCOI_all_mf_frac,Observatory!=\\\"Limfjord\\\" | c(Deployment!=\\\"2019-10-29\\\" \\u0026 Deployment!=\\\"2020-11-10\\\"))\\npsCOI_all_mf_frac\\u003c-subset_samples(psCOI_all_mf_frac,Observatory!=\\\"Plymouth\\\" | Deployment!=\\\"2020-07-17\\\")\\n\\npsCOI_sf40\\u003c-subset_samples(psCOI_sf40,Observatory!=\\\"Koster\\\" | c(Deployment!=\\\"2018-04-18\\\" \\u0026 Deployment!=\\\"2020-07-16\\\"))\\npsCOI_sf40\\u003c-subset_samples(psCOI_sf40,Observatory!=\\\"Limfjord\\\" | Deployment!=\\\"2019-06-18\\\")\\npsCOI_sf40\\u003c-subset_samples(psCOI_sf40,Observatory!=\\\"GulfOfPiran\\\" | Deployment!=\\\"2021-02-23\\\")\\npsCOI_sf40\\u003c-subset_samples(psCOI_sf40,Observatory!=\\\"Vigo\\\" | c(Deployment!=\\\"2018-06-07\\\" \\u0026 Deployment!=\\\"2019-06-25\\\"))\\n\\npsCOI_mf100\\u003c-subset_samples(psCOI_mf100,Observatory!=\\\"Koster\\\" | c(Deployment!=\\\"2018-04-18\\\" \\u0026 Deployment!=\\\"2020-07-16\\\"))\\npsCOI_mf100\\u003c-subset_samples(psCOI_mf100,Observatory!=\\\"Limfjord\\\" | c(Deployment!=\\\"2019-10-29\\\" \\u0026 Deployment!=\\\"2020-11-10\\\"))\\npsCOI_mf100\\u003c-subset_samples(psCOI_mf100,Observatory!=\\\"Svalbard\\\" | Deployment!=\\\"2018-07-08\\\")\\npsCOI_mf100\\u003c-subset_samples(psCOI_mf100,Observatory!=\\\"Plymouth\\\" | Deployment!=\\\"2019-07-16\\\")\\n\\npsCOI_mf500\\u003c-subset_samples(psCOI_mf500,Observatory!=\\\"Koster\\\" | c(Deployment!=\\\"2018-04-18\\\" \\u0026 Deployment!=\\\"2020-07-16\\\"))\\npsCOI_mf500\\u003c-subset_samples(psCOI_mf500,Observatory!=\\\"Limfjord\\\" | c(Deployment!=\\\"2019-10-29\\\" \\u0026 Deployment!=\\\"2020-11-10\\\"))\\npsCOI_mf500\\u003c-subset_samples(psCOI_mf500,Observatory!=\\\"Roscoff\\\" | Deployment!=\\\"2018-07-09\\\")\\npsCOI_mf500\\u003c-subset_samples(psCOI_mf500,Observatory!=\\\"Vigo\\\" | Deployment!=\\\"2019-06-25\\\")\\npsCOI_mf500\\u003c-subset_samples(psCOI_mf500,Observatory!=\\\"TZS\\\" | Deployment!=\\\"2020-06-08\\\")\\npsCOI_mf500\\u003c-subset_samples(psCOI_mf500,Observatory!=\\\"Plymouth\\\" | c(Deployment!=\\\"2018-07-01\\\" \\u0026Deployment!=\\\"2020-07-17\\\"))\\n\\n# Remove MOTUs which have a total abundance of zero after removing samples during the previous steps\\n\\npsCOI_all_frac\\u003c-prune_taxa(rowSums(otu_table(psCOI_all_frac))\\u003e0,psCOI_all_frac)\\npsCOI_all_mf_frac\\u003c-prune_taxa(rowSums(otu_table(psCOI_all_mf_frac))\\u003e0,psCOI_all_mf_frac)\\npsCOI_sf40\\u003c-prune_taxa(rowSums(otu_table(psCOI_sf40))\\u003e0,psCOI_sf40)\\npsCOI_mf100\\u003c-prune_taxa(rowSums(otu_table(psCOI_mf100))\\u003e0,psCOI_mf100)\\npsCOI_mf500\\u003c-prune_taxa(rowSums(otu_table(psCOI_mf500))\\u003e0,psCOI_mf500)\\n\\n# Save phyloseq objects to file\\n\\nsaveRDS(psCOI_all_frac,\\\"psCOI_all_frac.rds\\\")\\nsaveRDS(psCOI_all_mf_frac,\\\"psCOI_all_mf_frac.rds\\\")\\nsaveRDS(psCOI_sf40,\\\"psCOI_sf40.rds\\\")\\nsaveRDS(psCOI_mf100,\\\"psCOI_mf100.rds\\\")\\nsaveRDS(psCOI_mf500,\\\"psCOI_mf500.rds\\\")\\n\\n# Checkout deployment duration of ARMS in these five groups after the filtering \\n\\nduration_all_frac\\u003c-ggplot(sample_data(psCOI_all_frac), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"All fractions\\\")\\nduration_sf40\\n\\nduration_all_mf_frac\\u003c-ggplot(sample_data(psCOI_all_mf_frac), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"All MF fractions\\\")\\nduration_all_mf_frac\\n\\nduration_sf40\\u003c-ggplot(sample_data(psCOI_sf40), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"SF40\\\")\\nduration_sf40\\n\\nduration_mf100\\u003c-ggplot(sample_data(psCOI_mf100), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"MF 100\\\")\\nduration_mf100\\n\\nduration_mf500\\u003c-ggplot(sample_data(psCOI_mf500), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"MF 500\\\")\\nduration_mf500\\n\\npsCOI_all_periods\\u003c-ggarrange(duration_all_frac,duration_all_mf_frac,duration_sf40,duration_mf100,duration_mf500,ncol=3,nrow=2)\\n\\nggsave(psCOI_all_periods,file=\\\"psCOI_filtered_periods.png\\\",height=10,width=12)\\n\\n## For each of the five phyloseq objects, randomly sample two ARMS for each Field_Replicate group. Where only one ARMS exists, keep this one. ##\\n\\nset.seed(1) # set.seed for random subsampling\\n\\narms_all_frac\\u003c- data.frame(sample_data(psCOI_all_frac)) %\\u003e% select(ARMS,Field_Replicate) %\\u003e% distinct() %\\u003e% group_by(Field_Replicate) %\\u003e% slice_sample(n=2)\\npsCOI_all_frac\\u003c-subset_samples(psCOI_all_frac,ARMS %in% arms_all_frac$ARMS)\\n\\narms_all_mf_frac\\u003c- data.frame(sample_data(psCOI_all_mf_frac)) %\\u003e% select(ARMS,Field_Replicate) %\\u003e% distinct() %\\u003e% group_by(Field_Replicate) %\\u003e% slice_sample(n=2)\\npsCOI_all_mf_frac\\u003c-subset_samples(psCOI_all_mf_frac,ARMS %in% arms_all_mf_frac$ARMS)\\n\\narms_sf40\\u003c- data.frame(sample_data(psCOI_sf40)) %\\u003e% select(ARMS,Field_Replicate) %\\u003e% distinct() %\\u003e% group_by(Field_Replicate) %\\u003e% slice_sample(n=2)\\npsCOI_sf40\\u003c-subset_samples(psCOI_sf40,ARMS %in% arms_sf40$ARMS)\\n\\narms_mf100\\u003c- data.frame(sample_data(psCOI_mf100)) %\\u003e% select(ARMS,Field_Replicate) %\\u003e% distinct() %\\u003e% group_by(Field_Replicate) %\\u003e% slice_sample(n=2)\\npsCOI_mf100\\u003c-subset_samples(psCOI_mf100,ARMS %in% arms_mf100$ARMS)\\n\\narms_mf500\\u003c- data.frame(sample_data(psCOI_mf500)) %\\u003e% select(ARMS,Field_Replicate) %\\u003e% distinct() %\\u003e% group_by(Field_Replicate) %\\u003e% slice_sample(n=2)\\npsCOI_mf500\\u003c-subset_samples(psCOI_mf500,ARMS %in% arms_mf500$ARMS)\\n\\n# Merge samples based on Field_Replicate group\\n\\npsCOI_all_frac\\u003c-merge_samples(psCOI_all_frac,\\\"Field_Replicate\\\")\\npsCOI_all_mf_frac\\u003c-merge_samples(psCOI_all_mf_frac,\\\"Field_Replicate\\\")\\npsCOI_sf40\\u003c-merge_samples(psCOI_sf40,\\\"Field_Replicate\\\")\\npsCOI_mf100\\u003c-merge_samples(psCOI_mf100,\\\"Field_Replicate\\\")\\npsCOI_mf500\\u003c-merge_samples(psCOI_mf500,\\\"Field_Replicate\\\")\\n\\n# Determine NIS richness for each phyloseq object, get coordinates, taxonomy of NIS and presence-absence at each site and write to file\\n\\nrich_all_frac\\u003c-estimate_richness(psCOI_all_frac,measures=\\\"Observed\\\")\\nrich_all_frac$n_arms\\u003c-2 # add number of ARMS present for this Field_Replicate group\\n# Adjust number of ARMS per Field_Replicate for some cases\\nrich_all_frac$n_arms\\u003c-ifelse(rownames(rich_all_frac)==\\\"GDY\\\" | rownames(rich_all_frac)==\\\"Preemraff\\\" | rownames(rich_all_frac)== \\\"Gurr\\\" | rownames(rich_all_frac)==\\\"MBA\\\",1,rich_all_frac$n_arms)\\nfor(i in 1:nrow(sample_data(psCOI_all_frac))) { # add coordinates (have already been averaged for all ARMS of each site during the merge_samples step)\\n  rownames(rich_all_frac)\\u003c-gsub(\\\"\\\\\\\\.\\\",\\\"-\\\",rownames(rich_all_frac))\\n  rich_all_frac$Latitude[rownames(rich_all_frac) == rownames(sample_data(psCOI_all_frac))[i]] \\u003c- sample_data(psCOI_all_frac)$Latitude[i]\\n  rich_all_frac$Longitude[rownames(rich_all_frac) == rownames(sample_data(psCOI_all_frac))[i]] \\u003c- sample_data(psCOI_all_frac)$Longitude[i]\\n} \\notu_table_all_frac\\u003c-data.frame(t(otu_table(psCOI_all_frac)))\\notu_table_all_frac[otu_table_all_frac\\u003e0]\\u003c-1\\ntax_table_all_frac\\u003c-data.frame(tax_table(psCOI_all_frac))\\ntax_table_all_frac\\u003c-tax_table_all_frac[order(match(rownames(tax_table_all_frac),rownames(otu_table_all_frac))),]\\nall_frac_nis\\u003c-cbind(tax_table_all_frac,otu_table_all_frac)\\n\\nrich_all_mf_frac\\u003c-estimate_richness(psCOI_all_mf_frac,measures=\\\"Observed\\\")\\nrich_all_mf_frac$n_arms\\u003c-2 # add number of ARMS present for this Field_Replicate group\\n# Adjust number of ARMS per Field_Replicate for some cases\\nrich_all_mf_frac$n_arms\\u003c-ifelse(rownames(rich_all_mf_frac)==\\\"GDY\\\" | rownames(rich_all_mf_frac)==\\\"Preemraff\\\" | rownames(rich_all_mf_frac)== \\\"Gurr\\\" | rownames(rich_all_mf_frac)==\\\"MBA\\\" | rownames(rich_all_mf_frac)==\\\"AJJ.AZFP\\\",1,rich_all_mf_frac$n_arms)\\nfor(i in 1:nrow(sample_data(psCOI_all_mf_frac))) { # add coordinates (have already been averaged for all ARMS of each site during the merge_samples step)\\n  rownames(rich_all_mf_frac)\\u003c-gsub(\\\"\\\\\\\\.\\\",\\\"-\\\",rownames(rich_all_mf_frac))\\n  rich_all_mf_frac$Latitude[rownames(rich_all_mf_frac) == rownames(sample_data(psCOI_all_mf_frac))[i]] \\u003c- sample_data(psCOI_all_mf_frac)$Latitude[i]\\n  rich_all_mf_frac$Longitude[rownames(rich_all_mf_frac) == rownames(sample_data(psCOI_all_mf_frac))[i]] \\u003c- sample_data(psCOI_all_mf_frac)$Longitude[i]\\n} \\notu_table_all_mf_frac\\u003c-data.frame(t(otu_table(psCOI_all_mf_frac)))\\notu_table_all_mf_frac[otu_table_all_mf_frac\\u003e0]\\u003c-1\\ntax_table_all_mf_frac\\u003c-data.frame(tax_table(psCOI_all_mf_frac))\\ntax_table_all_mf_frac\\u003c-tax_table_all_mf_frac[order(match(rownames(tax_table_all_mf_frac),rownames(otu_table_all_mf_frac))),]\\nall_mf_frac_nis\\u003c-cbind(tax_table_all_mf_frac,otu_table_all_mf_frac)\\n\\nrich_sf40\\u003c-estimate_richness(psCOI_sf40,measures=\\\"Observed\\\")\\nrich_sf40$n_arms\\u003c-2 # add number of ARMS present for this Field_Replicate group\\n# Adjust number of ARMS per Field_Replicate for some cases\\nrich_sf40$n_arms\\u003c-ifelse(rownames(rich_sf40)==\\\"GDY\\\" | rownames(rich_sf40)==\\\"AZBE\\\" | rownames(rich_sf40)== \\\"Coastbusters\\\" | rownames(rich_sf40)==\\\"Laeso\\\" | rownames(rich_sf40)==\\\"Crete\\\",1,rich_sf40$n_arms)\\nfor(i in 1:nrow(sample_data(psCOI_sf40))) { # add coordinates (have already been averaged for all ARMS of each site during the merge_samples step)\\n  rownames(rich_sf40)\\u003c-gsub(\\\"\\\\\\\\.\\\",\\\"-\\\",rownames(rich_sf40))\\n  rich_sf40$Latitude[rownames(rich_sf40) == rownames(sample_data(psCOI_sf40))[i]] \\u003c- sample_data(psCOI_sf40)$Latitude[i]\\n  rich_sf40$Longitude[rownames(rich_sf40) == rownames(sample_data(psCOI_sf40))[i]] \\u003c- sample_data(psCOI_sf40)$Longitude[i]\\n} \\notu_table_sf40\\u003c-data.frame(t(otu_table(psCOI_sf40)))\\notu_table_sf40[otu_table_sf40\\u003e0]\\u003c-1\\ntax_table_sf40\\u003c-data.frame(tax_table(psCOI_sf40))\\ntax_table_sf40\\u003c-tax_table_sf40[order(match(rownames(tax_table_sf40),rownames(otu_table_sf40))),]\\nsf40_nis\\u003c-cbind(tax_table_sf40,otu_table_sf40)\\n\\nrich_mf100\\u003c-estimate_richness(psCOI_mf100,measures=\\\"Observed\\\")\\nrich_mf100$n_arms\\u003c-2 # add number of ARMS present for this Field_Replicate group\\n# Adjust number of ARMS per Field_Replicate for some cases\\nrich_mf100$n_arms\\u003c-ifelse(rownames(rich_mf100)==\\\"GDY\\\" | rownames(rich_mf100)==\\\"Crete\\\" | rownames(rich_mf100)== \\\"Coastbusters\\\" | rownames(rich_mf100)==\\\"S\\\" | rownames(rich_mf100)==\\\"Preemraff\\\",1,rich_mf100$n_arms)\\nfor(i in 1:nrow(sample_data(psCOI_mf100))) { # add coordinates (have already been averaged for all ARMS of each site during the merge_samples step)\\n  rownames(rich_mf100)\\u003c-gsub(\\\"\\\\\\\\.\\\",\\\"-\\\",rownames(rich_mf100))\\n  rich_mf100$Latitude[rownames(rich_mf100) == rownames(sample_data(psCOI_mf100))[i]] \\u003c- sample_data(psCOI_mf100)$Latitude[i]\\n  rich_mf100$Longitude[rownames(rich_mf100) == rownames(sample_data(psCOI_mf100))[i]] \\u003c- sample_data(psCOI_mf100)$Longitude[i]\\n}\\notu_table_mf100\\u003c-data.frame(t(otu_table(psCOI_mf100)))\\notu_table_mf100[otu_table_mf100\\u003e0]\\u003c-1\\ntax_table_mf100\\u003c-data.frame(tax_table(psCOI_mf100))\\ntax_table_mf100\\u003c-tax_table_mf100[order(match(rownames(tax_table_mf100),rownames(otu_table_mf100))),]\\nmf100_nis\\u003c-cbind(tax_table_mf100,otu_table_mf100)\\n\\nrich_mf500\\u003c-estimate_richness(psCOI_mf500,measures=\\\"Observed\\\")\\nrich_mf500$n_arms\\u003c-2 # add number of ARMS present for this Field_Replicate group\\n# Adjust number of ARMS per Field_Replicate for some cases\\nrich_mf500$n_arms\\u003c-ifelse(rownames(rich_mf500)==\\\"GDY\\\" | rownames(rich_mf500)==\\\"Crete\\\" | rownames(rich_mf500)== \\\"AZBE\\\" | rownames(rich_mf500)==\\\"S\\\" | rownames(rich_mf500)==\\\"Preemraff\\\" | rownames(rich_mf500)==\\\"Gurr\\\"| rownames(rich_mf500)==\\\"GulfOfPiran\\\",1,rich_mf500$n_arms)\\nfor(i in 1:nrow(sample_data(psCOI_mf500))) { # add coordinates (have already been averaged for all ARMS of each site during the merge_samples step)\\n  rownames(rich_mf500)\\u003c-gsub(\\\"\\\\\\\\.\\\",\\\"-\\\",rownames(rich_mf500))\\n  rich_mf500$Latitude[rownames(rich_mf500) == rownames(sample_data(psCOI_mf500))[i]] \\u003c- sample_data(psCOI_mf500)$Latitude[i]\\n  rich_mf500$Longitude[rownames(rich_mf500) == rownames(sample_data(psCOI_mf500))[i]] \\u003c- sample_data(psCOI_mf500)$Longitude[i]\\n}\\notu_table_mf500\\u003c-data.frame(t(otu_table(psCOI_mf500)))\\notu_table_mf500[otu_table_mf500\\u003e0]\\u003c-1\\ntax_table_mf500\\u003c-data.frame(tax_table(psCOI_mf500))\\ntax_table_mf500\\u003c-tax_table_mf500[order(match(rownames(tax_table_mf500),rownames(otu_table_mf500))),]\\nmf500_nis\\u003c-cbind(tax_table_mf500,otu_table_mf500)\\n\\n# Write number of NIS abundance per site = Field_Replicate group and otu_tables of the 5 phyloseq objects as several sheets to one xlsx file  \\n \\nxlsx::write.xlsx(rich_all_frac, file = \\\"NIS_presence_absence_comparison_COI.xlsx\\\",sheetName = \\\"all_frac_richness\\\", append = FALSE)\\nxlsx::write.xlsx(all_frac_nis, file = \\\"NIS_presence_absence_comparison_COI.xlsx\\\",sheetName = \\\"all_frac_counts_taxa\\\", append = TRUE)\\nxlsx::write.xlsx(rich_all_mf_frac, file = \\\"NIS_presence_absence_comparison_COI.xlsx\\\",sheetName = \\\"all_mf_frac_richness\\\", append = TRUE)\\nxlsx::write.xlsx(all_mf_frac_nis, file = \\\"NIS_presence_absence_comparison_COI.xlsx\\\",sheetName = \\\"all_mf_frac_counts_taxa\\\", append = TRUE)\\nxlsx::write.xlsx(rich_sf40, file = \\\"NIS_presence_absence_comparison_COI.xlsx\\\",sheetName = \\\"sf40_richness\\\", append = TRUE)\\nxlsx::write.xlsx(sf40_nis, file = \\\"NIS_presence_absence_comparison_COI.xlsx\\\",sheetName = \\\"sf40_counts_taxa\\\", append = TRUE)\\nxlsx::write.xlsx(rich_mf100, file = \\\"NIS_presence_absence_comparison_COI.xlsx\\\",sheetName = \\\"mf100_richness\\\", append = TRUE)\\nxlsx::write.xlsx(mf100_nis, file = \\\"NIS_presence_absence_comparison_COI.xlsx\\\",sheetName = \\\"mf100_counts_taxa\\\", append = TRUE)\\nxlsx::write.xlsx(rich_mf500, file = \\\"NIS_presence_absence_comparison_COI.xlsx\\\",sheetName = \\\"mf500_richness\\\", append = TRUE)\\nxlsx::write.xlsx(mf500_nis, file = \\\"NIS_presence_absence_comparison_COI.xlsx\\\",sheetName = \\\"mf500_counts_taxa\\\", append = TRUE)\",\"os_name\":\"\",\"os_version\":\"\"}},\"1\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Creating comparable 18S NIS data set in R\",\"description\":\"\",\"guid\":\"CCD0E4A5F73711EEAF630A58A9FEAC02\",\"name\":\"library(phyloseq)\\nlibrary(dplyr)\\nlibrary(vegan)\\nlibrary(ggplot2)\\nlibrary(tidyr)\\nlibrary(ggpubr)\\nlibrary(data.table)\\nlibrary(xlsx)\\n\\nsetwd(\\\"~/18S\\\")\\n\\n# read the unfiltered phyloseqy object\\n\\nps18S_cleaned\\u003c-readRDS(\\\"ps18S_unfiltered_ARMS.rds\\\")\\n\\n# Some samples were re-sequenced in August 2023 and in most of those cases two genetic samples for the respective MaterialSampleID are therefore present in the data set\\n# Assess rarefaction curves, OTU counts and taxonomy for each of those sample pairs and remove sample with lower diversity or taxonomic resolution\\n\\nsample_data(ps18S_cleaned)[duplicated(sample_data(ps18S_cleaned)$MaterialSampleID),\\\"MaterialSampleID\\\"] # Check which MaterialSampleIDs appear twice\\n\\nkoster1\\u003c-subset_samples(ps18S_cleaned,MaterialSampleID==\\\"ARMS_Koster_VH1_20190527_20200716_MF100\\\")\\nkoster1\\u003c-prune_taxa(rowSums(otu_table(koster1))\\u003e0,koster1)\\nrarecurve(t(otu_table(koster1)), step=50, cex=0.5)\\nkoster1\\u003c-cbind(tax_table(koster1),otu_table(koster1))\\nkoster1\\n\\nkoster2\\u003c-subset_samples(ps18S_cleaned,MaterialSampleID==\\\"ARMS_Koster_VH1_20190527_20200716_MF500\\\")\\nkoster2\\u003c-prune_taxa(rowSums(otu_table(koster2))\\u003e0,koster2)\\nrarecurve(t(otu_table(koster2)), step=50, cex=0.5)\\nkoster2\\u003c-cbind(tax_table(koster2),otu_table(koster2))\\nkoster2\\n\\nfornace1\\u003c-subset_samples(ps18S_cleaned,MaterialSampleID==\\\"ARMS_GulfOfPiran_Fornace_20180815_20181118_SF_ETOH\\\")\\nfornace1\\u003c-prune_taxa(rowSums(otu_table(fornace1))\\u003e0,fornace1)\\nrarecurve(t(otu_table(fornace1)), step=50, cex=0.5)\\nfornace1\\u003c-cbind(tax_table(fornace1),otu_table(fornace1))\\nfornace1\\n\\nfornace2\\u003c-subset_samples(ps18S_cleaned,MaterialSampleID==\\\"ARMS_GulfOfPiran_Fornace_20180815_20181118_MF500_ETOH\\\")\\nfornace2\\u003c-prune_taxa(rowSums(otu_table(fornace2))\\u003e0,fornace2)\\nrarecurve(t(otu_table(fornace2)), step=50, cex=0.5)\\nfornace2\\u003c-cbind(tax_table(fornace2),otu_table(fornace2))\\nfornace2\\n\\nkatza\\u003c-subset_samples(ps18S_cleaned,MaterialSampleID==\\\"ARMS_Eilat_Katza1_20181024_20200706_MF500\\\")\\nkatza\\u003c-prune_taxa(rowSums(otu_table(katza))\\u003e0,katza)\\nrarecurve(t(otu_table(katza)), step=50, cex=0.5)\\nkatza\\u003c-cbind(tax_table(katza),otu_table(katza))\\nkatza\\n\\ntorallaA\\u003c-subset_samples(ps18S_cleaned,MaterialSampleID==\\\"ARMS_Vigo_TorallaA_20190625_20191014_MF500\\\")\\ntorallaA\\u003c-prune_taxa(rowSums(otu_table(torallaA))\\u003e0,torallaA)\\nrarecurve(t(otu_table(torallaA)), step=50, cex=0.5)\\ntorallaA\\u003c-cbind(tax_table(torallaA),otu_table(torallaA))\\ntorallaA\\n\\ntorallaB\\u003c-subset_samples(ps18S_cleaned,MaterialSampleID==\\\"ARMS_Vigo_TorallaB_20190625_20191014_MF500\\\")\\ntorallaB\\u003c-prune_taxa(rowSums(otu_table(torallaB))\\u003e0,torallaB)\\nrarecurve(t(otu_table(torallaB)), step=50, cex=0.5)\\ntorallaB\\u003c-cbind(tax_table(torallaB),otu_table(torallaB))\\ntorallaB\\n\\nto_remove_samples\\u003c-c(\\\"ERR4914045\\\",\\\"ERR4914046\\\",\\\"ERR7127577\\\",\\\"ERR12541375\\\",\\\"ERR7127612\\\",\\\"ERR4018705\\\",\\\"ERR4018707\\\")\\n\\nps18S_cleaned \\u003c- prune_samples(!(sample_names(ps18S_cleaned) %in% to_remove_samples), ps18S_cleaned)\\n\\n# Some samples have been preserved in DMSO as well as EtOH initially as a trial. Where samples have been preserved in both, keep only DMSO samples.\\n\\nsample_data(ps18S_cleaned)[order(sample_data(ps18S_cleaned)$MaterialSampleID),1] # Check were MaterialSampleID is present twice with two preservatives\\n\\nto_remove_pres \\u003c-c(\\\"ERR9632061\\\",\\\"ERR4605087\\\",\\\"ERR4605085\\\",\\n                   \\\"ERR7127575\\\",\\\"ERR4605230\\\",\\\"ERR12541374\\\",\\n                   \\\"ERR7125584\\\",\\\"ERR7125586\\\",\\\"ERR7125588\\\",\\n                   \\\"ERR4605221\\\",\\\"ERR7125590\\\",\\\"ERR4605226\\\",\\n                   \\\"ERR7125621\\\",\\\"ERR7125626\\\",\\\"ERR7125628\\\",\\n                   \\\"ERR7125630\\\")\\n\\nps18S_cleaned \\u003c- prune_samples(!(sample_names(ps18S_cleaned) %in% to_remove_pres), ps18S_cleaned)\\n\\n# Two samples from Roscoff, France have been processed as replicates.\\n\\n# Assess rarefaction curves to see which sample to keep\\nrarecurve(t(otu_table(subset_samples(ps18S_cleaned,Lab_Replicate!=\\\"\\\"))), step=50, cex=0.5)\\n\\n# Remove sample with lower sample size/MOTU abundance\\nto_remove_rep \\u003c-\\\"ERR7125620\\\"\\nps18S_cleaned \\u003c- prune_samples(!(sample_names(ps18S_cleaned) %in% to_remove_rep), ps18S_cleaned)\\n\\n# Remove samples with a read number of \\u003c 10,000\\n\\nps18S_cleaned \\u003c- prune_samples(sample_sums(ps18S_cleaned) \\u003e 10000, ps18S_cleaned)\\n\\n### Rarefaction procedure ###\\n\\n# To get equal sequencing depth, rarefy samples of the MOTU data set to 10,000 reads with default set.seed(1) \\n\\nps18S_cleaned_rarefied \\u003c- rarefy_even_depth(ps18S_cleaned, rngseed=1, sample.size=10000, replace=F)\\nsaveRDS(ps18S_cleaned_rarefied,\\\"ps18S_cleaned_rarefied.rds\\\")\\n\\n# Read the ASV count table (result of blank correction) \\n\\nasv_counts\\u003c-read.table(\\\"asv_no_contaminants_18S.txt\\\",sep=\\\"\\\\t\\\",header=T)\\n\\n# Set ASV names as rownames\\nrownames(asv_counts) \\u003c-asv_counts[,1] \\nasv_counts[,1] \\u003c- NULL\\n\\n# Subset to samples which are present in ps18S_cleaned_rarefied and remove ASVs that have a read count of zero as a result\\n\\nasv_counts_select\\u003c-asv_counts[,colnames(asv_counts) %in% sample_names(ps18S_cleaned_rarefied)]\\nasv_counts_select\\u003c-asv_counts_select[rowSums(asv_counts_select[])\\u003e0,]\\n\\n# Rarefy to 10,000 reads per sample and remove ASVs that have a read count of zero as a result\\n\\nset.seed(1)\\nasv_counts_rarefied\\u003c-t(rrarefy(t(asv_counts_select),sample=10000))\\nasv_counts_rarefied\\u003c-asv_counts_rarefied[rowSums(asv_counts_rarefied[])\\u003e0,]\\n\\n## Create NIS data set based on rarefied MOTU and ASV data sets\\n\\n# read the previously created and manually curated table with MOTU-ASV NIS counts per sampling event\\n# Subset to MOTUs, ASVs and sample events still present after rarefaction\\n\\nasv_motu_counts\\u003c-read.table(\\\"NIS_MOTU_ASV_counts_18S_curated_filtered.txt\\\",sep=\\\"\\\\t\\\",header=T,check.names = F)\\nasv_motu_counts\\u003c-asv_motu_counts %\\u003e% filter(MOTU %in% taxa_names(ps18S_cleaned_rarefied))\\nasv_motu_counts\\u003c-asv_motu_counts %\\u003e% filter(ASV %in% rownames(asv_counts_rarefied))\\n# make sample_event data\\nvariable1 = as.character(get_variable(ps18S_cleaned_rarefied, \\\"ARMS\\\"))\\nvariable2 = as.character(get_variable(ps18S_cleaned_rarefied, \\\"Deployment\\\"))\\nvariable3 = as.character(get_variable(ps18S_cleaned_rarefied, \\\"Retrieval\\\"))\\nsample_data(ps18S_cleaned_rarefied)$sample_event \\u003c- paste(variable1, variable2, variable3,sep=\\\"_\\\")\\ncommon_cols \\u003c- intersect(colnames(asv_motu_counts[,-(1:3)]),sample_data(ps18S_cleaned_rarefied)$sample_event)\\nasv_motu_counts \\u003c- asv_motu_counts %\\u003e%  select(c(MOTU,Species,ASV,all_of(common_cols)))\\n\\n# Remove MOTUs in case they are only made up of occurrences with less than 10 reads per sampling event\\n\\nabundance_check\\u003c-asv_motu_counts %\\u003e% select(-c(Species,ASV))\\nabundance_check\\u003c-aggregate(.~ MOTU,abundance_check,FUN=sum)\\nrownames(abundance_check) \\u003c-abundance_check[,1] \\nabundance_check[,1] \\u003c- NULL\\nabundance_check[abundance_check \\u003c 10] \\u003c- 0\\nabundance_check\\u003c-abundance_check[rowSums(abundance_check[])\\u003e0,]\\nabundance_check\\u003c-abundance_check[, colSums(abundance_check != 0) \\u003e 0]\\n\\n# The previously created and curated NIS data set is based on sample events\\n# we will create a NIS phyloseq object based on individual samples, however occurrences belonging to sample events where NIS MOTUs were not considered as NIS need to be set to zero \\n\\nps_nis \\u003c- prune_taxa(taxa_names(ps18S_cleaned_rarefied) %in% rownames(abundance_check),ps18S_cleaned_rarefied)\\n\\n# Make a dataframes of NIS MOTU counts in long format to subsequently add the actual presence-absence info of NIS MOTUs\\nnis_counts\\u003c-as.data.frame(otu_table(ps_nis))\\nnis_counts$MOTU\\u003c-rownames(nis_counts)\\nnis_counts\\u003c-nis_counts %\\u003e% pivot_longer(cols=-MOTU)\\nabundance_check$MOTU\\u003c-rownames(abundance_check)\\nabundance_check\\u003c-abundance_check %\\u003e% pivot_longer(cols=-MOTU)\\n\\nfor(i in 1:nrow(sample_data(ps_nis))) { # add sample_event info to nisc_counts\\n  nis_counts$event[nis_counts$name == rownames(sample_data(ps_nis))[i]] \\u003c- sample_data(ps_nis)$sample_event[i]\\n} # ignore warning\\n\\nfor(j in 1:nrow(abundance_check)) { # for each MOTU-sample_event combination, add the curated NIS MOTU count\\n  nis_counts$count_new[nis_counts$event == abundance_check$name[j] \\u0026 nis_counts$MOTU == abundance_check$MOTU[j]] \\u003c- abundance_check$value[j]\\n} # ignore warning\\n\\nnis_counts$count_new[is.na(nis_counts$count_new)]\\u003c-0 # where a sample event was not present in the curated NIS data set because it contained no NIS, the count_new entry will be NA. we set it to zero.\\nnis_counts$value\\u003c-ifelse(nis_counts$count_new\\u003e0,nis_counts$value,0) # Where curated NIS MOTU count is zero, set count of MOTU in the respective samples to zero\\n\\n# There will be cases where the curated NIS MOTU count for a sample event is not zero (i.e., in the abundance_check object), but is zero in the rarefied phyloseq object (i.e., in ps_nis/nis_counts).\\n# This is the case when all counts of a NIS MOTU stem from a sample which was removed previously because it contained less than 10,00 reads\\n# The zero count will therefore be kept.\\n\\nnis_counts\\u003c-nis_counts[,-(4:5)] %\\u003e% pivot_wider(names_from = name,values_from = value)\\nnis_counts\\u003c-as.data.frame(nis_counts)\\nrownames(nis_counts)\\u003c-nis_counts$MOTU\\nnis_counts$MOTU \\u003c- NULL\\n\\n# Replace the otu_table of the NIS phyloseq object with the table we just created\\notu_table(ps_nis)\\u003c-otu_table(nis_counts,taxa_are_rows = TRUE)\\n\\n## Do statistical tests to check if NIS prevalence is higher at locations that are marinas / harbours / ports ##\\n\\n# Check levels of Monitoring_Area\\nunique(sample_data(ps_nis)$Monitoring_Area)\\n\\n# Subset samples of marinas, ports, harbours and estimate NIS richness\\nports\\u003c-subset_samples(ps_nis,Monitoring_Area %in% c(\\\"Marina/Harbour\\\",\\\"Marina\\\",\\\"Industrial port\\\",\\\"Harbour\\\"))\\nports_richness\\u003c-estimate_richness(ports,measures = \\\"Observed\\\")\\nports_richness$type\\u003c-\\\"port\\\"\\n\\n# Subset samples which are NOT from marinas, ports, harbours and estimate NIS richness\\nno_ports\\u003c-subset_samples(ps_nis,!Monitoring_Area %in% c(\\\"Marina/Harbour\\\",\\\"Marina\\\",\\\"Industrial port\\\",\\\"Harbour\\\"))\\nno_ports_richness\\u003c-estimate_richness(no_ports,measures = \\\"Observed\\\")\\nno_ports_richness$type\\u003c-\\\"no_port\\\"\\n\\n# Combine tables and do statistical tests (manually copy result from output field to Excel file)\\n\\nrichness\\u003c-rbind(ports_richness,no_ports_richness)\\nrichness$gene\\u003c-\\\"18S\\\" # add gene info to make plot with COI data later on\\nwrite.table(richness,\\\"NIS_richness_18S_port_no_port.txt\\\",sep=\\\"\\\\t\\\",row.names = F)\\n\\n# For The next command, make sure the plyr package is detached from your R session in case you used it for something else. \\n# It will not run properly if the plyr package has been loaded after dplyr (plyr is not part of this script, but could be in your environment from another script. Run detach(package:plyr) )\\n# Could also happen with ggplot2 packagae loaded.\\nrichness %\\u003e%  group_by(type) %\\u003e% summarize(mean = mean(Observed),sd=sd(Observed)) # Calculate Means and SD\\n\\n# Test normality\\nshapiro.test(richness$Observed) # significant\\nshapiro.test(sqrt(richness$Observed)) # significant\\nshapiro.test(log1p(richness$Observed)) # significant\\n\\n# Do non-parametric test\\nkruskal.test(Observed~type,richness)\\n\\n####\\n\\n# Add info on deployment duration\\n\\n# Format respective columns in sample_data as dates\\nsample_data(ps_nis)\\u003c-transform(sample_data(ps_nis),Deployment = as.Date(as.character(Deployment), \\\"%Y%m%d\\\"))\\nsample_data(ps_nis)\\u003c-transform(sample_data(ps_nis),Retrieval = as.Date(as.character(Retrieval), \\\"%Y%m%d\\\"))\\n\\n# Calculate number of days between retrieval and deployment and add a column\\nsample_data(ps_nis)$Deployment_Duration\\u003c-sample_data(ps_nis)$Retrieval-sample_data(ps_nis)$Deployment\\n\\n## Identify ARMS deployments for which all three fractions are present ##\\n\\n# Count samples present for each unique ARMS - Deployment - Retrieval combo\\n\\nfractions\\u003c-sample_data(ps_nis)  %\\u003e% count(ARMS, Deployment,Retrieval)\\n\\n# Make data.frame with column \\\"n\\\" representing number of samples for ARMS - Deployment - Retrieval combos\\n\\nfractions\\u003c- data.frame(lapply(fractions, function(x) Reduce(c, x)))\\n\\n# Merge sample_data and fractions table \\n\\nsamples_fractions\\u003c-merge(sample_data(ps_nis), fractions, by=c(\\\"ARMS\\\",\\\"Deployment\\\",\\\"Retrieval\\\"))\\n\\n# Sort this table based on MaterialSampleID in sample_data of ps_nis\\n\\nsamples_fractions\\u003c-samples_fractions[order(match(samples_fractions$MaterialSampleID,sample_data(ps_nis)$MaterialSampleID)),]\\n\\n# Add \\\"n\\\" column to sample_data\\n\\nsample_data(ps_nis)$Fractions_Present\\u003c-samples_fractions$n\\n\\n## Identify ARMS deployments for which all motile (MF) fractions are present ##\\n\\n# Count samples present for each unique ARMS - Deployment - Retrieval - MF combo\\n\\nfractions2\\u003c-sample_data(ps_nis)  %\\u003e% count(ARMS, Deployment,Retrieval,Fraction)\\n\\n# Make data.frame with column \\\"mf\\\" representing number of samples for ARMS - Deployment - Retrieval - MF combos\\n\\nfractions2\\u003c- data.frame(lapply(fractions2, function(x) Reduce(c, x)))\\ncolnames(fractions2)[5]\\u003c-\\\"mf\\\"\\n\\n# Merge sample_data and fractions table \\n\\nsamples_fractions2\\u003c-merge(sample_data(ps_nis), fractions2, by=c(\\\"ARMS\\\",\\\"Deployment\\\",\\\"Retrieval\\\",\\\"Fraction\\\"))\\n\\n# Sort this table based on MaterialSampleID in sample_data of ps_nis\\n\\nsamples_fractions2\\u003c-samples_fractions2[order(match(samples_fractions2$MaterialSampleID,sample_data(ps_nis)$MaterialSampleID)),]\\n\\n# Add \\\"mf\\\" column to sample_data\\n\\nsample_data(ps_nis)$Fractions_Present_MF\\u003c-samples_fractions2$mf\\n\\n# Create separate phyloseq objects for ARMS where samples are present in all size fractions for each sampling event\\n\\nps18S_all_frac\\u003c-subset_samples(ps_nis,Fractions_Present==3)\\n\\n# Create separate phyloseq objects for ARMS where samples are present in all motile (MF) fractions for each sampling event\\n\\nps18S_all_mf_frac\\u003c-subset_samples(ps_nis,Fractions_Present_MF==2)\\n\\n# Create separate phyloseq objects for samples separated by each fraction and for t\\n\\nps18S_mf100\\u003c-subset_samples(ps_nis,Fraction_Group==\\\"MF100\\\")\\n\\nps18S_mf500\\u003c-subset_samples(ps_nis,Fraction_Group==\\\"MF500\\\")\\n\\nps18S_sf40\\u003c-subset_samples(ps_nis,Fraction_Group==\\\"SF40\\\")\\n\\n# Checkout deployment duration of ARMS in these five groups \\n\\nduration_all_frac\\u003c-ggplot(sample_data(ps18S_all_frac), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"All fractions\\\")\\nduration_all_frac\\n\\nduration_all_mf_frac\\u003c-ggplot(sample_data(ps18S_all_mf_frac), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"All MF fractions\\\")\\nduration_all_mf_frac\\n\\nduration_sf40\\u003c-ggplot(sample_data(ps18S_sf40), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"SF40\\\")\\nduration_sf40\\n\\nduration_mf100\\u003c-ggplot(sample_data(ps18S_mf100), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"MF 100\\\")\\nduration_mf100\\n\\nduration_mf500\\u003c-ggplot(sample_data(ps18S_mf500), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"MF 500\\\")\\nduration_mf500\\n\\nps18S_all_periods\\u003c-ggarrange(duration_all_frac,duration_all_mf_frac,duration_sf40,duration_mf100,duration_mf500,ncol=3,nrow=2)\\n\\nggsave(ps18S_all_periods,file=\\\"ps18S_all_periods.png\\\",height=15,width=12)\\n\\n# Some sites have multiple deployments at different time points. Keep samples of one period only (the one that overlaps with most other deployments).\\n\\nps18S_all_frac\\u003c-subset_samples(ps18S_all_frac,Observatory!=\\\"Koster\\\" | Deployment!=\\\"2018-04-18\\\" )\\nps18S_all_frac\\u003c-subset_samples(ps18S_all_frac,Observatory!=\\\"Limfjord\\\" |c(Deployment!=\\\"2019-10-29\\\" \\u0026 Deployment!=\\\"2019-06-18\\\"))\\nps18S_all_frac\\u003c-subset_samples(ps18S_all_frac,Observatory!=\\\"Plymouth\\\" | Deployment!=\\\"2019-07-16\\\")\\n\\nps18S_all_mf_frac\\u003c-subset_samples(ps18S_all_mf_frac,Observatory!=\\\"Koster\\\" |  c(Deployment!=\\\"2018-04-18\\\" \\u0026 Deployment!=\\\"2019-05-27\\\"))\\nps18S_all_mf_frac\\u003c-subset_samples(ps18S_all_mf_frac,Observatory!=\\\"Limfjord\\\" | c(Deployment!=\\\"2019-10-29\\\" \\u0026 Deployment!=\\\"2019-06-18\\\"))\\nps18S_all_mf_frac\\u003c-subset_samples(ps18S_all_mf_frac,Observatory!=\\\"Plymouth\\\" | c(Deployment!=\\\"2018-07-01\\\" \\u0026 Deployment!=\\\"2019-07-16\\\"))\\nps18S_all_mf_frac\\u003c-subset_samples(ps18S_all_mf_frac,ARMS!=\\\"BasBloS1\\\" |  Deployment!=\\\"2018-07-11\\\")\\n\\nps18S_sf40\\u003c-subset_samples(ps18S_sf40,Observatory!=\\\"Koster\\\" |  c(Deployment!=\\\"2018-04-18\\\" \\u0026 Deployment!=\\\"2019-05-27\\\"))\\nps18S_sf40\\u003c-subset_samples(ps18S_sf40,Observatory!=\\\"Limfjord\\\" | c(Deployment!=\\\"2019-10-29\\\" \\u0026 Deployment!=\\\"2019-06-18\\\"))\\nps18S_sf40\\u003c-subset_samples(ps18S_sf40,Observatory!=\\\"GulfOfPiran\\\" | Deployment!=\\\"2021-02-23\\\")\\nps18S_sf40\\u003c-subset_samples(ps18S_sf40,Observatory!=\\\"Vigo\\\" | c(Deployment!=\\\"2018-06-07\\\" \\u0026 Deployment!=\\\"2019-06-25\\\"))\\nps18S_sf40\\u003c-subset_samples(ps18S_sf40,Observatory!=\\\"TZS\\\" | Deployment!=\\\"2018-07-11\\\")\\nps18S_sf40\\u003c-subset_samples(ps18S_sf40,Observatory!=\\\"Svalbard\\\" | Deployment!=\\\"2018-07-08\\\")\\nps18S_sf40\\u003c-subset_samples(ps18S_sf40,Observatory!=\\\"Plymouth\\\" | c(Deployment!=\\\"2018-07-01\\\" \\u0026 Deployment!=\\\"2019-07-16\\\"))\\nps18S_sf40\\u003c-subset_samples(ps18S_sf40,Observatory!=\\\"Getxo\\\" |  Deployment!=\\\"2019-06-24\\\")\\nps18S_sf40\\u003c-subset_samples(ps18S_sf40,Observatory!=\\\"Crete\\\" |  Deployment!=\\\"2018-09-28\\\")\\n\\nps18S_mf100\\u003c-subset_samples(ps18S_mf100,Observatory!=\\\"Koster\\\" |  c(Deployment!=\\\"2018-04-18\\\" \\u0026 Deployment!=\\\"2019-05-27\\\"))\\nps18S_mf100\\u003c-subset_samples(ps18S_mf100,Observatory!=\\\"Limfjord\\\" | c(Deployment!=\\\"2019-10-29\\\" \\u0026 Deployment!=\\\"2019-06-18\\\"))\\nps18S_mf100\\u003c-subset_samples(ps18S_mf100,Observatory!=\\\"GulfOfPiran\\\" | Deployment!=\\\"2021-02-23\\\")\\nps18S_mf100\\u003c-subset_samples(ps18S_mf100,Observatory!=\\\"Svalbard\\\" | Deployment!=\\\"2018-07-08\\\")\\nps18S_mf100\\u003c-subset_samples(ps18S_mf100,Observatory!=\\\"Plymouth\\\" | c(Deployment!=\\\"2018-07-01\\\" \\u0026 Deployment!=\\\"2019-07-16\\\"))\\nps18S_mf100\\u003c-subset_samples(ps18S_mf100,ARMS!=\\\"BasBloS1\\\" |  Deployment!=\\\"2018-07-11\\\")\\n\\nps18S_mf500\\u003c-subset_samples(ps18S_mf500,Observatory!=\\\"Koster\\\" | c(Deployment!=\\\"2018-04-18\\\" \\u0026 Deployment!=\\\"2020-07-16\\\"))\\nps18S_mf500\\u003c-subset_samples(ps18S_mf500,Observatory!=\\\"Limfjord\\\" | c(Deployment!=\\\"2019-10-29\\\" \\u0026 Deployment!=\\\"2019-06-18\\\"))\\nps18S_mf500\\u003c-subset_samples(ps18S_mf500,Observatory!=\\\"Roscoff\\\" | c(Deployment!=\\\"2018-07-09\\\" \\u0026 Deployment!=\\\"2018-07-11\\\"))\\nps18S_mf500\\u003c-subset_samples(ps18S_mf500,Observatory!=\\\"Vigo\\\" | c(Deployment!=\\\"2018-06-07\\\" \\u0026 Deployment!=\\\"2019-06-25\\\"))\\nps18S_mf500\\u003c-subset_samples(ps18S_mf500,Observatory!=\\\"TZS\\\" | Deployment!=\\\"2020-06-08\\\")\\nps18S_mf500\\u003c-subset_samples(ps18S_mf500,Observatory!=\\\"Plymouth\\\" | c(Deployment!=\\\"2018-07-01\\\" \\u0026 Deployment!=\\\"2019-07-16\\\"))\\nps18S_mf500\\u003c-subset_samples(ps18S_mf500,Observatory!=\\\"Crete\\\" |  Deployment!=\\\"2018-09-28\\\")\\nps18S_mf500\\u003c-subset_samples(ps18S_mf500,Observatory!=\\\"Svalbard\\\" | Deployment!=\\\"2018-07-06\\\")\\n\\n# Remove MOTUs which have a total abundance of zero after removing samples during the previous steps\\n\\nps18S_all_frac\\u003c-prune_taxa(rowSums(otu_table(ps18S_all_frac))\\u003e0,ps18S_all_frac)\\nps18S_all_mf_frac\\u003c-prune_taxa(rowSums(otu_table(ps18S_all_mf_frac))\\u003e0,ps18S_all_mf_frac)\\nps18S_sf40\\u003c-prune_taxa(rowSums(otu_table(ps18S_sf40))\\u003e0,ps18S_sf40)\\nps18S_mf100\\u003c-prune_taxa(rowSums(otu_table(ps18S_mf100))\\u003e0,ps18S_mf100)\\nps18S_mf500\\u003c-prune_taxa(rowSums(otu_table(ps18S_mf500))\\u003e0,ps18S_mf500)\\n\\n# Save phyloseq objects to file\\n\\nsaveRDS(ps18S_all_frac,\\\"ps18S_all_frac.rds\\\")\\nsaveRDS(ps18S_all_mf_frac,\\\"ps18S_all_mf_frac.rds\\\")\\nsaveRDS(ps18S_sf40,\\\"ps18S_sf40.rds\\\")\\nsaveRDS(ps18S_mf100,\\\"ps18S_mf100.rds\\\")\\nsaveRDS(ps18S_mf500,\\\"ps18S_mf500.rds\\\")\\n\\n# Checkout deployment duration of ARMS in these five groups after the filtering \\n\\nduration_all_frac\\u003c-ggplot(sample_data(ps18S_all_frac), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"All fractions\\\")\\nduration_sf40\\n\\nduration_all_mf_frac\\u003c-ggplot(sample_data(ps18S_all_mf_frac), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"All MF fractions\\\")\\nduration_all_mf_frac\\n\\nduration_sf40\\u003c-ggplot(sample_data(ps18S_sf40), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"SF40\\\")\\nduration_sf40\\n\\nduration_mf100\\u003c-ggplot(sample_data(ps18S_mf100), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"MF 100\\\")\\nduration_mf100\\n\\nduration_mf500\\u003c-ggplot(sample_data(ps18S_mf500), aes(x = Deployment, y = ARMS, colour = Deployment_Duration)) +\\n  geom_segment(aes(xend = Retrieval, yend = ARMS), colour = \\\"black\\\") +\\n  geom_point(size = 3) +\\n  geom_point(aes(x = Retrieval), size = 3) +\\n  theme(legend.position = \\\"none\\\")+\\n  ggtitle(\\\"MF 500\\\")\\nduration_mf500\\n\\nps18S_all_periods\\u003c-ggarrange(duration_all_frac,duration_all_mf_frac,duration_sf40,duration_mf100,duration_mf500,ncol=3,nrow=2)\\n\\nggsave(ps18S_all_periods,file=\\\"ps18S_filtered_periods.png\\\",height=15,width=12)\\n\\n## For each of the five phyloseq objects, randomly sample two ARMS for each Field_Replicate group. Where only one ARMS exists, keep this one. ##\\n\\nset.seed(1) # set.seed for random subsampling\\n\\narms_all_frac\\u003c- data.frame(sample_data(ps18S_all_frac)) %\\u003e% select(ARMS,Field_Replicate) %\\u003e% distinct() %\\u003e% group_by(Field_Replicate) %\\u003e% slice_sample(n=2)\\nps18S_all_frac\\u003c-subset_samples(ps18S_all_frac,ARMS %in% arms_all_frac$ARMS)\\n\\narms_all_mf_frac\\u003c- data.frame(sample_data(ps18S_all_mf_frac)) %\\u003e% select(ARMS,Field_Replicate) %\\u003e% distinct() %\\u003e% group_by(Field_Replicate) %\\u003e% slice_sample(n=2)\\nps18S_all_mf_frac\\u003c-subset_samples(ps18S_all_mf_frac,ARMS %in% arms_all_mf_frac$ARMS)\\n\\narms_sf40\\u003c- data.frame(sample_data(ps18S_sf40)) %\\u003e% select(ARMS,Field_Replicate) %\\u003e% distinct() %\\u003e% group_by(Field_Replicate) %\\u003e% slice_sample(n=2)\\nps18S_sf40\\u003c-subset_samples(ps18S_sf40,ARMS %in% arms_sf40$ARMS)\\n\\narms_mf100\\u003c- data.frame(sample_data(ps18S_mf100)) %\\u003e% select(ARMS,Field_Replicate) %\\u003e% distinct() %\\u003e% group_by(Field_Replicate) %\\u003e% slice_sample(n=2)\\nps18S_mf100\\u003c-subset_samples(ps18S_mf100,ARMS %in% arms_mf100$ARMS)\\n\\narms_mf500\\u003c- data.frame(sample_data(ps18S_mf500)) %\\u003e% select(ARMS,Field_Replicate) %\\u003e% distinct() %\\u003e% group_by(Field_Replicate) %\\u003e% slice_sample(n=2)\\nps18S_mf500\\u003c-subset_samples(ps18S_mf500,ARMS %in% arms_mf500$ARMS)\\n\\n# Merge samples based on Field_Replicate group\\n\\nps18S_all_frac\\u003c-merge_samples(ps18S_all_frac,\\\"Field_Replicate\\\")\\nps18S_all_mf_frac\\u003c-merge_samples(ps18S_all_mf_frac,\\\"Field_Replicate\\\")\\nps18S_sf40\\u003c-merge_samples(ps18S_sf40,\\\"Field_Replicate\\\")\\nps18S_mf100\\u003c-merge_samples(ps18S_mf100,\\\"Field_Replicate\\\")\\nps18S_mf500\\u003c-merge_samples(ps18S_mf500,\\\"Field_Replicate\\\")\\n\\n# Determine NIS richness for each phyloseq object, get coordinates, taxonomy of NIS and presence-absence at each site and write to file\\n\\nrich_all_frac\\u003c-estimate_richness(ps18S_all_frac,measures=\\\"Observed\\\")\\nrich_all_frac$n_arms\\u003c-2 # add number of ARMS present for this Field_Replicate group\\n# Adjust number of ARMS per Field_Replicate for some cases\\nrich_all_frac$n_arms\\u003c-ifelse(rownames(rich_all_frac)==\\\"Varberg\\\" | rownames(rich_all_frac)== \\\"TZS\\\" | rownames(rich_all_frac)==\\\"Marstrand\\\" | rownames(rich_all_frac)==\\\"Laeso\\\"| rownames(rich_all_frac)==\\\"Eilat\\\"| rownames(rich_all_frac)==\\\"Gbg\\\"| rownames(rich_all_frac)==\\\"Helsingborg\\\"| rownames(rich_all_frac)==\\\"GulfOfPiran\\\"| rownames(rich_all_frac)==\\\"AZBE\\\"| rownames(rich_all_frac)==\\\"AJJ.AZFP\\\",1,rich_all_frac$n_arms)\\nfor(i in 1:nrow(sample_data(ps18S_all_frac))) { # add coordinates (have already been averaged for all ARMS of each site during the merge_samples step)\\n  rownames(rich_all_frac)\\u003c-gsub(\\\"\\\\\\\\.\\\",\\\"-\\\",rownames(rich_all_frac))\\n  rich_all_frac$Latitude[rownames(rich_all_frac) == rownames(sample_data(ps18S_all_frac))[i]] \\u003c- sample_data(ps18S_all_frac)$Latitude[i]\\n  rich_all_frac$Longitude[rownames(rich_all_frac) == rownames(sample_data(ps18S_all_frac))[i]] \\u003c- sample_data(ps18S_all_frac)$Longitude[i]\\n} \\notu_table_all_frac\\u003c-data.frame(t(otu_table(ps18S_all_frac)))\\notu_table_all_frac[otu_table_all_frac\\u003e0]\\u003c-1\\ntax_table_all_frac\\u003c-data.frame(tax_table(ps18S_all_frac))\\ntax_table_all_frac\\u003c-tax_table_all_frac[order(match(rownames(tax_table_all_frac),rownames(otu_table_all_frac))),]\\nall_frac_nis\\u003c-cbind(tax_table_all_frac,otu_table_all_frac)\\n\\nrich_all_mf_frac\\u003c-estimate_richness(ps18S_all_mf_frac,measures=\\\"Observed\\\")\\nrich_all_mf_frac$n_arms\\u003c-2 # add number of ARMS present for this Field_Replicate group\\n# Adjust number of ARMS per Field_Replicate for some cases\\nrich_all_mf_frac$n_arms\\u003c-ifelse(rownames(rich_all_mf_frac)==\\\"Varberg\\\" | rownames(rich_all_mf_frac)==\\\"GulfOfPiran\\\"| rownames(rich_all_mf_frac)==\\\"AZBE\\\"| rownames(rich_all_mf_frac)==\\\"G\\\",1,rich_all_mf_frac$n_arms)\\nfor(i in 1:nrow(sample_data(ps18S_all_mf_frac))) { # add coordinates (have already been averaged for all ARMS of each site during the merge_samples step)\\n  rownames(rich_all_mf_frac)\\u003c-gsub(\\\"\\\\\\\\.\\\",\\\"-\\\",rownames(rich_all_mf_frac))\\n  rich_all_mf_frac$Latitude[rownames(rich_all_mf_frac) == rownames(sample_data(ps18S_all_mf_frac))[i]] \\u003c- sample_data(ps18S_all_mf_frac)$Latitude[i]\\n  rich_all_mf_frac$Longitude[rownames(rich_all_mf_frac) == rownames(sample_data(ps18S_all_mf_frac))[i]] \\u003c- sample_data(ps18S_all_mf_frac)$Longitude[i]\\n} \\notu_table_all_mf_frac\\u003c-data.frame(t(otu_table(ps18S_all_mf_frac)))\\notu_table_all_mf_frac[otu_table_all_mf_frac\\u003e0]\\u003c-1\\ntax_table_all_mf_frac\\u003c-data.frame(tax_table(ps18S_all_mf_frac))\\ntax_table_all_mf_frac\\u003c-tax_table_all_mf_frac[order(match(rownames(tax_table_all_mf_frac),rownames(otu_table_all_mf_frac))),]\\nall_mf_frac_nis\\u003c-cbind(tax_table_all_mf_frac,otu_table_all_mf_frac)\\n\\nrich_sf40\\u003c-estimate_richness(ps18S_sf40,measures=\\\"Observed\\\")\\nrich_sf40$n_arms\\u003c-2 # add number of ARMS present for this Field_Replicate group\\n# Adjust number of ARMS per Field_Replicate for some cases\\nrich_sf40$n_arms\\u003c-ifelse(rownames(rich_sf40)==\\\"Laeso\\\"| rownames(rich_sf40)==\\\"Eilat\\\"| rownames(rich_sf40)==\\\"Helsingborg\\\" | rownames(rich_sf40)==\\\"AZBE\\\"| rownames(rich_sf40)==\\\"Coastbusters\\\"| rownames(rich_sf40)==\\\"Crete\\\",1,rich_sf40$n_arms)\\nfor(i in 1:nrow(sample_data(ps18S_sf40))) { # add coordinates (have already been averaged for all ARMS of each site during the merge_samples step)\\n  rownames(rich_sf40)\\u003c-gsub(\\\"\\\\\\\\.\\\",\\\"-\\\",rownames(rich_sf40))\\n  rich_sf40$Latitude[rownames(rich_sf40) == rownames(sample_data(ps18S_sf40))[i]] \\u003c- sample_data(ps18S_sf40)$Latitude[i]\\n  rich_sf40$Longitude[rownames(rich_sf40) == rownames(sample_data(ps18S_sf40))[i]] \\u003c- sample_data(ps18S_sf40)$Longitude[i]\\n} \\notu_table_sf40\\u003c-data.frame(t(otu_table(ps18S_sf40)))\\notu_table_sf40[otu_table_sf40\\u003e0]\\u003c-1\\ntax_table_sf40\\u003c-data.frame(tax_table(ps18S_sf40))\\ntax_table_sf40\\u003c-tax_table_sf40[order(match(rownames(tax_table_sf40),rownames(otu_table_sf40))),]\\nsf40_nis\\u003c-cbind(tax_table_sf40,otu_table_sf40)\\n\\nrich_mf100\\u003c-estimate_richness(ps18S_mf100,measures=\\\"Observed\\\")\\nrich_mf100$n_arms\\u003c-2 # add number of ARMS present for this Field_Replicate group\\n# Adjust number of ARMS per Field_Replicate for some cases\\nrich_mf100$n_arms\\u003c-ifelse(rownames(rich_mf100)==\\\"Getxo\\\" | rownames(rich_mf100)==\\\"AZBE\\\"| rownames(rich_mf100)==\\\"Coastbusters\\\"| rownames(rich_mf100)==\\\"GulfOfPiran\\\",1,rich_mf100$n_arms)\\nfor(i in 1:nrow(sample_data(ps18S_mf100))) { # add coordinates (have already been averaged for all ARMS of each site during the merge_samples step)\\n  rownames(rich_mf100)\\u003c-gsub(\\\"\\\\\\\\.\\\",\\\"-\\\",rownames(rich_mf100))\\n  rich_mf100$Latitude[rownames(rich_mf100) == rownames(sample_data(ps18S_mf100))[i]] \\u003c- sample_data(ps18S_mf100)$Latitude[i]\\n  rich_mf100$Longitude[rownames(rich_mf100) == rownames(sample_data(ps18S_mf100))[i]] \\u003c- sample_data(ps18S_mf100)$Longitude[i]\\n}\\notu_table_mf100\\u003c-data.frame(t(otu_table(ps18S_mf100)))\\notu_table_mf100[otu_table_mf100\\u003e0]\\u003c-1\\ntax_table_mf100\\u003c-data.frame(tax_table(ps18S_mf100))\\ntax_table_mf100\\u003c-tax_table_mf100[order(match(rownames(tax_table_mf100),rownames(otu_table_mf100))),]\\nmf100_nis\\u003c-cbind(tax_table_mf100,otu_table_mf100)\\n\\nrich_mf500\\u003c-estimate_richness(ps18S_mf500,measures=\\\"Observed\\\")\\nrich_mf500$n_arms\\u003c-2 # add number of ARMS present for this Field_Replicate group\\n# Adjust number of ARMS per Field_Replicate for some cases\\nrich_mf500$n_arms\\u003c-ifelse(rownames(rich_mf500)==\\\"Crete\\\" | rownames(rich_mf500)== \\\"AZBE\\\" | rownames(rich_mf500)==\\\"varberg\\\" | rownames(rich_mf500)==\\\"GulfOfPiran\\\",1,rich_mf500$n_arms)\\nfor(i in 1:nrow(sample_data(ps18S_mf500))) { # add coordinates (have already been averaged for all ARMS of each site during the merge_samples step)\\n  rownames(rich_mf500)\\u003c-gsub(\\\"\\\\\\\\.\\\",\\\"-\\\",rownames(rich_mf500))\\n  rich_mf500$Latitude[rownames(rich_mf500) == rownames(sample_data(ps18S_mf500))[i]] \\u003c- sample_data(ps18S_mf500)$Latitude[i]\\n  rich_mf500$Longitude[rownames(rich_mf500) == rownames(sample_data(ps18S_mf500))[i]] \\u003c- sample_data(ps18S_mf500)$Longitude[i]\\n}\\notu_table_mf500\\u003c-data.frame(t(otu_table(ps18S_mf500)))\\notu_table_mf500[otu_table_mf500\\u003e0]\\u003c-1\\ntax_table_mf500\\u003c-data.frame(tax_table(ps18S_mf500))\\ntax_table_mf500\\u003c-tax_table_mf500[order(match(rownames(tax_table_mf500),rownames(otu_table_mf500))),]\\nmf500_nis\\u003c-cbind(tax_table_mf500,otu_table_mf500)\\n\\n# Write number of NIS abundance per site = Field_Replicate group and otu_tables of the 5 phyloseq objects as several sheets to one xlsx file  \\nxlsx::write.xlsx(rich_all_frac, file = \\\"NIS_presence_absence_comparison_18S.xlsx\\\",sheetName = \\\"all_frac_richness\\\", append = FALSE)\\nxlsx::write.xlsx(all_frac_nis, file = \\\"NIS_presence_absence_comparison_18S.xlsx\\\",sheetName = \\\"all_frac_counts_taxa\\\", append = TRUE)\\nxlsx::write.xlsx(rich_all_mf_frac, file = \\\"NIS_presence_absence_comparison_18S.xlsx\\\",sheetName = \\\"all_mf_frac_richness\\\", append = TRUE)\\nxlsx::write.xlsx(all_mf_frac_nis, file = \\\"NIS_presence_absence_comparison_18S.xlsx\\\",sheetName = \\\"all_mf_frac_counts_taxa\\\", append = TRUE)\\nxlsx::write.xlsx(rich_sf40, file = \\\"NIS_presence_absence_comparison_18S.xlsx\\\",sheetName = \\\"sf40_richness\\\", append = TRUE)\\nxlsx::write.xlsx(sf40_nis, file = \\\"NIS_presence_absence_comparison_18S.xlsx\\\",sheetName = \\\"sf40_counts_taxa\\\", append = TRUE)\\nxlsx::write.xlsx(rich_mf100, file = \\\"NIS_presence_absence_comparison_18S.xlsx\\\",sheetName = \\\"mf100_richness\\\", append = TRUE)\\nxlsx::write.xlsx(mf100_nis, file = \\\"NIS_presence_absence_comparison_18S.xlsx\\\",sheetName = \\\"mf100_counts_taxa\\\", append = TRUE)\\nxlsx::write.xlsx(rich_mf500, file = \\\"NIS_presence_absence_comparison_18S.xlsx\\\",sheetName = \\\"mf500_richness\\\", append = TRUE)\\nxlsx::write.xlsx(mf500_nis, file = \\\"NIS_presence_absence_comparison_18S.xlsx\\\",sheetName = \\\"mf500_counts_taxa\\\", append = TRUE)\\n\",\"os_name\":\"\",\"os_version\":\"\"}}}}","data":null,"protocol_id":86559,"case_id":0,"critical_ids":"","duration":0,"original_id":0,"number":"15","cases":[],"critical":null},{"id":2009542,"guid":"919A5CBA21F54344948DB65AF86B302B","previous_id":1766865,"previous_guid":"E044BC123C5411EE882A0A58A9FEAC02","section":"\u003cp\u003eDetermining sites / groups of field replicates \u003c/p\u003e","section_color":"#FFED92","section_duration":0,"is_substep":false,"step":"{\"blocks\":[{\"key\":\"2psng\",\"text\":\"ARMS-MBON consists of observatories regularly deploying ARMS at one to seven sights each. Within an observatory, sites are locations with one or more ARMS being deplyoed with a certain distance from each other. However, during the initial phase of ARMS-MBON, there was no clear definition for what requirements define a group of ARMS units as field replicates. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"fco4h\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"4maq3\",\"text\":\"For downstream analysis purposes, we consider ARMS units deployed with a distance of maximum 20 km from each other as one site = one Field_Replicate group. We took information on coordinates of each deployed ARMS unit from the https://github.com/arms-mbon/data_workspace/blob/main/qualitycontrolled_data/combined/combined_ObservatoryData.csv file from the ARMS-MBON GitHub repository. \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":0,\"offset\":227,\"length\":114}],\"data\":{}},{\"key\":\"1h78g\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"b6upj\",\"text\":\"The follwoing script calculates distances in km in R/RStudio for each ARMS unit combination:\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":51,\"length\":9}],\"entityRanges\":[],\"data\":{}},{\"key\":\"1spe5\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"key\":1,\"offset\":0,\"length\":1}],\"data\":{}},{\"key\":\"6c8j3\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"s12\",\"text\":\"The resulting file is a distance matrix and contains more ARMS then present in the data set processed here, as there are more observatories in ARMS-MBON than data was available for at this time. We manually checked the distances between the ARMS units which are part of our data set and manually added the column Field_Replicate to the XX_sample_data.txt files used in the next section (see below). In this column, we defined a code for each site = Field_Replicate group an ARMS unit belongs to.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"style\":\"italic\",\"offset\":313,\"length\":15},{\"style\":\"italic\",\"offset\":336,\"length\":19}],\"entityRanges\":[],\"data\":{}},{\"key\":\"961bk\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}},{\"key\":\"e05pp\",\"text\":\"\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{\"0\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"guid\":\"2D3E0CD5E12711EEA5730A58A9FEAC02\",\"url\":\"https://github.com/arms-mbon/data_workspace/blob/main/qualitycontrolled_data/combined/combined_ObservatoryData.csv\"}},\"1\":{\"type\":\"command\",\"mutability\":\"IMMUTABLE\",\"data\":{\"can_edit\":true,\"command_name\":\"Calculate distances between ARMS in R\",\"description\":\"\",\"guid\":\"3F920BC3F73C11EEAF630A58A9FEAC02\",\"name\":\"library(dplyr)\\nlibrary(geosphere) # install.packages(\\\"geosphere\\\")\\n\\n# prepare metadata\\nmeta\\u003c-read.csv(\\\"combined_ObservatoryData.csv\\\",header = T) # read csv file with info on observatories available on ARMS-MBON GitHub repo\\nmeta\\u003c-meta %\\u003e% select(c(ObservatoryID,UnitID,Longitude,Latitude)) # subset necessary columns\\nmeta\\u003c-meta[order(meta$ObservatoryID),]\\nrownames(meta)\\u003c-meta$UnitID\\nmeta\\u003c-meta[,-(1:2)]\\n\\n# calculate distances\\ndistances\\u003c-distm(meta)\\ndistances\\u003c-distances/1000 # distances are given in meters and have to be changed to km\\nrownames(distances)\\u003c-rownames(meta)\\ncolnames(distances)\\u003c-rownames(meta)\\nwrite.table(distances,\\\"distances.txt\\\",sep=\\\"\\\\t\\\",col.names = NA)\\n\",\"os_name\":\"\",\"os_version\":\"\"}}}}","data":null,"protocol_id":86559,"case_id":0,"critical_ids":"","duration":0,"original_id":0,"number":"11","cases":[],"critical":null}],"template_id":5,"title":"ARMS-MBON 18S rRNA and COI gene metabarcoding: scanning for non-indigenous species ","title_html":"ARMS-MBON 18S rRNA and COI gene metabarcoding: scanning for non-indigenous species ","type_id":1,"units":[{"id":1,"type_id":3,"name":"µL","can_manage":0,"read_only":0},{"id":2,"type_id":3,"name":"mL","can_manage":0,"read_only":0},{"id":3,"type_id":3,"name":"L","can_manage":0,"read_only":0},{"id":4,"type_id":3,"name":"µg","can_manage":0,"read_only":0},{"id":5,"type_id":3,"name":"mg","can_manage":0,"read_only":0},{"id":6,"type_id":3,"name":"g","can_manage":0,"read_only":0},{"id":7,"type_id":3,"name":"kg","can_manage":0,"read_only":0},{"id":8,"type_id":3,"name":"ng","can_manage":0,"read_only":0},{"id":9,"type_id":3,"name":"Hz","can_manage":0,"read_only":0},{"id":10,"type_id":24,"name":"°C","can_manage":0,"read_only":0},{"id":11,"type_id":24,"name":"°К","can_manage":0,"read_only":0},{"id":12,"type_id":24,"name":"°F","can_manage":0,"read_only":0},{"id":13,"type_id":25,"name":"Mass Percent","can_manage":0,"read_only":0},{"id":14,"type_id":25,"name":"% volume","can_manage":0,"read_only":0},{"id":15,"type_id":25,"name":"Mass / % volume","can_manage":0,"read_only":0},{"id":16,"type_id":25,"name":"Parts per Million (PPM)","can_manage":0,"read_only":0},{"id":17,"type_id":25,"name":"Parts per Billion (PPB)","can_manage":0,"read_only":0},{"id":18,"type_id":25,"name":"Parts per Trillion (PPT)","can_manage":0,"read_only":0},{"id":19,"type_id":25,"name":"Mole Fraction","can_manage":0,"read_only":0},{"id":20,"type_id":25,"name":"Mole Percent","can_manage":0,"read_only":0},{"id":21,"type_id":25,"name":"Molarity (M)","can_manage":0,"read_only":1},{"id":22,"type_id":25,"name":"Molarity (M)","can_manage":0,"read_only":0},{"id":23,"type_id":25,"name":"Genome copies per ml","can_manage":0,"read_only":0},{"id":24,"type_id":3,"name":"μV","can_manage":0,"read_only":0},{"id":25,"type_id":3,"name":"ms","can_manage":0,"read_only":0},{"id":26,"type_id":3,"name":"pg","can_manage":0,"read_only":0},{"id":27,"type_id":25,"name":"Molarity dilutions","can_manage":0,"read_only":0},{"id":28,"type_id":25,"name":"millimolar (mM)","can_manage":0,"read_only":0},{"id":29,"type_id":25,"name":"micromolar (µM)","can_manage":0,"read_only":0},{"id":30,"type_id":25,"name":"nanomolar (nM)","can_manage":0,"read_only":0},{"id":31,"type_id":25,"name":"picomolar (pM)","can_manage":0,"read_only":0},{"id":32,"type_id":24,"name":"Room temperature","can_manage":0,"read_only":0},{"id":33,"type_id":30,"name":"rpm","can_manage":0,"read_only":0},{"id":34,"type_id":30,"name":"x g","can_manage":0,"read_only":0},{"id":165,"type_id":24,"name":"On ice","can_manage":0,"read_only":0},{"id":200,"type_id":32,"name":"cm","can_manage":0,"read_only":0},{"id":201,"type_id":32,"name":"mm","can_manage":0,"read_only":0},{"id":202,"type_id":32,"name":"µm","can_manage":0,"read_only":0},{"id":203,"type_id":32,"name":"nm","can_manage":0,"read_only":0},{"id":204,"type_id":25,"name":"mg/mL","can_manage":0,"read_only":0},{"id":205,"type_id":25,"name":"µg/µL","can_manage":0,"read_only":0},{"id":206,"type_id":25,"name":"% (v/v)","can_manage":0,"read_only":0},{"id":207,"type_id":3,"name":"V","can_manage":0,"read_only":0},{"id":1324,"type_id":30,"name":"rcf","can_manage":0,"read_only":0},{"id":1359,"type_id":35,"name":"Bar","can_manage":0,"read_only":0},{"id":1360,"type_id":35,"name":"Pa","can_manage":0,"read_only":0}],"uri":"arms-mbon-18s-rrna-and-coi-gene-metabarcoding-scan-cyr7xv9n","url":"https://www.protocols.io/view/arms-mbon-18s-rrna-and-coi-gene-metabarcoding-scan-cyr7xv9n","version_class":86559,"version_data":{"id":0,"code":"cyr7xv9n","version_class":86559,"parent_id":null,"parent_uri":null,"is_same_owner":false,"is_parent_public":false,"has_pending_merge_request":false,"has_approved_merge_request":false,"merge_request":null},"version_id":0,"version_uri":"arms-mbon-18s-rrna-and-coi-gene-metabarcoding-scan-n92ldmmmnl5b/v1","versions":[{"id":86559,"title":"ARMS-MBON 18S rRNA and COI gene metabarcoding: scanning for non-indigenous species ","title_html":"ARMS-MBON 18S rRNA and COI gene metabarcoding: scanning for non-indigenous species ","image":{"source":"https://www.protocols.io/img/default_protocol.png","webp_source":null,"placeholder":"https://www.protocols.io/img/default_protocol.png","webp_placeholder":null},"doi":"dx.doi.org/10.17504/protocols.io.n92ldmmmnl5b/v1","uri":"arms-mbon-18s-rrna-and-coi-gene-metabarcoding-scan-cyr7xv9n","published_on":1730104865,"modified_on":1730104865,"version_class":86559,"version_id":0,"version_code":"cyr7xv9n","version_uri":"arms-mbon-18s-rrna-and-coi-gene-metabarcoding-scan-n92ldmmmnl5b/v1","created_on":1692204573,"categories":null,"type_id":1,"creator":{"name":"Nauras Daraghmeh","affiliation":"Department of Marine Sciences, University of Gothenburg, Sweden","affiliation_url":"","username":"nauras-daraghmeh","link":"https://www.researchgate.net/profile/Nauras-Daraghmeh","user_image_file":{"guid":"36D872605B2B11EFA2EF0A58A9FEAC02","file_name":"prs2bimqf.jpg","url":"https://files.protocols.io/external/prs2bimqf-36D872605B2B11EFA2EF0A58A9FEAC02.jpg","mime":"image/jpeg","size":300289,"width":0,"height":0,"avg_color":"","scan_status":0,"created_at":1723742663}},"stats":{"number_of_comments":0,"last_comment_time":0}}],"warning":""}